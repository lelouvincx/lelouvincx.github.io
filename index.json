[{"content":"","date":"2023-04-16","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"2023-04-16","permalink":"/tags/dagster/","section":"Tags","summary":"","title":"dagster"},{"content":"","date":"2023-04-16","permalink":"/categories/data-engineering/","section":"Categories","summary":"","title":"data engineering"},{"content":"","date":"2023-04-16","permalink":"/tags/dbt/","section":"Tags","summary":"","title":"dbt"},{"content":"","date":"2023-04-16","permalink":"/tags/elt/","section":"Tags","summary":"","title":"ELT"},{"content":"","date":"2023-04-16","permalink":"/tags/goodreads/","section":"Tags","summary":"","title":"goodreads"},{"content":"Trong project này mình sẽ hướng dẫn xây dựng một data pipeline cơ bản theo mô hình ELT (extract - load - transform), sử dụng bộ dữ liệu từ goodreads để ingest, transform data phục vụ hệ thống recommend sách cho bản thân.\nProject này hoàn thành dựa trên kiến thức đã học được từ khóa Fundamental Data Engineering 02 của AIDE. Xin gửi lời cảm ơn đặc biệt tới thầy Nguyễn Thanh Bình, anh Ông Xuân Hồng và anh Hùng Lê.\n   lelouvincx/goodreads-elt-pipeline   This project implements an ELT (Extract - Load - Transform) data pipeline with the goodreads dataset, using dagster (orchestration), spark (calculation) and dbt (transformation)  Jupyter Notebook     2     0     \n    Demo video  \n  1. Introduction #  Mình thích đọc sách. Và mình có một chiếc máy đọc sách kindle dùng cho việc đọc hàng ngày.\n   Có một điểm mình thích ở chiếc kindle là nó có một địa chỉ email riêng biệt được amazon cấp phát. Nếu sử dụng email của mình để gửi file sách (dạng .epub/.mobi), hệ thống trên amazon sẽ tự động gửi file sách vào kindle giúp mình, miễn là có kết nối mạng.\nThế thì tại sao mình không tự build một app, có thể lấy data từ goodreads (một mạng xã hội cho các mọt sách), xử lý và đưa ra lời gợi ý cho những cuốn sách tiếp theo cho mình nhỉ? Và thế là project bắt đầu :D\n 2. Objective #  Dataset cơ bản được lấy từ Kaggle, OpenLibrary API, Google Drive API và Notion API\nMục tiêu của project, là với nguồn data được thu thập và xử lý sẵn, khi người dùng nhập thông tin cuốn sách đã đọc, có thể gợi ý các cuốn sách tiếp theo. Nếu cuốn sách đó có file .epub, app sẽ hiện tính năng gửi sách vào kindle.\n 3. Design #   3.1 Directory tree #      app: Giao diện để tương tác với người dùng, viết bằng streamlit dagster_home: Config cho dagit (dagster UI) và dagster daemon dataset: Lưu dataset dưới dạng .csv, mặc định load vào MySQL docker-compose: Compose docker containers dockerimages: Chứa các image tự build như dagster (dagit + daemon), spark master, streamlit app EDA.ipynb: Exploratory Data Analysis, xem trực tiếp tại đây elt_pipeline: Toàn bộ pipeline  dbt_transform: code location của dbt, dùng cho bước transform ở cuối Dockerfile + requirements.txt: Thông tin build image elt_pipeline: pipeline từ đầu tới trước dbt   .env + .spark_master.env + .spark_worker.env: Lưu biến môi trường (ví dụ POSTGRES_USER, MYSQL_USER, SPARK, \u0026hellip;) env.template: Template để bạn tự điền thông tin biến môi trường .git + .gitignore: Thuộc git, quản lý code version Makefile: Shortcut câu lệnh trên terminal load_dataset: Chứa script .sql để tạo schema và load vào database MySQL và Postgres requirements.txt + Pipfile + Pipfile.lock: Dependencies của python  Ngoài ra còn có các thư mục sở hữu riêng của container:\n minio storage  mysql_data postgres_data metabase_data    Chi tiết xem ở file tree.txt\n 3.2 Pipeline design #     Ta sử dụng docker để đóng gói ứng dụng và dagster để orchestrate assets (theo định nghĩa của daster) Dữ liệu Goodreads được download từ kaggle dưới dạng .csv, sau đó import vào MySQL mô phỏng dữ liệu development Sau khi có thông tin ISBN (mã định danh quốc tế) của sách, tiến hành collect thêm data từ các api liên quan  Genre, author, pages number, image, description từ OpenLibrary API Download link từ Notion API Epub file từ Google Drive API Image từ OpenLibrary API hoặc Google Drive API   Extract dữ liệu dạng bảng ở trên bằng polars, load vào datalake - MinIO Từ MinIO load ra spark để transform thành layer silver và gold Convert Spark DataFrame thành parquet, load lại vào MinIO Gold layer được load tiếp vào data warehouse - postgreSQL, tạo thành warehouse layer Transform tùy mục đích bằng dbt trên nền postgres Trực quan hóa dữ liệu bằng metabase Giao diện app gợi ý sách bằng streamlit   3.3 Database schema #      book: OLTP table chứa thông tin cuốn sách (ISBN, Authors, Rating, Description\u0026hellip;) genre: table chứa tên các thể loại sách book_genre: quan hệ n-n giữa book và genre book_download_link: table chứa link google drive để tham chiếu BookFile files: object storage chứa file download sách (.epub/.pdf/.mobi) images: object storage chứa hình ảnh cuốn sách   3.4 Datalake structure #      Datalake chia theo các layer: bronze, silver, gold Các loại file đều dạng .parquet để cho kết quả đọc tốt hơn .csv Ngoài ra có files: lưu file .epub theo dạng abc.epub, trong đó abc là ISBN của cuốn sách Tương tự, abc.jpeg lưu hình ảnh của cuốn sách   3.5 Data lineage #   Tổng quan     Với data lineage dày đặc, dagster là big help khi có thể visualize chúng một cách trực quan:\n Dữ liệu xuất phát từ MySQL và các loại API, load vào bronze layer Từ bronze layer, dữ liệu được dedupe, clean và fill missing ở silver layer Sau đó tính toán nâng cao và phân tách ở gold layer Load vào data warehouse - Postgres ở warehouse layer Và cuối cùng, transform theo nhu cầu ở recommendations layer bằng dbt  Bronze layer     Gồm các asset:\n bronze_book: Bảng book từ MySQL, vì quá lớn (trên 1.2 triệu dòng nên được partitions theo năm từ 1975 tới 2022) bronze_genre: Bảng genre từ MySQL bronze_book_genre: Bảng book_genre từ MySQL bronze_book_download_link: Bảng book_download_link từ MySQL bronze_images_and_files_download: Đảm nhận việc kết nối tới google drive api, kéo file .epub và hình ảnh về, lưu trong datalake  Silver layer     Gồm các asset:\n silver_cleaned_book: Clean data từ upstream bronze_book, được partition để đảm bảo spark standalone mode có thể chạy silver_collected_book: Collect thêm missing data từ upstream như authors, pages number, description từ OpenLibrary API silver_isbn: Tách cột isbn từ book để làm dependency cho asset liên quan tới genre silver_cleaned_genre: Tương tự silver_cleaned_book, khác cái không cần partition vì size không lớn lắm silver_collected_genre: Dựa vào silver_isbn, collect thêm genre cho mỗi cuốn sách bị thiếu genre. Nếu không có genre thì không thể làm recommendations cho các task sau silver_collected_book_genre: Kết nối quan hệ n-n giữa book và genre  Gold layer     Gồm các asset:\n gold_genre: Tính toán, sắp xếp các genre cho phù hợp từ upstream silver_collected_genre, đồng thời lưu vào minIO gold_book_genre: Tương tự, từ upstream silver_collected_book_genre gold_with_info: Phân tách, chỉ chứa thông tin cơ bản về cuốn sách như ISBN, Name, Authors, Language, PagesNumber gold_with_publish: Phân tách, chỉ chứa thông tin về nhà xuất bản, thời gian xuất bản gold_with_rating: Phân tách và tính toán các loại rating  Warehouse layer     Load các asset từ gold layer vào postgres. Trong đó có 1 asset từ bronze layer là book_download_link.\nTrong tương lai sẽ cập nhật asset để bổ sung download link tự động từ Notion API, và thiết lập schedule.\nTransform layer     Gồm các model (asset):\n search: Transform thông tin để tạo bảng index, khi người dùng tìm kiếm sẽ query trên bảng này search_prior: Cũng là bảng index, nhưng chứa các cuốn sách được ưu tiên hơn dựa vào việc có download link, OpenLibrary API hoạt động, rating cao, \u0026hellip; criteria: Tiêu chí để query những cuốn sách liên quan khi tìm kiếm 1 cuốn sách   4. Setup #   4.1 Prequisites #  Để sử dụng pipeline này, download những phần mềm sau:\n Git Docker ít nhất 4GB RAM, 6 core CPU, 2GB swap, 16GB disk CMake, nếu dùng hệ máy UNIX (Linux/MacOS), check make --version được cài sẵn Python 3.x (3.9.16 recommended vì image của spark chạy trên version này, khuyến khích cài bằng asdf) và môi trường ảo (pipenv recommended) Máy local đã free các port sau: 3306, 5432, 9000, 9001, 3001, 8501, 4040, 7077, 8080, 3030 Dbeaver hoặc một db client bất kỳ (nếu không có thể dùng command-line)     Nếu dùng Windows, setup thêm WSL2 và một máy ảo local Ubuntu, cài đặt những thứ trên cho ubuntu.  Clone the repository\ngit clone https://github.com/lelouvincx/goodreads-elt-pipeline.git project cd project  4.2 Setup google drive api #  Đầu tiên chúng ta cần tạo một OAuth 2.0 token tới google, Google API Console.\nChọn create new project để mở hộp thoại.\n   Điền tên của project vào (goodreads-elt_pipeline), tùy chọn location của bạn (mặc định No organization).\n   Sau khi tạo xong project, chọn tab Library.\n   Search Google Drive API, enable nó.\n      Tiếp theo, chọn tab OAuth consent screen,\n   Điền thông tin như dưới\n   Ở mục scopes, chọn add or remove scopes, tìm google drive api, readonly rồi tick vào, save and continue tới hết\n   Vào tab credentials -\u0026gt; create credentials và chọn OAuth client ID.\n   Chọn Desktop app, đặt tên tùy thích (goodreads-elt-pipeline)\n   Download json và đặt file vào project/elt_pipeline/elt_pipeline\n    4.3 Setup local infrastructure #  Create env file\ntouch .env cp env.template .env touch .spark_master.env cp spark_master.env.template .spark_master.env touch .spark_worker.env cp spark_worker.env.template .spark_worker.env Sau đó điền thông tin biến môi trường vào 3 file env trên. Dưới đây là ví dụ:\n# MySQL MYSQL_HOST=de_mysql MYSQL_PORT=3306 MYSQL_DATABASE=goodreads MYSQL_USER=admin MYSQL_PASSWORD=admin123 MYSQL_ROOT_PASSWORD=root123 # PostgreSQL POSTGRES_HOST=de_psql POSTGRES_PORT=5432 POSTGRES_USER=admin POSTGRES_PASSWORD=admin123 POSTGRES_DB=goodreads POSTGRES_HOST_AUTH_METHOD=trust # Google Drive GDRIVE_CLIENT_SECRET_FILE=client_secret.json GDRIVE_PICKLE_FILE=token_drive_v3.pickle GDRIVE_API_NAME=drive GDRIVE_API_VERSION=v3 GDRIVE_SCOPES=https://www.googleapis.com/auth/drive.readonly # Dagster DAGSTER_PG_HOSTNAME=de_psql DAGSTER_PG_USERNAME=admin DAGSTER_PG_PASSWORD=admin123 DAGSTER_PG_DB=postgres DAGSTER_OVERALL_CONCURRENCY_LIMIT=1 DAGSTER_HOME=/opt/dagster/dagster_home # dbt DBT_HOST=de_psql DBT_USER=admin DBT_PASSWORD=admin123 DBT_DATABASE=goodreads DBT_SCHEMA=recommendations # MinIO MINIO_ENDPOINT=minio:9000 MINIO_ROOT_USER=minio MINIO_ROOT_PASSWORD=minio123 MINIO_ACCESS_KEY=minio MINIO_SECRET_KEY=minio123 DATALAKE_BUCKET=lakehouse AWS_ACCESS_KEY_ID=minio AWS_SECRET_ACCESS_KEY=minio123 AWS_REGION=us-east-1 # MinIO client (mc) AWS_ACCESS_KEY_ID=minio AWS_SECRET_ACCESS_KEY=minio123 AWS_REGION=us-east-1 # Spark SPARK_MASTER_URL=spark://spark-master:7077 SPARK_VERSION=3.3.2 HADOOP_VERSION=3 # Metabase MB_DB_TYPE=postgres MB_DB_DBNAME=goodreads MB_DB_PORT=5432 MB_DB_USER=admin MB_DB_PASS=admin123 MB_DB_HOST=de_psql MB_DB_FILE=/metabase_data/metabase.db Bạn có thể thay các thông tin về user, pasword, \u0026hellip;\n   Chỉ dùng cho môi trường development, không dùng cho testing, staging, production.  Bây giờ chúng ta import dataset spotify (dạng csv) vào mySQL:\nChạy các lệnh sau để setup:\n# DO NOT RUN BOTH BELOW COMMANDS, ONLY CHOOSE ONE # Setup python environment pipenv install # Or create virtualenv and install manually by requirements.txt make install # Build docker images make build-dagster make build-spark make build-pipeline make build-streamlit # Run containers dettached make up-bg # Check running containers docker compose ps -a # Check code quality make check make lint # Format pipelines black ./elt_pipeline # Test coverage make test     Lúc này sẽ có 11 services sau đang chạy:           Ports   MySQL: 3306 PostgreSQL: 5432 Dagit: 3001 MinIO  UI: 9001 API: 9000   Spark master:  UI: 8080 API: 7077   Pipeline:  Spark jobs running: 4040   Metabase: 3030 Streamlit: 8501   4.4 Import data into MySQL #  Source từng file theo thứ tự:\nmake to_mysql_root SETGLOBALlocal_infile=TRUE;-- Check if local_infile was turned on SHOWVARIABLESLIKE\u0026#34;local_infile\u0026#34;;exit# Create tables with schema make mysql_create # Load csv into created tables make mysql_load  4.5 Create schema in Postgres #  make psql_create  4.6 User interfaces #   http://localhost:3001 - Dagit http://localhost:4040 - Spark jobs http://localhost:8080 - Spark master http://localhost:9001 - MinIO http://localhost:3030 - Metabase http://localhost:8501 - Streamlit   5. Detailed code walkthrough #   5.1 Exploratory Data Analysis #  Chi tiết xem ở gist: https://gist.github.com/lelouvincx/a88fa6caf59d7ff76086ab485ecc69bd\n 5.2 Extract (MySQL/API) #  mysql_io_manager\ndef connect_mysql(config) -\u0026gt; str: conn_info = ( f\u0026#34;mysql://{config[\u0026#39;user\u0026#39;]}:{config[\u0026#39;password\u0026#39;]}\u0026#34; + f\u0026#34;@{config[\u0026#39;host\u0026#39;]}:{config[\u0026#39;port\u0026#39;]}\u0026#34; + f\u0026#34;/{config[\u0026#39;database\u0026#39;]}\u0026#34; ) return conn_info def extract_data(self, sql: str) -\u0026gt; pl.DataFrame: \u0026#34;\u0026#34;\u0026#34; Extract data from MySQL database as polars DataFrame \u0026#34;\u0026#34;\u0026#34; conn_info = connect_mysql(self._config) df_data = pl.read_database(query=sql, connection_uri=conn_info) return df_data constants\nCOMPUTE_KIND = \u0026#34;SQL\u0026#34; LAYER = \u0026#34;bronze\u0026#34; YEARLY = StaticPartitionsDefinition( [str(year) for year in range(1975, datetime.today().year)] ) bronze_book\n# book from my_sql @asset( description=\u0026#34;Load table \u0026#39;book\u0026#39; from MySQL database as polars DataFrame, and save to minIO\u0026#34;, partitions_def=YEARLY, io_manager_key=\u0026#34;minio_io_manager\u0026#34;, required_resource_keys={\u0026#34;mysql_io_manager\u0026#34;}, key_prefix=[\u0026#34;bronze\u0026#34;, \u0026#34;goodreads\u0026#34;], compute_kind=COMPUTE_KIND, group_name=LAYER, ) def bronze_book(context) -\u0026gt; Output[pl.DataFrame]: query = \u0026#34;SELECT * FROM book\u0026#34; try: partion_year_str = context.asset_partition_key_for_output() partition_by = \u0026#34;PublishYear\u0026#34; query += f\u0026#34; WHERE {partition_by}= {partion_year_str}\u0026#34; context.log.info(f\u0026#34;Partition by {partition_by}= {partion_year_str}\u0026#34;) except Exception: context.log.info(\u0026#34;No partition key found, full load data\u0026#34;) df_data = context.resources.mysql_io_manager.extract_data(query) context.log.info(f\u0026#34;Table extracted with shape: {df_data.shape}\u0026#34;) return Output( value=df_data, metadata={ \u0026#34;table\u0026#34;: \u0026#34;book\u0026#34;, \u0026#34;row_count\u0026#34;: df_data.shape[0], \u0026#34;column_count\u0026#34;: df_data.shape[1], \u0026#34;columns\u0026#34;: df_data.columns, }, ) bronze_genre\n# genre from my_sql @asset( description=\u0026#34;Load table \u0026#39;genre\u0026#39; from MySQL database as polars DataFrame, and save to minIO\u0026#34;, io_manager_key=\u0026#34;minio_io_manager\u0026#34;, required_resource_keys={\u0026#34;mysql_io_manager\u0026#34;}, key_prefix=[\u0026#34;bronze\u0026#34;, \u0026#34;goodreads\u0026#34;], compute_kind=COMPUTE_KIND, group_name=LAYER, ) def bronze_genre(context) -\u0026gt; Output[pl.DataFrame]: query = \u0026#34;SELECT * FROM genre;\u0026#34; df_data = context.resources.mysql_io_manager.extract_data(query) context.log.info(f\u0026#34;Table extracted with shape: {df_data.shape}\u0026#34;) return Output( value=df_data, metadata={ \u0026#34;table\u0026#34;: \u0026#34;genre\u0026#34;, \u0026#34;row_count\u0026#34;: df_data.shape[0], \u0026#34;column_count\u0026#34;: df_data.shape[1], \u0026#34;columns\u0026#34;: df_data.columns, }, ) Các asset khác tương tự.\n 5.3 Load (datalake - minIO) #  minio_io_manager\n@contextmanager def connect_minio(config): client = Minio( endpoint=config.get(\u0026#34;endpoint_url\u0026#34;), access_key=config.get(\u0026#34;minio_access_key\u0026#34;), secret_key=config.get(\u0026#34;minio_secret_key\u0026#34;), secure=False, ) try: yield client except Exception as e: raise e # Make bucket if not exists def make_bucket(client: Minio, bucket_name): found = client.bucket_exists(bucket_name) if not found: client.make_bucket(bucket_name) else: print(f\u0026#34;Bucket {bucket_name}already exists.\u0026#34;) def _get_path(self, context: Union[InputContext, OutputContext]): # E.g context.asset_key.path: [\u0026#39;bronze\u0026#39;, \u0026#39;goodreads\u0026#39;, \u0026#39;book\u0026#39;] layer, schema, table = context.asset_key.path # NOTE: E.g: bronze/goodreads/book key = \u0026#34;/\u0026#34;.join([layer, schema, table.replace(f\u0026#34;{layer}_\u0026#34;, \u0026#34;\u0026#34;)]) # E.g /tmp/file_bronze_goodreads_book_20210101000000.parquet tmp_file_path = \u0026#34;/tmp/file_{}_{}.parquet\u0026#34;.format( \u0026#34;_\u0026#34;.join(context.asset_key.path), datetime.today().strftime(\u0026#34;%Y%m%d%H%M%S\u0026#34;) ) # Partition by year if context.has_partition_key: # E.g partition_str: book_2021 partition_str = str(table) + \u0026#34;_\u0026#34; + context.asset_partition_key # E.g key_name: bronze/goodreads/book/book_2021.parquet # tmp_file_path: /tmp/file_bronze_goodreads_book_20210101000000.parquet return os.path.join(key, f\u0026#34;{partition_str}.parquet\u0026#34;), tmp_file_path else: # E.g key_name: bronze/goodreads/book.parquet return f\u0026#34;{key}.parquet\u0026#34;, tmp_file_path def handle_output(self, context: \u0026#34;OutputContext\u0026#34;, obj: pl.DataFrame): key_name, tmp_file_path = self._get_path(context) # Convert from polars DataFrame to parquet format obj.write_parquet(tmp_file_path) # Upload file to minIO try: bucket_name = self._config.get(\u0026#34;bucket\u0026#34;) with connect_minio(self._config) as client: # Make bucket if not exist make_bucket(client, bucket_name) # Upload file to minIO # E.g bucket_name: lakehouse, # key_name: bronze/goodreads/book/book_2021.parquet, # tmp_file_path: /tmp/file_bronze_goodreads_book_20210101000000.parquet client.fput_object(bucket_name, key_name, tmp_file_path) context.log.info( f\u0026#34;(MinIO handle_output) Number of rows and columns: {obj.shape}\u0026#34; ) context.add_output_metadata({\u0026#34;path\u0026#34;: key_name, \u0026#34;tmp\u0026#34;: tmp_file_path}) # Clean up tmp file os.remove(tmp_file_path) except Exception as e: raise e def load_input(self, context: \u0026#34;InputContext\u0026#34;) -\u0026gt; pl.DataFrame: \u0026#34;\u0026#34;\u0026#34; Prepares input for downstream asset, and downloads parquet file from minIO and converts to polars DataFrame \u0026#34;\u0026#34;\u0026#34; bucket_name = self._config.get(\u0026#34;bucket\u0026#34;) key_name, tmp_file_path = self._get_path(context) try: with connect_minio(self._config) as client: # Make bucket if not exist make_bucket(client=client, bucket_name=bucket_name) # E.g bucket_name: lakehouse, # key_name: bronze/goodreads/book/book_2021.parquet, # tmp_file_path: /tmp/file_bronze_goodreads_book_20210101000000.parquet context.log.info(f\u0026#34;(MinIO load_input) from key_name: {key_name}\u0026#34;) client.fget_object(bucket_name, key_name, tmp_file_path) df_data = pl.read_parquet(tmp_file_path) context.log.info( f\u0026#34;(MinIO load_input) Got polars dataframe with shape: {df_data.shape}\u0026#34; ) return df_data except Exception as e: raise e  5.4 Transform (spark) #  # Silver cleaned book @asset( description=\u0026#34;Load book table from bronze layer in minIO, into a Spark dataframe, then clean data\u0026#34;, partitions_def=YEARLY, ins={ \u0026#34;bronze_book\u0026#34;: AssetIn( key_prefix=[\u0026#34;bronze\u0026#34;, \u0026#34;goodreads\u0026#34;], ), }, io_manager_key=\u0026#34;spark_io_manager\u0026#34;, key_prefix=[\u0026#34;silver\u0026#34;, \u0026#34;goodreads\u0026#34;], compute_kind=COMPUTE_KIND, group_name=LAYER, ) def silver_cleaned_book(context, bronze_book: pl.DataFrame): \u0026#34;\u0026#34;\u0026#34; Load book table from bronze layer in minIO, into a Spark dataframe, then clean data \u0026#34;\u0026#34;\u0026#34; config = { \u0026#34;endpoint_url\u0026#34;: os.getenv(\u0026#34;MINIO_ENDPOINT\u0026#34;), \u0026#34;minio_access_key\u0026#34;: os.getenv(\u0026#34;MINIO_ACCESS_KEY\u0026#34;), \u0026#34;minio_secret_key\u0026#34;: os.getenv(\u0026#34;MINIO_SECRET_KEY\u0026#34;), } context.log.debug(\u0026#34;Start creating spark session\u0026#34;) with get_spark_session(config, str(context.run.run_id).split(\u0026#34;-\u0026#34;)[0]) as spark: # Convert bronze_book from polars DataFrame to Spark DataFrame pandas_df = bronze_book.to_pandas() context.log.debug( f\u0026#34;Converted to pandas DataFrame with shape: {pandas_df.shape}\u0026#34; ) spark_df = spark.createDataFrame(pandas_df) spark_df.cache() context.log.info(\u0026#34;Got Spark DataFrame\u0026#34;) # Dedupe books spark_df = spark_df.dropDuplicates() # Drop rows with null value in column \u0026#39;Name\u0026#39; spark_df = spark_df.na.drop(subset=[\u0026#34;Name\u0026#34;]) # Drop rows with null values in column ISBN spark_df = spark_df.na.drop(subset=[\u0026#34;isbn\u0026#34;]) # Drop rows will null values in column \u0026#39;Language\u0026#39; spark_df = spark_df.na.drop(subset=[\u0026#34;Language\u0026#34;]) # Drop rows with value \u0026#39;--\u0026#39; in column \u0026#39;Language\u0026#39; spark_df = spark_df.filter(spark_df.Language != \u0026#34;--\u0026#34;) # Drop rows with value \u0026gt; 350 in column \u0026#39;PagesNumber\u0026#39; spark_df = spark_df.filter(spark_df.PagesNumber \u0026lt;= 350) # Drop column \u0026#39;CountsOfReview\u0026#39; (overlap with \u0026#39;RatingDistTotal\u0026#39;) spark_df = spark_df.drop(\u0026#34;CountsOfReview\u0026#34;) # Choose rows with \u0026#39;PublishYear\u0026#39; from 1900 to datetime.today().year spark_df = spark_df.filter( (spark_df.PublishYear \u0026gt;= 1975) \u0026amp; (spark_df.PublishYear \u0026lt;= datetime.today().year) ) # Update value of column \u0026#39;RatingDist...\u0026#39;, splitting by \u0026#39;:\u0026#39; and take the second value spark_df = spark_df.withColumn( \u0026#34;RatingDist5\u0026#34;, split_take_second(col(\u0026#34;RatingDist5\u0026#34;)) ) spark_df = spark_df.withColumn( \u0026#34;RatingDist4\u0026#34;, split_take_second(col(\u0026#34;RatingDist4\u0026#34;)) ) spark_df = spark_df.withColumn( \u0026#34;RatingDist3\u0026#34;, split_take_second(col(\u0026#34;RatingDist3\u0026#34;)) ) spark_df = spark_df.withColumn( \u0026#34;RatingDist2\u0026#34;, split_take_second(col(\u0026#34;RatingDist2\u0026#34;)) ) spark_df = spark_df.withColumn( \u0026#34;RatingDist1\u0026#34;, split_take_second(col(\u0026#34;RatingDist1\u0026#34;)) ) spark_df = spark_df.withColumn( \u0026#34;RatingDistTotal\u0026#34;, split_take_second(col(\u0026#34;RatingDistTotal\u0026#34;)) ) # Cast column \u0026#39;RatingDist...\u0026#39; to Interger spark_df = spark_df.withColumn( \u0026#34;RatingDist5\u0026#34;, spark_df.RatingDist5.cast(\u0026#34;Integer\u0026#34;) ) spark_df = spark_df.withColumn( \u0026#34;RatingDist4\u0026#34;, spark_df.RatingDist4.cast(\u0026#34;Integer\u0026#34;) ) spark_df = spark_df.withColumn( \u0026#34;RatingDist3\u0026#34;, spark_df.RatingDist3.cast(\u0026#34;Integer\u0026#34;) ) spark_df = spark_df.withColumn( \u0026#34;RatingDist2\u0026#34;, spark_df.RatingDist2.cast(\u0026#34;Integer\u0026#34;) ) spark_df = spark_df.withColumn( \u0026#34;RatingDist1\u0026#34;, spark_df.RatingDist1.cast(\u0026#34;Integer\u0026#34;) ) spark_df = spark_df.withColumn( \u0026#34;RatingDistTotal\u0026#34;, spark_df.RatingDistTotal.cast(\u0026#34;Integer\u0026#34;) ) # Change column name \u0026#39;Count of text reviews\u0026#39; to \u0026#39;CountOfTextReviews\u0026#39; spark_df = spark_df.withColumnRenamed( \u0026#34;Count of text reviews\u0026#34;, \u0026#34;CountOfTextReviews\u0026#34; ) # Change value of column \u0026#39;Language\u0026#39; from [\u0026#39;en-US\u0026#39;, \u0026#39;en-GB\u0026#39;, \u0026#39;en-CA\u0026#39;], to \u0026#39;eng\u0026#39;, from \u0026#39;nl\u0026#39; to \u0026#39;nld\u0026#39; spark_df = spark_df.withColumn( \u0026#34;Language\u0026#34;, regexp_replace(\u0026#34;Language\u0026#34;, \u0026#34;en-US\u0026#34;, \u0026#34;eng\u0026#34;) ) spark_df = spark_df.withColumn( \u0026#34;Language\u0026#34;, regexp_replace(\u0026#34;Language\u0026#34;, \u0026#34;en-GB\u0026#34;, \u0026#34;eng\u0026#34;) ) spark_df = spark_df.withColumn( \u0026#34;Language\u0026#34;, regexp_replace(\u0026#34;Language\u0026#34;, \u0026#34;en-CA\u0026#34;, \u0026#34;eng\u0026#34;) ) spark_df = spark_df.withColumn( \u0026#34;Language\u0026#34;, regexp_replace(\u0026#34;Language\u0026#34;, \u0026#34;nl\u0026#34;, \u0026#34;nld\u0026#34;) ) spark_df.unpersist() return Output( value=spark_df, metadata={ \u0026#34;table\u0026#34;: \u0026#34;silver_cleaned_book\u0026#34;, \u0026#34;row_count\u0026#34;: spark_df.count(), \u0026#34;column_count\u0026#34;: len(spark_df.columns), \u0026#34;columns\u0026#34;: spark_df.columns, }, ) # Silver collected book @asset( description=\u0026#34;Collect more infomation about cleaned books, such as authors, number of pages\u0026#34;, partitions_def=YEARLY, ins={ \u0026#34;silver_cleaned_book\u0026#34;: AssetIn( key_prefix=[\u0026#34;silver\u0026#34;, \u0026#34;goodreads\u0026#34;], metadata={\u0026#34;full_load\u0026#34;: False}, ), }, io_manager_key=\u0026#34;spark_io_manager\u0026#34;, key_prefix=[\u0026#34;silver\u0026#34;, \u0026#34;goodreads\u0026#34;], compute_kind=\u0026#34;OpenLibrary API\u0026#34;, group_name=LAYER, ) def silver_collected_book(context, silver_cleaned_book: DataFrame) -\u0026gt; Output[DataFrame]: \u0026#34;\u0026#34;\u0026#34; Collect more infomation about cleaned books - Authors: if missing - Number of pages: if missing \u0026#34;\u0026#34;\u0026#34; spark_df = silver_cleaned_book context.log.debug(\u0026#34;Caching spark_df ...\u0026#34;) spark_df.cache() context.log.info(\u0026#34;Starting filling missing data ...\u0026#34;) null_authors_df = spark_df.filter( (spark_df.Authors.isNull()) | (spark_df.Authors == \u0026#34;\u0026#34;) ) null_pages_number_df = spark_df.filter((spark_df.PagesNumber.isNull())) count = 0 for row in null_authors_df.select(\u0026#34;ISBN\u0026#34;).collect(): isbn = row[0] context.log.debug(f\u0026#34;Got isbn: {isbn}\u0026#34;) if isbn is not None: # Get request from OpenLibrary API req = requests.get( f\u0026#34;https://openlibrary.org/api/books?bibkeys=ISBN:{isbn}\u0026amp;format=json\u0026amp;jscmd=data\u0026#34; ) json = req.json() if len(json.keys()) \u0026gt; 0: context.log.debug(\u0026#34;Got json with data\u0026#34;) # Check if spark_df with column \u0026#39;ISBN\u0026#39; = isbn has missing value in column \u0026#39;Authors\u0026#39; row_from_df = spark_df.filter(spark_df.ISBN == isbn).collect()[0] if row_from_df.Authors is None or row_from_df.Authors is \u0026#34;\u0026#34;: context.log.debug(\u0026#34;Authors is missing, start filling ...\u0026#34;) # Take the first author author = json.get(f\u0026#34;ISBN:{isbn}\u0026#34; or {}).get(\u0026#34;authors\u0026#34; or []) author = author[0].get(\u0026#34;name\u0026#34;) if len(author) \u0026gt; 0 else None if author: count += 1 # Update spark_df with column \u0026#39;ISBN\u0026#39; = isbn and column \u0026#39;Authors\u0026#39; = author spark_df = spark_df.withColumn( \u0026#34;Authors\u0026#34;, when( (spark_df.ISBN == isbn) \u0026amp; ( (spark_df.Authors.isNull()) | (spark_df.Authors == \u0026#34;\u0026#34;) ), author, ).otherwise(spark_df.Authors), ) context.log.info(f\u0026#34;Filled in {count}authors\u0026#34;) count = 0 for row in null_pages_number_df.select(\u0026#34;ISBN\u0026#34;).collect(): isbn = row[0] context.log.debug(f\u0026#34;Got isbn: {isbn}\u0026#34;) if isbn is not None: # Get request from OpenLibrary API req = requests.get( f\u0026#34;https://openlibrary.org/api/books?bibkeys=ISBN:{isbn}\u0026amp;format=json\u0026amp;jscmd=data\u0026#34; ) json = req.json() if len(json.keys()) \u0026gt; 0: context.log.debug(\u0026#34;Got json with real data\u0026#34;) # Check if spark_df with column \u0026#39;ISBN\u0026#39; = isbn has missing value in column \u0026#39;Authors\u0026#39; row_from_df = spark_df.filter(spark_df.ISBN == isbn).collect()[0] # Check if spark_df with column \u0026#39;ISBN\u0026#39; = isbn has missing value in column \u0026#39;PagesNumber\u0026#39; if row_from_df.PagesNumber is None or row_from_df.PagesNumber == 0: context.log.debug(\u0026#34;PagesNumber is missing, start filling ...\u0026#34;) # Take the number of pages pages_number = json.get(f\u0026#34;ISBN:{isbn}\u0026#34; or {}).get(\u0026#34;number_of_pages\u0026#34;) if pages_number: count += 1 # Update spark_df with column \u0026#39;ISBN\u0026#39; = isbn and column \u0026#39;PagesNumber\u0026#39; = pages_number spark_df = spark_df.withColumn( \u0026#34;PagesNumber\u0026#34;, when( (spark_df.ISBN == isbn) \u0026amp; (spark_df.PagesNumber.isNull()), pages_number, ).otherwise(spark_df.PagesNumber), ) context.log.info(f\u0026#34;Filled in {count}pages numbers\u0026#34;) spark_df.unpersist() return Output( value=spark_df, metadata={ \u0026#34;table\u0026#34;: \u0026#34;silver_collected_book\u0026#34;, \u0026#34;row_count\u0026#34;: spark_df.count(), \u0026#34;column_count\u0026#34;: len(spark_df.columns), \u0026#34;columns\u0026#34;: spark_df.columns, }, )  5.5 Load (data warehouse - Postgres) #  psql_io_manager\n@contextmanager def connect_psql(config): try: yield psycopg2.connect( host=config[\u0026#34;host\u0026#34;], port=config[\u0026#34;port\u0026#34;], database=config[\u0026#34;database\u0026#34;], user=config[\u0026#34;user\u0026#34;], password=config[\u0026#34;password\u0026#34;], ) except (Exception) as e: print(f\u0026#34;Error while connecting to PostgreSQL: {e}\u0026#34;) def handle_output(self, context: \u0026#34;OutputContext\u0026#34;, obj: pl.DataFrame): # E.g context.asset_key.path = [\u0026#39;warehouse\u0026#39;, \u0026#39;gold\u0026#39;, \u0026#39;book_genre\u0026#39;] schema = context.asset_key.path[-2] # NOTE: Replace pattern is \u0026#39;warehouse\u0026#39;, not general table = str(context.asset_key.path[-1]).replace(\u0026#34;warehouse_\u0026#34;, \u0026#34;\u0026#34;) context.log.debug(f\u0026#34;Schema: {schema}, Table: {table}\u0026#34;) tmp_tbl = f\u0026#34;{table}_tmp_{datetime.now().strftime(\u0026#39;%Y_%m_%d\u0026#39;)}\u0026#34; try: with connect_psql(self._config) as conn: context.log.debug(f\u0026#34;Connected to PostgreSQL: {conn}\u0026#34;) primary_keys = (context.metadata or {}).get(\u0026#34;primary_keys\u0026#34;, []) context.log.debug(f\u0026#34;Primary keys: {primary_keys}\u0026#34;) with conn.cursor() as cursor: context.log.debug(f\u0026#34;Cursor info: {cursor}\u0026#34;) cursor.execute(\u0026#34;SELECT version()\u0026#34;) context.log.info(f\u0026#34;PostgreSQL version: {cursor.fetchone()}\u0026#34;) # Create temp file cursor.execute( f\u0026#34;CREATE TEMP TABLE IF NOT EXISTS {tmp_tbl}(LIKE {schema}.{table})\u0026#34; ) cursor.execute(f\u0026#34;SELECT COUNT(*) FROM {tmp_tbl}\u0026#34;) context.log.debug( f\u0026#34;Log for creating temp table: {cursor.fetchone()}\u0026#34; ) # Create sql identifiers for the column names # Do this to safely insert into a sql query columns = sql.SQL(\u0026#34;,\u0026#34;).join( sql.Identifier(name.lower()) for name in obj.columns ) # Create a placeholder for the values. These will be filled later values = sql.SQL(\u0026#34;,\u0026#34;).join(sql.Placeholder() for _ in obj.columns) # Create the insert query context.log.debug(\u0026#34;Inserting data into temp table\u0026#34;) insert_query = sql.SQL(\u0026#34;INSERT INTO {}({}) VALUES({});\u0026#34;).format( sql.Identifier(tmp_tbl), columns, values ) # Execute the insert query psycopg2.extras.execute_batch(cursor, insert_query, obj.rows()) conn.commit() # Check data inserted context.log.debug(\u0026#34;Checking data inserted\u0026#34;) cursor.execute(f\u0026#34;SELECT COUNT(*) FROM {tmp_tbl};\u0026#34;) context.log.info(f\u0026#34;Number of rows inserted: {cursor.fetchone()}\u0026#34;) # Upsert data if len(primary_keys) \u0026gt; 0: context.log.debug(\u0026#34;Table has primary keys, upserting data\u0026#34;) conditions = \u0026#34; AND \u0026#34;.join( [ f\u0026#34;\u0026#34;\u0026#34; {schema}.{table}.\u0026#34;{k}\u0026#34; = {tmp_tbl}.\u0026#34;{k}\u0026#34; \u0026#34;\u0026#34;\u0026#34; for k in primary_keys ] ) command = f\u0026#34;\u0026#34;\u0026#34; BEGIN TRANSACTION; DELETE FROM {schema}.{table}USING {tmp_tbl}WHERE {conditions}; INSERT INTO {schema}.{table}SELECT * FROM {tmp_tbl}; END TRANSACTION; \u0026#34;\u0026#34;\u0026#34; else: context.log.debug(\u0026#34;Table has no primary keys, replacing data\u0026#34;) command = f\u0026#34;\u0026#34;\u0026#34; BEGIN TRANSACTION; DELETE FROM {schema}.{table}; INSERT INTO {schema}.{table}SELECT * FROM {tmp_tbl}; END TRANSACTION; \u0026#34;\u0026#34;\u0026#34; # context.log.debug(f\u0026#34;Command: {command}\u0026#34;) context.log.debug(f\u0026#34;Upserting data into {schema}.{table}\u0026#34;) cursor.execute(command) context.log.debug(f\u0026#34;{cursor.statusmessage}\u0026#34;) conn.commit() except (Exception) as e: print(f\u0026#34;Error while handling output to PostgreSQL: {e}\u0026#34;) try: with connect_psql(self._config) as conn: with conn.cursor() as cursor: context.log.debug(f\u0026#34;{cursor.fetchone()}\u0026#34;) cursor.execute(f\u0026#34;SELECT COUNT(*) FROM {schema}.{table};\u0026#34;) context.log.info( f\u0026#34;Number of rows upserted in {schema}.{table}: {cursor.fetchone()}\u0026#34; ) # Drop temp table cursor.execute(f\u0026#34;DROP TABLE {tmp_tbl}\u0026#34;) conn.commit() except (Exception) as e: print(f\u0026#34;Error while testing handle_output to PostgreSQL: {e}\u0026#34;)  5.6 Transform (dbt) #  search\nselectisbn,namefrom{{source(\u0026#39;gold\u0026#39;,\u0026#39;book_with_info\u0026#39;)}}search_prior\nselectisbn,namefrom{{source(\u0026#39;gold\u0026#39;,\u0026#39;book_with_info\u0026#39;)}}rightjoin{{source(\u0026#39;recommendations\u0026#39;,\u0026#39;book_download_link\u0026#39;)}}using(isbn)wherelinkisnotnullcriteria\nwithtmp_avg_ratingas(selectisbn,ratingfrom{{source(\u0026#39;gold\u0026#39;,\u0026#39;book_with_rating\u0026#39;)}}),tmp_download_linkas(selectisbn,casewhenlinkisnullthen0else1endashasdownloadlink,ratingfrom{{source(\u0026#39;recommendations\u0026#39;,\u0026#39;book_download_link\u0026#39;)}}rightjointmp_avg_ratingusing(isbn))select*fromtmp_download_link 6. Tear down infrastructure #  Dỡ bỏ containers sau khi xong việc\nmake down  7. Considerations #  Giờ là lúc đánh giá project:\n Tốc độ: spark được cài ở chế độ standalone nên không đạt hiệu suất cao, đôi khi bị crash giữa chừng khi thực hiện các task shuffle/read/write Môi trường phát triển: Hiện tại mới có develop, tương lai sẽ xem xét môi trường testing, staging, production dbt hiện tại là project nhỏ, tương lai nếu cần transform nhiều thì cần tách service riêng, phân quyền, \u0026hellip; Deployment: Sử dụng một trong các dịnh vụ điện toán đám mây: AWS, Azure, GCP   8. Further actions #   Hoàn thiện recommender system Tích hợp jupyter notebook để làm task của data scientist - dagstermill Testing environment Continuous Integration với Github Actions  ","date":"2023-04-16","permalink":"/projects/fde2-goodreads-elt-pipeline/","section":"Projects","summary":"Xây dựng ELT data pipelines hoàn chỉnh với bộ dữ liệu sách từ Goodreads","title":"Goodreads ELT pipelines"},{"content":"","date":"2023-04-16","permalink":"/","section":"lelouvincx's blog","summary":"","title":"lelouvincx's blog"},{"content":"","date":"2023-04-16","permalink":"/tags/minio/","section":"Tags","summary":"","title":"minio"},{"content":"","date":"2023-04-16","permalink":"/projects/","section":"Projects","summary":"","title":"Projects"},{"content":"","date":"2023-04-16","permalink":"/tags/spark/","section":"Tags","summary":"","title":"spark"},{"content":"","date":"2023-04-16","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"","date":"2022-12-18","permalink":"/tags/aws/","section":"Tags","summary":"","title":"AWS"},{"content":"","date":"2022-12-18","permalink":"/tags/spotify/","section":"Tags","summary":"","title":"spotify"},{"content":" 1. Introduction #  Trong project này mình sẽ hướng dẫn xây dựng một data pipeline cơ bản theo mô hình ELT (extract - load - transform), sử dụng bộ dữ liệu từ spotify để phân tích xu hướng nghe nhạc.\nProject này hoàn thành dựa trên kiến thức đã học được từ khóa Fundamental Data Engineering của AIDE. Xin gửi lời cảm ơn đặc biệt tới thầy Nguyễn Thanh Bình, anh Ông Xuân Hồng và anh Hùng Lê.\n   lelouvincx / fde01_project_fde220103_dinhminhchinh   A basic data pipeline follows ELT principle.    0     0      2. Objective #  Mục tiêu của project này là xây dựng một data pipeline để đưa dữ liệu của bảng spotify_tracks từ mySQL và my_tracks từ API của Spotify thành dashboard để phân tích. Các bảng được hỗ trợ bởi spotify_albums và spotify_artists như mô tả dưới đây:\n spotify_tracks: OLTP table chứa thông tin bài hát từ spotify my_tracks: lịch sử stream nhạc của bản thân, lấy schema giống spotify_tracks spotify_albums: chứa thông tin albums từ dataset spotify_artists: thông tin nghệ sĩ từ dataset    table schema  Chi tiết hơn xem ở: Exploratory Data Analysis\n 3. Design #   3.1 Pipeline design #  Chúng ta sử dụng máy ảo AWS EC2 để tính toán và dagster để orchestrate tasks.\n Dữ liệu spotify được download từ kaggle dưới dạng csv, sau đó import vào mySQL mô phỏng làm dữ liệu doanh nghiệp Dữ liệu streaming history của bản thân được extract từ spotify API Extract 2 nguồn dữ liệu trên bằng pandas để preprocessing (optimize size consumed) Load vào Amazon S3, từ đó load tiếp vào data warehouse (PostgreSQL) để làm analytics Transform dữ liệu bằng dbt trên nền PostgreSQL Trực quan hóa dữ liệu bằng Metabase    ELT pipeline design   3.2 Data lake structure #  Chúng ta sử dụng AWS S3 làm data lake. Mọi dữ liệu trước hết sẽ được chứa ở đây. Trong project này, ta chỉ cần 1 bucket với nhiều thư mục.\n Bronze: Lưu dữ liệu thô mới lấy về. Chúng là step 1, 2, 3 trong pipeline design Silver: Lưu dữ liệu được tiền xử lý. Chúng là step 4 trong pipeline design Gold: Lưu dữ liệu sạch khi transform bằng dbt (step 5)    structure of S3   3.3 Directory tree #      docker-compose: compose các container chạy trong docker EDA: khám phá dataset và profiling .gitignore: giúp git không track file (như env, cache, \u0026hellip;) .gitlab-ci: config quá trình CI trên gitlab Makefile: thu gọn câu lệnh requirements.txt: packages python cần thiết và thiết lập virtualenv Folder dagser_home chứa dagster.yaml để config thành phần dagster còn workspace.yaml để chỉ định dagster chạy host elt_pipeline Folder dockers chứa file config các container: dagster và jupyter Folder load_dataset chứa các file dùng để load dữ liệu ban đầu vào mySQL Folder terraform để khởi tạo và config server trên AWS  Chi tiết cây thư mục xem ở: directory tree\n 4. Setup #   4.1 Prequisites #  Để sử dụng pipeline này, download những phần mềm sau:\n Git Tài khoản gitlab Terraform Tài khoản AWS AWS CLI và configure Docker ít nhất 4GB RAM và Docker Compose ít nhất v1.29.2  Nếu dùng Windows, setup thêm WSL và một máy ảo local Ubuntu, cài đặt những thứ trên cho ubuntu.\n 4.2 Setup data infrastructure local #  Clone repository:\ngit clone https://gitlab.com/lelouvincx/fde01_project_fde220103_dinhminhchinh.git mv fde01_project_fde220103_dinhminhchinh project cd project # Create env file touch env cp env.template env Điền các biến môi trường vào file env.\nChạy các lệnh sau để setup infra dưới local:\n# Setup python packages make install # Build docker images make build # Run locally make up # Check running containers docker ps # Check code quality make check make lint # Use black to reformat if any tests failed, then try again black ./elt_pipeline # Test coverage make test Lúc này sẽ có 7 services sau đang chạy:  running services  Bây giờ chúng ta import dataset spotify (dạng csv) vào mySQL:\n# Enter mysql cli make to_mysql SETGLOBALlocal_infile=true;-- Check if local_infile is turned on SHOWVARIABLESLIKE\u0026#34;local_infile\u0026#34;;exitSource từng file theo thứ tự:\n# Create tables with schema make mysql_create # Load csv into created tables make mysql_load # Set their foreign keys make mysql_set_foreign_key Khởi tạo schema và table trong PostgreSQL:\n# Enter psql cli make psql_create Testing:\n# Test utils python3 -m pytest -vv --cov=utils elt_pipeline/tests/utils # Test ops python3 -m pytest -vv --cov=ops elt_pipeline/tests/ops Truy cập giao diện của pipeline bằng dagit: https://localhost:3001/\n 4.3 Setup data infrastructure on AWS #   Chúng ta dùng terraform làm IaC (Infrastructure as Code) để setup hạ tầng trên AWS (nhớ cấp credential key cho AWS nhé)  cd terraform # Initialize infra make tf-init # Checkout planned infra make tf-plan # Build up infra make tf-up Đợi một chút để setup xong. Chúng ta lên Amazon Web Services\n Trong EC2 sẽ thấy 1 máy ảo tên project-spotify-EC2    Trong S3 thấy 1 bucket tên project-spotify-bucket     Sau khi project-spotify-EC2 hiện đã pass hết status thì chúng ta đã setup thành công.\nBây giờ chúng ta truy cập vào EC2 để hoàn tất setup  # Connect to EC2 from local terminal make ssh-ec2 # Generate new ssh key for gitlab ssh-keygen # Then press Enter until done cat ~/.ssh/id_rsa.pub  Copy đoạn mã SSH Vào gitlab, phía trên góc phải có hình avatar -\u0026gt; Preferences -\u0026gt; SSH Keys -\u0026gt; paste key vừa copy vào -\u0026gt; đặt tên là \u0026lsquo;project-spotify-vm\u0026rsquo; -\u0026gt; Add key Vào terminal của EC2 (vừa connect lúc nãy), clone về bằng SSH Lặp lại bước setup infra local đã trình bày ở phần trên   5. Detailed code walkthrough #  ELT pipeline gồm 2 job chạy 2 tác vụ độc lập: EL data từ MySQL và EL data từ API nhưng nhìn chung chúng có cấu trúc giống nhau. Cụ thể:\n extractdata_from{mysql/api}: Lấy data từ MySQL hoặc api (thông qua access token) và lưu tạm dưới dạng pandas.DataFrame. Tùy theo chiến lược ingest data (full load/incremental by partition/incremental by watermark) mà có cách giải quyết phù hợp. load_data_to_s3: Tiền xử lý data types cho DataFrame từ upstream và load vào S3 dưới dạng parquet. load_data_to_psql: Extract data dạng parquet trong S3 thành pandas.DataFrame và load vào PostgreSQL. Để dữ liệu được toàn vẹn (không bị crash, lỗi đường truyền) trong quá trình crash, ta tạo TEMP TABLE và load vào đó trước. validate_{mssql2psql/api2psql}_ingestion: Thẩm định 3 step trên đã được EL thành công hay chưa trigger_dbt_spotify: Sensor để trigger dbt nhằm transform data.  job_mssql2psql_ingestion    job_api2psql_ingestion     5.1 Extract #  Lấy data từ MySQL hoặc api (thông qua access token) và lưu tạm dưới dạng pandas.DataFrame. Tùy theo chiến lược ingest data (full load/incremental by partition/incremental by watermark) mà có cách giải quyết phù hợp.\nTa định nghĩa phương thức extract data của mysql và api trong thư mục utils.\nutils/mysql_loader/extract  def extract_data(self, sql: str) -\u0026gt; pd.DataFrame: pd_data = None with self.get_db_connection() as db_conn: pd_data = pd.read_sql(sql, db_conn) return pd_data utils/api_loader/get_recently  def get_recently(self, number: int, token: str) -\u0026gt; (int, dict): headers = { \u0026#34;Accept\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer \u0026#34; + token, } params = [(\u0026#34;limit\u0026#34;, number),] try: response = requests.get( \u0026#34;https://api.spotify.com/v1/me/player/recently-played\u0026#34;, headers=headers, params=params, timeout=10, ) return (response.status_code, response.json()) except: return None utils/api_loader/extract  def extract_data(self, token: str) -\u0026gt; pd.DataFrame: (code, content) = self.get_recently(50, token) my_tracks = { \u0026#34;album_id\u0026#34;: [], \u0026#34;artists_id\u0026#34;: [], \u0026#34;track_id\u0026#34;: [], \u0026#34;track_unique_id\u0026#34;: [], \u0026#34;name\u0026#34;: [], \u0026#34;popularity\u0026#34;: [], \u0026#34;type\u0026#34;: [], \u0026#34;duration_ms\u0026#34;: [], \u0026#34;played_at\u0026#34;: [], \u0026#34;danceability\u0026#34;: [], \u0026#34;energy\u0026#34;: [], \u0026#34;track_key\u0026#34;: [], \u0026#34;loudness\u0026#34;: [], \u0026#34;mode\u0026#34;: [], \u0026#34;speechiness\u0026#34;: [], \u0026#34;acousticness\u0026#34;: [], \u0026#34;instrumentalness\u0026#34;: [], \u0026#34;liveness\u0026#34;: [], \u0026#34;valence\u0026#34;: [], \u0026#34;tempo\u0026#34;: [], } items = content.get(\u0026#34;items\u0026#34;, []) for item in items: # Take album_id, artists_id, track_id, name, popularity, type, duration_ms played_at = item.get(\u0026#34;played_at\u0026#34;, []) track = item.get(\u0026#34;track\u0026#34;, []) album = track.get(\u0026#34;album\u0026#34;, []) album_id = album.get(\u0026#34;id\u0026#34;, []) artists = track.get(\u0026#34;artists\u0026#34;, []) artists_id = [] for artist in artists: artists_id.append(artist.get(\u0026#34;id\u0026#34;, [])) track_id = track.get(\u0026#34;id\u0026#34;, []) name = track.get(\u0026#34;name\u0026#34;, []) popularity = track.get(\u0026#34;popularity\u0026#34;, []) type = track.get(\u0026#34;type\u0026#34;, []) duration_ms = track.get(\u0026#34;duration_ms\u0026#34;, []) # Take features features = self.get_features(track_id, token) danceability = features.get(\u0026#34;danceability\u0026#34;, []) energy = features.get(\u0026#34;energy\u0026#34;, []) track_key = features.get(\u0026#34;key\u0026#34;, []) loudness = features.get(\u0026#34;loudness\u0026#34;, []) mode = features.get(\u0026#34;mode\u0026#34;, []) speechiness = features.get(\u0026#34;speechiness\u0026#34;, []) acousticness = features.get(\u0026#34;acousticness\u0026#34;, []) instrumentalness = features.get(\u0026#34;instrumentalness\u0026#34;, []) liveness = features.get(\u0026#34;liveness\u0026#34;, []) valence = features.get(\u0026#34;valence\u0026#34;, []) tempo = features.get(\u0026#34;tempo\u0026#34;, []) # Extract row into dict my_tracks[\u0026#34;album_id\u0026#34;].append(album_id) my_tracks[\u0026#34;artists_id\u0026#34;].append(artists_id) my_tracks[\u0026#34;track_id\u0026#34;].append(track_id) my_tracks[\u0026#34;track_unique_id\u0026#34;].append(track_id + played_at) my_tracks[\u0026#34;name\u0026#34;].append(name) my_tracks[\u0026#34;popularity\u0026#34;].append(popularity) my_tracks[\u0026#34;type\u0026#34;].append(type) my_tracks[\u0026#34;duration_ms\u0026#34;].append(duration_ms) my_tracks[\u0026#34;played_at\u0026#34;].append(played_at[:10]) my_tracks[\u0026#34;danceability\u0026#34;].append(danceability) my_tracks[\u0026#34;energy\u0026#34;].append(energy) my_tracks[\u0026#34;track_key\u0026#34;].append(track_key) my_tracks[\u0026#34;loudness\u0026#34;].append(loudness) my_tracks[\u0026#34;mode\u0026#34;].append(mode) my_tracks[\u0026#34;speechiness\u0026#34;].append(speechiness) my_tracks[\u0026#34;acousticness\u0026#34;].append(acousticness) my_tracks[\u0026#34;instrumentalness\u0026#34;].append(instrumentalness) my_tracks[\u0026#34;liveness\u0026#34;].append(liveness) my_tracks[\u0026#34;valence\u0026#34;].append(valence) my_tracks[\u0026#34;tempo\u0026#34;].append(tempo) pd_data = pd.DataFrame(my_tracks) return pd_data Giờ là lúc extract data.\nextract_data_from_mysql  def extract_data_from_mysql(context, run_config): updated_at = context.op_config.get(\u0026#34;updated_at\u0026#34;) context.log.info(f\u0026#34;Updated at: {updated_at}\u0026#34;) if updated_at is None or updated_at == \u0026#34;\u0026#34;: context.log.info(\u0026#34;Nothing to do!\u0026#34;) return None context.log.info(f\u0026#34;Op extracts data from MySQL at {updated_at}\u0026#34;) # Choose extract strategy (default: full load) sql_stm = f\u0026#34;\u0026#34;\u0026#34; SELECT * FROM {run_config.get(\u0026#39;src_tbl\u0026#39;)}WHERE 1=1 \u0026#34;\u0026#34;\u0026#34; if run_config.get(\u0026#34;strategy\u0026#34;) == \u0026#34;incremental_by_partition\u0026#34;: if updated_at != \u0026#34;init_dump\u0026#34;: sql_stm += f\u0026#34;\u0026#34;\u0026#34; AND CAST({run_config.get(\u0026#39;partition\u0026#39;)}AS DATE) = \u0026#39;{updated_at}\u0026#39; \u0026#34;\u0026#34;\u0026#34; if run_config.get(\u0026#34;strategy\u0026#34;) == \u0026#34;incremental_by_watermark\u0026#34;: data_loader = get_data_loader( run_config.get(\u0026#34;db_provider\u0026#34;), run_config.get(\u0026#34;target_db_params\u0026#34;) ) watermark = data_loader.get_watermark( f\u0026#34;{run_config.get(\u0026#39;target_schema\u0026#39;)}.{run_config.get(\u0026#39;target_tbl\u0026#39;)}\u0026#34;, run_config.get(\u0026#34;watermark\u0026#34;), ) watermark = ( updated_at if watermark is None or watermark \u0026gt; updated_at else watermark ) if updated_at != \u0026#34;init_dump\u0026#34;: sql_stm += f\u0026#34;\u0026#34;\u0026#34; AND {run_config.get(\u0026#39;watermark\u0026#39;)}\u0026gt;= \u0026#39;{watermark}\u0026#39; \u0026#34;\u0026#34;\u0026#34; context.log.info(f\u0026#34;Extracting with SQL: {sql_stm}\u0026#34;) db_loader = MysqlLoader(run_config.get(\u0026#34;src_db_params\u0026#34;)) pd_data = db_loader.extract_data(sql_stm) context.log.info(f\u0026#34;Data extracted successfully with shape: {pd_data.shape}\u0026#34;) # Update params run_config.update( { \u0026#34;updated_at\u0026#34;: updated_at, \u0026#34;data\u0026#34;: pd_data, \u0026#34;s3_path\u0026#34;: f\u0026#34;bronze/{run_config.get(\u0026#39;data_source\u0026#39;)}/{run_config.get(\u0026#39;ls_target\u0026#39;).get(\u0026#39;target_tbl\u0026#39;)}\u0026#34;, \u0026#34;load_dtypes\u0026#34;: run_config.get(\u0026#34;load_dtypes\u0026#34;), } ) return run_config extract_data_from_api  def extract_data_from_api(context, run_config): updated_at = context.op_config.get(\u0026#34;updated_at\u0026#34;) context.log.info(f\u0026#34;Updated at: {updated_at}\u0026#34;) if updated_at is None or updated_at == \u0026#34;\u0026#34;: context.log.info(\u0026#34;Nothing to do!\u0026#34;) return None context.log.info(f\u0026#34;Op extracts data from API at {updated_at}\u0026#34;) # Extract strategy (only support incremental_by_partition) context.log.info(f\u0026#34;Extracting on date: {updated_at}\u0026#34;) api_loader = ApiLoader(run_config.get(\u0026#34;src_api_params\u0026#34;)) token = api_loader.get_api_token() pd_data = api_loader.extract_data(token) index_played_at = pd_data[pd_data[\u0026#34;played_at\u0026#34;] != updated_at].index # Drop data pd_data.drop(index_played_at, inplace=True) context.log.info( f\u0026#34;Data loaded and filtered successfully with shape: {pd_data.shape}\u0026#34; ) run_config.update( { \u0026#34;updated_at\u0026#34;: updated_at, \u0026#34;data\u0026#34;: pd_data, \u0026#34;s3_path\u0026#34;: f\u0026#34;bronze/{run_config.get(\u0026#39;data_source\u0026#39;)}/{run_config.get(\u0026#39;ls_target\u0026#39;).get(\u0026#39;target_tbl\u0026#39;)}\u0026#34;, \u0026#34;load_dtypes\u0026#34;: run_config.get(\u0026#34;load_dtypes\u0026#34;), } ) return run_config  5.2 Load #  Tiền xử lý data types cho DataFrame từ upstream và load vào S3 dưới dạng parquet.\nload_data_to_s3  def load_data_to_s3(context, upstream): if upstream is None: return None updated_at = upstream.get(\u0026#34;updated_at\u0026#34;) s3_bucket = os.getenv(\u0026#34;DATALAKE_BUCKET\u0026#34;) if type(updated_at) == list: updated_at = max(updated_at) s3_file = f\u0026#34;s3://{s3_bucket}/{upstream.get(\u0026#39;s3_path\u0026#39;)}/updated_at={updated_at}\u0026#34; context.log.info(f\u0026#34;Loading data to S3: {s3_file}\u0026#34;) # Load data to S3 pd_data = upstream.get(\u0026#34;data\u0026#34;) # Preprocess data load_dtypes = upstream.get(\u0026#34;load_dtypes\u0026#34;) try: for col, data_type in load_dtypes.items(): if data_type == \u0026#34;str\u0026#34;: pd_data[col] = pd_data[col].fillna(\u0026#34;\u0026#34;) pd_data[col] = pd_data[col].astype(str) pd_data[col] = pd_data[col].str.strip() pd_data[col] = pd_data[col].str.rstrip() pd_data[col] = pd_data[col].str.replace(\u0026#34;\u0026#39;\u0026#34;, \u0026#34;\u0026#34;) pd_data[col] = pd_data[col].str.replace(\u0026#39;\u0026#34;\u0026#39;, \u0026#34;\u0026#34;) pd_data[col] = pd_data[col].str.replace(r\u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;, regex=True) elif data_type == \u0026#34;int\u0026#34;: cur_bit = np.log2(pd_data[col].max()) if cur_bit \u0026gt; 32: pd_data[col] = pd_data[col].astype({col: \u0026#34;int64\u0026#34;}) elif cur_bit \u0026gt; 16: pd_data[col] = pd_data[col].astype({col: \u0026#34;int32\u0026#34;}) elif cur_bit \u0026gt; 8: pd_data[col] = pd_data[col].astype({col: \u0026#34;int16\u0026#34;}) else: pd_data[col] = pd_data[col].astype({col: \u0026#34;int8\u0026#34;}) elif data_type == \u0026#34;float\u0026#34;: pd_data[col] = pd_data[col].astype({col: \u0026#34;float32\u0026#34;}) context.log.info(f\u0026#34;Data preprocessed successfully\u0026#34;) except Exception as e: context.log.info(f\u0026#34;Exception: {e}\u0026#34;) # Write parquet object to S3 pa_data = pa.Table.from_pandas(df=pd_data, preserve_index=False) pq.write_table(pa_data, s3_file) context.log.info(\u0026#34;Data loaded successfully to S3\u0026#34;) # Update stream upstream.update({\u0026#34;s3_bucket\u0026#34;: s3_bucket, \u0026#34;s3_file\u0026#34;: s3_file}) return upstream load_data_to_psql  def load_data_to_psql(context, upstream): if upstream is None: return None # Load data to target context.log.info(\u0026#34;Loading data to postgreSQL\u0026#34;) context.log.info(f\u0026#34;Extracting data from {upstream.get(\u0026#39;s3_file\u0026#39;)}\u0026#34;) pd_stag = pd.read_parquet(upstream.get(\u0026#34;s3_file\u0026#34;)) context.log.info(f\u0026#34;Extracted data shape: {pd_stag.shape}\u0026#34;) if len(pd_stag) == 0: context.log.info(\u0026#34;No data to upload!\u0026#34;) return \u0026#34;No data to upload!\u0026#34; # Execute db_loader = PsqlLoader(upstream.get(\u0026#34;target_db_params\u0026#34;)) result = db_loader.load_data(pd_stag, upstream) context.log.info(f\u0026#34;Batch inserted status: {result}\u0026#34;) return result  5.3 Transform #  Do dataset hầu hết đã được clean. Chúng ta chọn các column cần thiết cho việc visualization.\ncleaned_my_tracks.sql  {{config(materialized=\u0026#39;table\u0026#39;)}}withmy_unique_tracksAS(select*fromspotify.my_tracksgroupbytrack_unique_id)select*frommy_unique_trackscleaned_spotify_tracks.sql  {{config(materialized=\u0026#39;table\u0026#39;)}}withunique_tracksAS(selectacousticness,album_id,artists_id,country,danceability,duration_ms,energy,track_id,instrumentalness,track_key,liveness,loudness,mode,name,popularity,speechiness,tempo,valencefromspotify.spotify_tracksgroupbytrack_id)select*fromunique_trackscleaned_spotify_artists.sql  {{config(materialized=\u0026#39;table\u0026#39;)}}withunique_artistsAS(selectartist_popularity,followers,genres,artist_id,name,track_idfromspotify.spotify_artistsgroupbyartist_id)select*fromunique_artistscleaned_spotify_albums.sql  {{config(materialized=\u0026#39;table\u0026#39;)}}withunique_albumsAS(selectalbum_id,artist_id,album_type,name,release_date,release_date_precision,total_tracks,track_idfromspotify.spotify_albumsgroupbyalbum_id)select*fromunique_albums 5.4 Check results #  Sau khi transform thành công, mở dbeaver lên và chúng ta thấy các table đã sẵn sàng để analytics.\n 6. Tear down infrastructure #  Dỡ bỏ infra sau khi xong việc (thực hiện dưới máy local):\n# Tear down containers make down # Tear down AWS cd terraform make tf-down     Note: Lên AWS kiểm tra lại các services sau đã dừng và bị xóa chưa (nếu không muốn mất tiền oan như mình): EC2, S3.   7. Design considerations #  Sau khi deploy thành công pipeline, giờ là lúc đánh giá project.\n Tốc độ: Tốc độ extract data khá chậm (vì load vào pandas.DataFrame 2 lần). Một số giải pháp thay thế: polars, json, \u0026hellip; Kích thước: Chuyện gì sẽ xảy ra khi data lớn lên gấp 10x, 100x, 1000x? Lúc đấy ta cần xem xét các giải pháp giúp lưu trữ big data, thay đổi data warehouse thành Amazon RDS, Google BigQuery, \u0026hellip; Môi trường phát triển: Khi project có thêm nhiều người cùng sử dụng là cũng là lúc phân chia môi trường thành testing, staging, production.   8. Further actions #   Tăng lượng data: Tích hợp nhiều data hơn từ Spotify API: Khi ingest bài hát mới, ingest luôn thông tin về artist, album, tạo thành hệ sinh thái bài hát đầy đủ. Stream ingestion: Dùng một tech stack khác cho job API theo hướng streaming. Hệ thống sẽ listen mỗi lần nghe xong bài hát là tự động cập nhật vào pipeline. My wrap-up: Tự thực hành phân tích dữ liệu như tính năng wrap-up của spotify. Recommender system: Thực hành làm một hệ thống gợi ý dựa trên những bài đã nghe.  ","date":"2022-12-18","permalink":"/projects/fde_project/","section":"Projects","summary":"Xây dựng một data pipeline ELT để ingest dữ liệu từ MySQL và API đơn giản từ Spotify (batch ingestion)","title":"Spotify data pipeline ingestion"},{"content":"Xin chào! Mình là lelouvincx, bình thường mọi người gọi là Chính, tác giả của blog này. Bạn có thể gọi mình theo cả hai cách đều được.   Nếu gặp mình ngoài đời, bạn có thể sẽ cảm thấy thất vọng một chút. Mình không giỏi giao tiếp và duy trì mood cho cuộc trò chuyện. Đừng hiểu nhầm! Mình thích giao tiếp, chia sẻ thông tin, nhưng mình không giỏi làm nó. Vậy nên mình viết blog này để truyền đạt tri thức, tinh thần và đam mê của bản thân. Bạn có thể xem mỗi bài viết như một cuộc trò chuyện giữa bạn và mình.\nHiện tại mình là sinh viên trường Đại học Khoa học Tự nhiên HCM, ngành khoa học dữ liệu. Mình thích xây dựng và tối ưu hóa hệ thống, ứng dụng, cuộc sống, vân vân cứ cái gì có thể tối ưu được là mình thích. Vì vậy mình thường dành thời gian tìm hiểu, build, đập đi build lại x3.14 (thường xuyên tới 2, 3 giờ sáng) cho tới khi đạt được mong muốn (hoặc lười không làm nữa).\n Về blog lelouvincx #  Đây là một blog về bản thân mình.\nBạn có thể tìm thấy ở đây kiến thức về data science, dev, productivity, tản mạn, đôi khi là review sách, nhạc, tranh, \u0026hellip; Bên cạnh đó là những kinh nghiệm mình học được trên con đường mình trưởng thành, những câu chuyện ý nghĩa mình gặp trong cuộc sống hằng ngày.\nPhải thú thật là mình rất thích viết, nhưng đồng thời cũng rất lười. Lúc nhỏ, mình từng có một blog trên spiderum chuyên chém gió về vài vấn đề tuổi teen. Bẵng qua một thời gian, mình lên tỉnh học cấp 3 chuyên, blog đó cũng dần bị bỏ ngỏ vì lười. Bước chân lên đại học, mình cũng hùng hổ lập blog mới, đòi tự build lại từ đầu này nọ, nhưng rồi cũng bỏ dở vì lười. Rồi mình nhận ra những gì mình cần làm chỉ là viết thôi. Còn những thứ khác, để sau hãy tính (không phải lười đâu nhé =))).\n Về bản thân mình #  Mình tốt nghiệp cấp 3 ở trường THPT Chuyên Hùng Vương - Gia Lai, khóa 18 - 21.   Suốt những năm cấp 3, mình điều hành một câu lạc bộ về tranh biện và tổ chức hội nghị MUN (Model United Nations) cho học sinh trong tỉnh.   Mình cũng là thành viên của đội tuyển Tin học quốc gia ở trường.   Hiện tại mình là sinh viên năm hai, khóa K21 trường đại học KHTN HCM.\n Những thứ mình làm #  Chủ yếu là những thứ mình giúp ích cho cuộc sống của mình. Bạn có thể xem thêm ở trang project và github của mình.\n Kết nối với mình #   Email (active hàng ngày) Trò chuyện trực tiếp với mình Nhắn tin qua discord (active thường xuyên)   Q\u0026amp;A #  ","date":"0001-01-01","permalink":"/about/","section":"About me","summary":"Xin chào! Mình là lelouvincx, bình thường mọi người gọi là Chính, tác giả của blog này. Bạn có thể gọi mình theo cả hai cách đều được.   Nếu gặp mình ngoài đời, bạn có thể sẽ cảm thấy thất vọng một chút.","title":"About me"},{"content":"","date":"0001-01-01","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":" 1. Tại sao nên đọc bài này? #  Chuyện là mình vừa đọc được một bài rất tâm huyết về các zsh frameworks trên Reddit, trong đó có đoạn viết như này:\n They took the best ideas, and wrote them as brand new modules with even higher-quality code, and ended up becoming by far the fastest \u0026ldquo;full-featured zsh framework\u0026rdquo; with the fastest startup time, most efficient Git status prompt updates, etc. Its clean code organization ideas were inspired by Prezto, and some of the modules (such as the git module) were taken from Prezto and cleaned up and refactored to be faster and better. Most modules were written from scratch. The entire project is very minimalistic with super clean, tiny code modules, and it\u0026rsquo;s well-organized within the \u0026ldquo;ZimFW\u0026rdquo; organization on GitHub.\n Tạm dịch: Họ chắt lọc những ý tưởng tốt nhất, viết lại những modules hoàn toàn mới với code chất lượng cao, tạo thành một zsh framework đầy đủ tính năng, với thời gian khởi động nhanh nhất, nhiều updates hiệu quả nhất, etc. Cấu trúc code sạch đẹp được truyền cảm hứng bởi Prezto, và một vài modules (ví dụ như git module) lấy từ Prezto được clean và refactor. Hầu hết modules được viết lại từ đầu. Toàn bộ project đều tối giản với modules siêu sạch, gọn, nằm trong \u0026ldquo;ZimFW\u0026rdquo; organization trên GitHub. (\u0026hellip;)\n   Wow, cũng đáng để thử đấy chứ! Cộng thêm việc mình đã quá ngán với con zsh của mình khi lúc nào bật lên cũng tốn ~0.5 - 1 giây (quá chậm cho 1 shell), vậy là quá đủ lý do để từ bỏ oh-my-zsh. Và thế là công cuộc chuyển nhà từ oh-my-zsh của mình sang Zim bắt đầu.\n   zimfw/zimfw   Zim: Modular, customizable, and blazing fast Zsh framework  HTML     3066     173      2. Cài đặt #  Đầu tiên bạn cần gỡ bỏ oh-my-zsh trước:\nsource ~/.oh-my-zsh/tools/uninstall.sh Sau đấy khởi động lại terminal, zsh sẽ trống trơn. Tiếp theo, cài Zim bằng curl:\ncurl -fsSL https://raw.githubusercontent.com/zimfw/install/master/install.zsh | zsh Hoặc bằng wget:\nwget -nv -O - https://raw.githubusercontent.com/zimfw/install/master/install.zsh | zsh Nếu không thành công, hãy thử cài thủ công ở link này. Khởi động lại terminal và kiểm tra Zim đã được cài thành công chưa:\nzimfw  Zim themes #  Theme mặc định của Zim là asciiship, bạn có thể chọn themes được Zim build sẵn hoặc theme khác tùy ý. Ví dụ ở đây mình sẽ dùng theme eriner, vào file ~/.zimrc, comment dòng zmodule asciiship và thêm zmodule enriner:\n# ~/.zimrc # Theme enriner # zmodule asciiship zmodule eriner Sau đó chạy lệnh:\nzimfw uninstall \u0026amp;\u0026amp; zimfw install Khởi động lại terminal và enjoy!\n   Như vậy tới đây bạn đã biết cách thêm 1 zsh plugin vào bằng Zim: thêm zmodule + tên plugin vào file ~/.zimrc, rồi chạy lệnh zimfw install.\nCác module được cài nằm ở ~/.zim/modules\n  Nhưng vì mình quen dùng powerlevel10k rồi nên mình sẽ cài powerlevel10k:\n# ~/.zimrc # ... config modules zmodule romkatv/powerlevel10k zimfw install, khởi động lại terminal rồi p10k configure.\n   p10k sequence ưa thích của mình là y y y n 3 1 3 4 4 5 2 3 4 2 2 1 1 y 2 y  Done! Khởi động lại terminal và enjoy.    Modules hỗ trợ #  Mặc định Zim đã cài sẵn cho bạn vài plugin như git, zsh-autosuggestions, zsh-syntax-highlighting, etc. Bạn có thể dùng luôn mà không cần config gì thêm. Nhưng ở đây mình muốn shell của mình được việc hơn tí nên sẽ cài thêm zoxide và zsh-autoswitch-virtualenv.\nZoxide là một tool tự động nhớ các đường dẫn mà bạn đã cd, giúp bạn di chuyển nhan hơn trong terminal.    ajeetdsouza/zoxide   A smarter cd command. Supports all major shells.  Rust     10176     364     Nếu chưa có zoxide, hãy cài nó vào trước:\n# Ubuntu sudo apt-get update sudo apt-get install zoxide Thêm dòng này vào ~/.zshrc:\n# Zoxide eval \u0026#34;$(zoxide init zsh)\u0026#34; Sau đó cài module vào ~/.zimrc:\n# ~/.zimrc # ... config modules # Zoxide zmodule kiesman99/zim-zoxide Sau đấy thì zimfw install thôi :D   zsh-autoswitch-virtualenv là tool giúp tự activate/deactivate khi bạn cd vào project có virtualenv của python. Nó hỗ trợ nhiều loại cho bạn như venv, pipenv, poetry bằng cách nhận diện một trong các file sau (xem thêm ở GitHub):    MichaelAquilina/zsh-autoswitch-virtualenv   🐍 ZSH plugin to automatically switch python virtualenvs (including pipenv and poetry) as you move between directories  Shell     384     66    \n setup.py requirements.txt Pipfile poetry.lock  Các bước cài cũng như trước, thêm đoạn dưới vào cuối file ~/.zimrc rồi zimfw install:\n# ~/.zimrc # ... config modules # Auto-switch python virtualenv zmodule MichaelAquilina/zsh-autoswitch-virtualenv Đến đây module đã được download về ~/.zim/modules, nhưng Zim chưa source được nó vì trong module không có file init.zsh. Để khắc phục, vào ~/.zim/modules/zsh-autoswitch-virtualenv chạy các lệnh\ntouch init.zsh cp autoswitch-virtualenv.plugin.zsh init.zsh zimfw build Kiểm tra file ~/.zim/init.zsh có module này là OK.   Module sẽ tự động activate/deactivate khi ra/vào project python có virtualenv    Một vài config khác #  Một vài config khác giúp mình làm việc nhanh và hiệu quả hơn:\n Source Ruby với rvm, khi dùng chỉ cần gõ source_ruby:  # ~/.zshrc # Add RVM to PATH for scripting. Make sure this is the last PATH variable change. export PATH=\u0026#34;$PATH:$HOME/.rvm/bin\u0026#34; alias source_ruby=\u0026#34;source $HOME/.rvm/scripts/rvm\u0026#34;  Dùng exa thay cho ls, exa nhanh và cho màu đẹp hơn ls:     ogham/exa   A modern replacement for ‘ls’.  Rust     21403     626     # ~/.zshrc # Use exa instead of ls alias ls=\u0026#34;exa\u0026#34; alias la=\u0026#34;exa -a\u0026#34; alias ll=\u0026#34;exa -l\u0026#34;  Dùng bat thay cho cat:  # ~/.zshrc # Use bat instead for cat alias cat=\u0026#34;bat\u0026#34;  Shortcut cho tmux:  # ~/.zshrc # Map tmux shortcut alias ta=\u0026#34;tmux a\u0026#34; alias tl=\u0026#34;tmux ls\u0026#34;  Kết luận #  Ưu điểm:\n Zim cho mình tốc độ nhanh hơn hẳn oh-my-zsh từng dùng, mở cái là lên liền chứ không cần đợi như oh-my-zsh Dễ dùng, chỉ cần nhớ vài câu lệnh như zimfw install, uninstall, compile, build là được  Nhược điểm:\n Cần một chút thời gian thích nghi Phải cần file init.zsh trong mỗi module để load vào shell  Một framework tối giản, thú vị. Mình sẽ tìm hiểu sâu hơn và đưa ra các updates sau này.\n","date":"0001-01-01","permalink":"/posts/oh-my-zsh-to-zim/","section":"Posts","summary":"Hướng dẫn cách chuyển từ oh-my-zsh sang zim và bước đầu làm quen với framework này","title":"Chuyển nhà từ oh-my-zsh sang zimfw"},{"content":" 1. Tại sao nên đọc bài này? #  Với những ai yêu thích Neovim và sử dụng nó làm IDE chính, bạn sẽ đôi lúc gặp vấn đề khó chịu khi code Ruby trên Neovim như thiếu suggestion, lint, format, \u0026hellip; Nhân dịp năm mới 2023, mình sẽ hướng dẫn các bạn cách setup môi trường phát triển Ruby bằng Neovim sử dụng LSP Solargraph.\nSolargraph là một language server cung cấp IntelliSense, code completion và docs cho Ruby. Nó hỗ trợ nhiều IDE và text editor như VSCode, Emacs, Atom, etc và dĩ nhiên, Neovim. Trang chủ của solargraph.\n 2. Yêu cầu #  Để setup bạn cần config trước những thứ sau:\n Ruby cài và chạy được Neovim version 0.5.0, đã cài mason và config LSP   3. Cài đặt #  Đầu tiên bạn cần cài gem solargraph:\ngem install solargraph Kiểm tra solargraph đã được cài chưa:\ngem list | grep solargraph # Nếu chưa thì dùng command sau gem install --user-install solargraph Hoặc thêm dòng sau vào Gemfile:\ngem \u0026#39;solargraph\u0026#39;, group: :development Tiếp theo vào Neovim và mở mason: :Mason, tìm và cài 2 gói sau: solargraph và rubocop.\nNếu có config LSP theo dotfiles của mình, bạn chỉ cần thêm đoạn sau vào ~/config/nvim/lua/$USER/plugins/lsp/lspconfig.lua:\n-- ... -- configure ruby language server lspconfig[\u0026#34;solargraph\u0026#34;].setup({ capabilities = capabilities, on_attach = on_attach, filetypes = { \u0026#34;ruby\u0026#34; }, }) -- ... Xong! Bây giờ chỉ cần mở file Ruby và check :LspInfo, lúc này sẽ hiện 1 client có tên solargraph được attach vào buffer .rb:\n   ","date":"0001-01-01","permalink":"/posts/neovim-setup-ruby/","section":"Posts","summary":"Hướng dẫn setup môi trường phát triển Ruby trên Neovim","title":"Code Ruby trên Neovim"},{"content":"","date":"0001-01-01","permalink":"/categories/linux/","section":"Categories","summary":"","title":"linux"},{"content":" \u0026ndash;\u0026gt;\n --  ","date":"0001-01-01","permalink":"/posts/","section":"Posts","summary":" \u0026ndash;\u0026gt;\n --  ","title":"Posts"},{"content":"","date":"0001-01-01","permalink":"/resources/","section":"Resources","summary":"","title":"Resources"},{"content":"","date":"0001-01-01","permalink":"/tags/ruby/","section":"Tags","summary":"","title":"ruby"},{"content":"","date":"0001-01-01","permalink":"/series/","section":"Series","summary":"","title":"Series"},{"content":"","date":"0001-01-01","permalink":"/categories/vim/","section":"Categories","summary":"","title":"vim"},{"content":"Cập nhật ngày 05/01/2023, Việt Nam.\nHành trình vạn dặm khởi đầu bằng bước chân đầu tiên. ~ Lão Tử\n  Học #  Mình đang học năm hai, trường Đại học Khoa học Tự nhiên TP. HCM, ngành Khoa Học Dữ Liệu.\nTrong một dự án, mình bỗng dưng \u0026ldquo;fall in love\u0026rdquo; với Ruby. Mình đang học OOP của Ruby song song với Rails.\nĐang cải thiện trình độ SQL và Python bằng việc làm project và Hackerrank hàng ngày. Ngoài ra, mình cũng có niềm yêu thích đặc biệt với an toàn thông tin (cybersecurity).\n Làm #  Vài project nhỏ mà mình và bạn mình nổi hứng nghĩ ra:\n Spotify data pipeline ingestion: Data pipeline đơn giản theo hướng ELT, để ingest dữ liệu từ MySQL và Spotify API (batch ingestion). Ubunchuu Trường Ú: Một project nhỏ với mục tiêu đem lại giúp các bạn sinh viên tiếp cận dễ hơn với linux (ubuntu). Tự design và implement một RDBMS đơn giản (với B-Tree).   Đọc #   Goodreads của mình   Big Data: Công nghệ cốt lõi trong kỷ nguyên số - Thomas Davenport Atomic Habits - James Clear  ","date":"0001-01-01","permalink":"/now/","section":"What I'm doing now","summary":"Cập nhật ngày 05/01/2023, Việt Nam.\nHành trình vạn dặm khởi đầu bằng bước chân đầu tiên. ~ Lão Tử\n  Học #  Mình đang học năm hai, trường Đại học Khoa học Tự nhiên TP.","title":"What I'm doing now"},{"content":"","date":"0001-01-01","permalink":"/tags/zimfw/","section":"Tags","summary":"","title":"zimfw"},{"content":"","date":"0001-01-01","permalink":"/tags/zsh/","section":"Tags","summary":"","title":"zsh"}]