[{"content":"","date":"2022-12-18","permalink":"/tags/data-engineering/","section":"Tags","summary":"","title":"data engineering"},{"content":"","date":"2022-12-18","permalink":"/","section":"lelouvincx's blog","summary":"","title":"lelouvincx's blog"},{"content":"","date":"2022-12-18","permalink":"/tags/project/","section":"Tags","summary":"","title":"project"},{"content":"","date":"2022-12-18","permalink":"/projects/","section":"Projects","summary":"","title":"Projects"},{"content":"1. Introduction\u003e 1. Introduction # Trong project nÃ y mÃ¬nh sáº½ hÆ°á»›ng dáº«n xÃ¢y dá»±ng má»™t data pipeline cÆ¡ báº£n theo mÃ´ hÃ¬nh ELT (extract - load - transform), sá»­ dá»¥ng bá»™ dá»¯ liá»‡u tá»« spotify Ä‘á»ƒ phÃ¢n tÃ­ch xu hÆ°á»›ng nghe nháº¡c.\nProject nÃ y hoÃ n thÃ nh dá»±a trÃªn kiáº¿n thá»©c Ä‘Ã£ há»c Ä‘Æ°á»£c tá»« khÃ³a Fundamental Data Engineering cá»§a AIDE. Xin gá»­i lá»i cáº£m Æ¡n Ä‘áº·c biá»‡t tá»›i tháº§y Nguyá»…n Thanh BÃ¬nh, anh Ã”ng XuÃ¢n Há»“ng vÃ  anh HÃ¹ng LÃª.\nSource code cá»§a project.\n2. Objective\u003e 2. Objective # Má»¥c tiÃªu cá»§a project nÃ y lÃ  xÃ¢y dá»±ng má»™t data pipeline Ä‘á»ƒ Ä‘Æ°a dá»¯ liá»‡u cá»§a báº£ng spotify_tracks tá»« mySQL vÃ  my_tracks tá»« API cá»§a Spotify thÃ nh dashboard Ä‘á»ƒ phÃ¢n tÃ­ch. CÃ¡c báº£ng Ä‘Æ°á»£c há»— trá»£ bá»Ÿi spotify_albums vÃ  spotify_artists nhÆ° mÃ´ táº£ dÆ°á»›i Ä‘Ã¢y:\nspotify_tracks: OLTP table chá»©a thÃ´ng tin bÃ i hÃ¡t tá»« spotify my_tracks: lá»‹ch sá»­ stream nháº¡c cá»§a báº£n thÃ¢n, láº¥y schema giá»‘ng spotify_tracks spotify_albums: chá»©a thÃ´ng tin albums tá»« dataset spotify_artists: thÃ´ng tin nghá»‡ sÄ© tá»« dataset table schema Chi tiáº¿t hÆ¡n xem á»Ÿ: Exploratory Data Analysis\n3. Design\u003e 3. Design # 3.1 Pipeline design\u003e 3.1 Pipeline design # ChÃºng ta sá»­ dá»¥ng mÃ¡y áº£o AWS EC2 Ä‘á»ƒ tÃ­nh toÃ¡n vÃ  dagster Ä‘á»ƒ orchestrate tasks.\nDá»¯ liá»‡u spotify Ä‘Æ°á»£c download tá»« kaggle dÆ°á»›i dáº¡ng csv, sau Ä‘Ã³ import vÃ o mySQL mÃ´ phá»ng lÃ m dá»¯ liá»‡u doanh nghiá»‡p Dá»¯ liá»‡u streaming history cá»§a báº£n thÃ¢n Ä‘Æ°á»£c extract tá»« spotify API Extract 2 nguá»“n dá»¯ liá»‡u trÃªn báº±ng pandas Ä‘á»ƒ preprocessing (optimize size consumed) Load vÃ o Amazon S3, tá»« Ä‘Ã³ load tiáº¿p vÃ o data warehouse (PostgreSQL) Ä‘á»ƒ lÃ m analytics Transform dá»¯ liá»‡u báº±ng dbt trÃªn ná»n PostgreSQL Trá»±c quan hÃ³a dá»¯ liá»‡u báº±ng Metabase ELT pipeline design 3.2 Data lake structure\u003e 3.2 Data lake structure # ChÃºng ta sá»­ dá»¥ng AWS S3 lÃ m data lake. Má»i dá»¯ liá»‡u trÆ°á»›c háº¿t sáº½ Ä‘Æ°á»£c chá»©a á»Ÿ Ä‘Ã¢y. Trong project nÃ y, ta chá»‰ cáº§n 1 bucket vá»›i nhiá»u thÆ° má»¥c.\nBronze: LÆ°u dá»¯ liá»‡u thÃ´ má»›i láº¥y vá». ChÃºng lÃ  step 1, 2, 3 trong pipeline design Silver: LÆ°u dá»¯ liá»‡u Ä‘Æ°á»£c tiá»n xá»­ lÃ½. ChÃºng lÃ  step 4 trong pipeline design Gold: LÆ°u dá»¯ liá»‡u sáº¡ch khi transform báº±ng dbt (step 5) structure of S3 3.3 Directory tree\u003e 3.3 Directory tree # docker-compose: compose cÃ¡c container cháº¡y trong docker EDA: khÃ¡m phÃ¡ dataset vÃ  profiling .gitignore: giÃºp git khÃ´ng track file (nhÆ° env, cache, \u0026hellip;) .gitlab-ci: config quÃ¡ trÃ¬nh CI trÃªn gitlab Makefile: thu gá»n cÃ¢u lá»‡nh requirements.txt: packages python cáº§n thiáº¿t vÃ  thiáº¿t láº­p virtualenv Folder dagser_home chá»©a dagster.yaml Ä‘á»ƒ config thÃ nh pháº§n dagster cÃ²n workspace.yaml Ä‘á»ƒ chá»‰ Ä‘á»‹nh dagster cháº¡y host elt_pipeline Folder dockers chá»©a file config cÃ¡c container: dagster vÃ  jupyter Folder load_dataset chá»©a cÃ¡c file dÃ¹ng Ä‘á»ƒ load dá»¯ liá»‡u ban Ä‘áº§u vÃ o mySQL Folder terraform Ä‘á»ƒ khá»Ÿi táº¡o vÃ  config server trÃªn AWS Chi tiáº¿t cÃ¢y thÆ° má»¥c xem á»Ÿ: directory tree\n4. Setup\u003e 4. Setup # 4.1 Prequisites\u003e 4.1 Prequisites # Äá»ƒ sá»­ dá»¥ng pipeline nÃ y, download nhá»¯ng pháº§n má»m sau:\nGit TÃ i khoáº£n gitlab Terraform TÃ i khoáº£n AWS AWS CLI vÃ  configure Docker Ã­t nháº¥t 4GB RAM vÃ  Docker Compose Ã­t nháº¥t v1.29.2 Náº¿u dÃ¹ng Windows, setup thÃªm WSL vÃ  má»™t mÃ¡y áº£o local Ubuntu, cÃ i Ä‘áº·t nhá»¯ng thá»© trÃªn cho ubuntu.\n4.2 Setup data infrastructure local\u003e 4.2 Setup data infrastructure local # Clone repository:\ngit clone https://gitlab.com/lelouvincx/fde01_project_fde220103_dinhminhchinh.git mv fde01_project_fde220103_dinhminhchinh project cd project # Create env file touch env cp env.template env Äiá»n cÃ¡c biáº¿n mÃ´i trÆ°á»ng vÃ o file env.\nCháº¡y cÃ¡c lá»‡nh sau Ä‘á»ƒ setup infra dÆ°á»›i local:\n# Setup python packages make install # Build docker images make build # Run locally make up # Check running containers docker ps # Check code quality make check make lint # Use black to reformat if any tests failed, then try again black ./elt_pipeline # Test coverage make test LÃºc nÃ y sáº½ cÃ³ 7 services sau Ä‘ang cháº¡y: running services BÃ¢y giá» chÃºng ta import dataset spotify (dáº¡ng csv) vÃ o mySQL:\n# Enter mysql cli make to_mysql SET GLOBAL local_infile=true; -- Check if local_infile is turned on SHOW VARIABLES LIKE \u0026#34;local_infile\u0026#34;; exit Source tá»«ng file theo thá»© tá»±:\n# Create tables with schema make mysql_create # Load csv into created tables make mysql_load # Set their foreign keys make mysql_set_foreign_key Khá»Ÿi táº¡o schema vÃ  table trong PostgreSQL:\n# Enter psql cli make psql_create Testing:\n# Test utils python3 -m pytest -vv --cov=utils elt_pipeline/tests/utils # Test ops python3 -m pytest -vv --cov=ops elt_pipeline/tests/ops Truy cáº­p giao diá»‡n cá»§a pipeline báº±ng dagit: https://localhost:3001/\n4.3 Setup data infrastructure on AWS\u003e 4.3 Setup data infrastructure on AWS # ChÃºng ta dÃ¹ng terraform lÃ m IaC (Infrastructure as Code) Ä‘á»ƒ setup háº¡ táº§ng trÃªn AWS (nhá»› cáº¥p credential key cho AWS nhÃ©) cd terraform # Initialize infra make tf-init # Checkout planned infra make tf-plan # Build up infra make tf-up Äá»£i má»™t chÃºt Ä‘á»ƒ setup xong. ChÃºng ta lÃªn Amazon Web Services\nTrong EC2 sáº½ tháº¥y 1 mÃ¡y áº£o tÃªn project-spotify-EC2 Trong S3 tháº¥y 1 bucket tÃªn project-spotify-bucket Sau khi project-spotify-EC2 hiá»‡n Ä‘Ã£ pass háº¿t status thÃ¬ chÃºng ta Ä‘Ã£ setup thÃ nh cÃ´ng.\nBÃ¢y giá» chÃºng ta truy cáº­p vÃ o EC2 Ä‘á»ƒ hoÃ n táº¥t setup # Connect to EC2 from local terminal make ssh-ec2 # Generate new ssh key for gitlab ssh-keygen # Then press Enter until done cat ~/.ssh/id_rsa.pub Copy Ä‘oáº¡n mÃ£ SSH VÃ o gitlab, phÃ­a trÃªn gÃ³c pháº£i cÃ³ hÃ¬nh avatar -\u0026gt; Preferences -\u0026gt; SSH Keys -\u0026gt; paste key vá»«a copy vÃ o -\u0026gt; Ä‘áº·t tÃªn lÃ  \u0026lsquo;project-spotify-vm\u0026rsquo; -\u0026gt; Add key VÃ o terminal cá»§a EC2 (vá»«a connect lÃºc nÃ£y), clone vá» báº±ng SSH Láº·p láº¡i bÆ°á»›c setup infra local Ä‘Ã£ trÃ¬nh bÃ y á»Ÿ pháº§n trÃªn 5. Detailed code walkthrough\u003e 5. Detailed code walkthrough # ELT pipeline gá»“m 2 job cháº¡y 2 tÃ¡c vá»¥ Ä‘á»™c láº­p: EL data tá»« MySQL vÃ  EL data tá»« API nhÆ°ng nhÃ¬n chung chÃºng cÃ³ cáº¥u trÃºc giá»‘ng nhau. Cá»¥ thá»ƒ:\nextractdata_from{mysql/api}: Láº¥y data tá»« MySQL hoáº·c api (thÃ´ng qua access token) vÃ  lÆ°u táº¡m dÆ°á»›i dáº¡ng pandas.DataFrame. TÃ¹y theo chiáº¿n lÆ°á»£c ingest data (full load/incremental by partition/incremental by watermark) mÃ  cÃ³ cÃ¡ch giáº£i quyáº¿t phÃ¹ há»£p. load_data_to_s3: Tiá»n xá»­ lÃ½ data types cho DataFrame tá»« upstream vÃ  load vÃ o S3 dÆ°á»›i dáº¡ng parquet. load_data_to_psql: Extract data dáº¡ng parquet trong S3 thÃ nh pandas.DataFrame vÃ  load vÃ o PostgreSQL. Äá»ƒ dá»¯ liá»‡u Ä‘Æ°á»£c toÃ n váº¹n (khÃ´ng bá»‹ crash, lá»—i Ä‘Æ°á»ng truyá»n) trong quÃ¡ trÃ¬nh crash, ta táº¡o TEMP TABLE vÃ  load vÃ o Ä‘Ã³ trÆ°á»›c. validate_{mssql2psql/api2psql}_ingestion: Tháº©m Ä‘á»‹nh 3 step trÃªn Ä‘Ã£ Ä‘Æ°á»£c EL thÃ nh cÃ´ng hay chÆ°a trigger_dbt_spotify: Sensor Ä‘á»ƒ trigger dbt nháº±m transform data. job_mssql2psql_ingestion job_api2psql_ingestion 5.1 Extract\u003e 5.1 Extract # Láº¥y data tá»« MySQL hoáº·c api (thÃ´ng qua access token) vÃ  lÆ°u táº¡m dÆ°á»›i dáº¡ng pandas.DataFrame. TÃ¹y theo chiáº¿n lÆ°á»£c ingest data (full load/incremental by partition/incremental by watermark) mÃ  cÃ³ cÃ¡ch giáº£i quyáº¿t phÃ¹ há»£p.\nTa Ä‘á»‹nh nghÄ©a phÆ°Æ¡ng thá»©c extract data cá»§a mysql vÃ  api trong thÆ° má»¥c utils.\nutils/mysql_loader/extract def extract_data(self, sql: str) -\u0026gt; pd.DataFrame: pd_data = None with self.get_db_connection() as db_conn: pd_data = pd.read_sql(sql, db_conn) return pd_data utils/api_loader/get_recently def get_recently(self, number: int, token: str) -\u0026gt; (int, dict): headers = { \u0026#34;Accept\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer \u0026#34; + token, } params = [(\u0026#34;limit\u0026#34;, number),] try: response = requests.get( \u0026#34;https://api.spotify.com/v1/me/player/recently-played\u0026#34;, headers=headers, params=params, timeout=10, ) return (response.status_code, response.json()) except: return None utils/api_loader/extract def extract_data(self, token: str) -\u0026gt; pd.DataFrame: (code, content) = self.get_recently(50, token) my_tracks = { \u0026#34;album_id\u0026#34;: [], \u0026#34;artists_id\u0026#34;: [], \u0026#34;track_id\u0026#34;: [], \u0026#34;track_unique_id\u0026#34;: [], \u0026#34;name\u0026#34;: [], \u0026#34;popularity\u0026#34;: [], \u0026#34;type\u0026#34;: [], \u0026#34;duration_ms\u0026#34;: [], \u0026#34;played_at\u0026#34;: [], \u0026#34;danceability\u0026#34;: [], \u0026#34;energy\u0026#34;: [], \u0026#34;track_key\u0026#34;: [], \u0026#34;loudness\u0026#34;: [], \u0026#34;mode\u0026#34;: [], \u0026#34;speechiness\u0026#34;: [], \u0026#34;acousticness\u0026#34;: [], \u0026#34;instrumentalness\u0026#34;: [], \u0026#34;liveness\u0026#34;: [], \u0026#34;valence\u0026#34;: [], \u0026#34;tempo\u0026#34;: [], } items = content.get(\u0026#34;items\u0026#34;, []) for item in items: # Take album_id, artists_id, track_id, name, popularity, type, duration_ms played_at = item.get(\u0026#34;played_at\u0026#34;, []) track = item.get(\u0026#34;track\u0026#34;, []) album = track.get(\u0026#34;album\u0026#34;, []) album_id = album.get(\u0026#34;id\u0026#34;, []) artists = track.get(\u0026#34;artists\u0026#34;, []) artists_id = [] for artist in artists: artists_id.append(artist.get(\u0026#34;id\u0026#34;, [])) track_id = track.get(\u0026#34;id\u0026#34;, []) name = track.get(\u0026#34;name\u0026#34;, []) popularity = track.get(\u0026#34;popularity\u0026#34;, []) type = track.get(\u0026#34;type\u0026#34;, []) duration_ms = track.get(\u0026#34;duration_ms\u0026#34;, []) # Take features features = self.get_features(track_id, token) danceability = features.get(\u0026#34;danceability\u0026#34;, []) energy = features.get(\u0026#34;energy\u0026#34;, []) track_key = features.get(\u0026#34;key\u0026#34;, []) loudness = features.get(\u0026#34;loudness\u0026#34;, []) mode = features.get(\u0026#34;mode\u0026#34;, []) speechiness = features.get(\u0026#34;speechiness\u0026#34;, []) acousticness = features.get(\u0026#34;acousticness\u0026#34;, []) instrumentalness = features.get(\u0026#34;instrumentalness\u0026#34;, []) liveness = features.get(\u0026#34;liveness\u0026#34;, []) valence = features.get(\u0026#34;valence\u0026#34;, []) tempo = features.get(\u0026#34;tempo\u0026#34;, []) # Extract row into dict my_tracks[\u0026#34;album_id\u0026#34;].append(album_id) my_tracks[\u0026#34;artists_id\u0026#34;].append(artists_id) my_tracks[\u0026#34;track_id\u0026#34;].append(track_id) my_tracks[\u0026#34;track_unique_id\u0026#34;].append(track_id + played_at) my_tracks[\u0026#34;name\u0026#34;].append(name) my_tracks[\u0026#34;popularity\u0026#34;].append(popularity) my_tracks[\u0026#34;type\u0026#34;].append(type) my_tracks[\u0026#34;duration_ms\u0026#34;].append(duration_ms) my_tracks[\u0026#34;played_at\u0026#34;].append(played_at[:10]) my_tracks[\u0026#34;danceability\u0026#34;].append(danceability) my_tracks[\u0026#34;energy\u0026#34;].append(energy) my_tracks[\u0026#34;track_key\u0026#34;].append(track_key) my_tracks[\u0026#34;loudness\u0026#34;].append(loudness) my_tracks[\u0026#34;mode\u0026#34;].append(mode) my_tracks[\u0026#34;speechiness\u0026#34;].append(speechiness) my_tracks[\u0026#34;acousticness\u0026#34;].append(acousticness) my_tracks[\u0026#34;instrumentalness\u0026#34;].append(instrumentalness) my_tracks[\u0026#34;liveness\u0026#34;].append(liveness) my_tracks[\u0026#34;valence\u0026#34;].append(valence) my_tracks[\u0026#34;tempo\u0026#34;].append(tempo) pd_data = pd.DataFrame(my_tracks) return pd_data Giá» lÃ  lÃºc extract data.\nextract_data_from_mysql def extract_data_from_mysql(context, run_config): updated_at = context.op_config.get(\u0026#34;updated_at\u0026#34;) context.log.info(f\u0026#34;Updated at: {updated_at}\u0026#34;) if updated_at is None or updated_at == \u0026#34;\u0026#34;: context.log.info(\u0026#34;Nothing to do!\u0026#34;) return None context.log.info(f\u0026#34;Op extracts data from MySQL at {updated_at}\u0026#34;) # Choose extract strategy (default: full load) sql_stm = f\u0026#34;\u0026#34;\u0026#34; SELECT * FROM {run_config.get(\u0026#39;src_tbl\u0026#39;)} WHERE 1=1 \u0026#34;\u0026#34;\u0026#34; if run_config.get(\u0026#34;strategy\u0026#34;) == \u0026#34;incremental_by_partition\u0026#34;: if updated_at != \u0026#34;init_dump\u0026#34;: sql_stm += f\u0026#34;\u0026#34;\u0026#34; AND CAST({run_config.get(\u0026#39;partition\u0026#39;)} AS DATE) = \u0026#39;{updated_at}\u0026#39; \u0026#34;\u0026#34;\u0026#34; if run_config.get(\u0026#34;strategy\u0026#34;) == \u0026#34;incremental_by_watermark\u0026#34;: data_loader = get_data_loader( run_config.get(\u0026#34;db_provider\u0026#34;), run_config.get(\u0026#34;target_db_params\u0026#34;) ) watermark = data_loader.get_watermark( f\u0026#34;{run_config.get(\u0026#39;target_schema\u0026#39;)}.{run_config.get(\u0026#39;target_tbl\u0026#39;)}\u0026#34;, run_config.get(\u0026#34;watermark\u0026#34;), ) watermark = ( updated_at if watermark is None or watermark \u0026gt; updated_at else watermark ) if updated_at != \u0026#34;init_dump\u0026#34;: sql_stm += f\u0026#34;\u0026#34;\u0026#34; AND {run_config.get(\u0026#39;watermark\u0026#39;)} \u0026gt;= \u0026#39;{watermark}\u0026#39; \u0026#34;\u0026#34;\u0026#34; context.log.info(f\u0026#34;Extracting with SQL: {sql_stm}\u0026#34;) db_loader = MysqlLoader(run_config.get(\u0026#34;src_db_params\u0026#34;)) pd_data = db_loader.extract_data(sql_stm) context.log.info(f\u0026#34;Data extracted successfully with shape: {pd_data.shape}\u0026#34;) # Update params run_config.update( { \u0026#34;updated_at\u0026#34;: updated_at, \u0026#34;data\u0026#34;: pd_data, \u0026#34;s3_path\u0026#34;: f\u0026#34;bronze/{run_config.get(\u0026#39;data_source\u0026#39;)}/{run_config.get(\u0026#39;ls_target\u0026#39;).get(\u0026#39;target_tbl\u0026#39;)}\u0026#34;, \u0026#34;load_dtypes\u0026#34;: run_config.get(\u0026#34;load_dtypes\u0026#34;), } ) return run_config extract_data_from_api def extract_data_from_api(context, run_config): updated_at = context.op_config.get(\u0026#34;updated_at\u0026#34;) context.log.info(f\u0026#34;Updated at: {updated_at}\u0026#34;) if updated_at is None or updated_at == \u0026#34;\u0026#34;: context.log.info(\u0026#34;Nothing to do!\u0026#34;) return None context.log.info(f\u0026#34;Op extracts data from API at {updated_at}\u0026#34;) # Extract strategy (only support incremental_by_partition) context.log.info(f\u0026#34;Extracting on date: {updated_at}\u0026#34;) api_loader = ApiLoader(run_config.get(\u0026#34;src_api_params\u0026#34;)) token = api_loader.get_api_token() pd_data = api_loader.extract_data(token) index_played_at = pd_data[pd_data[\u0026#34;played_at\u0026#34;] != updated_at].index # Drop data pd_data.drop(index_played_at, inplace=True) context.log.info( f\u0026#34;Data loaded and filtered successfully with shape: {pd_data.shape}\u0026#34; ) run_config.update( { \u0026#34;updated_at\u0026#34;: updated_at, \u0026#34;data\u0026#34;: pd_data, \u0026#34;s3_path\u0026#34;: f\u0026#34;bronze/{run_config.get(\u0026#39;data_source\u0026#39;)}/{run_config.get(\u0026#39;ls_target\u0026#39;).get(\u0026#39;target_tbl\u0026#39;)}\u0026#34;, \u0026#34;load_dtypes\u0026#34;: run_config.get(\u0026#34;load_dtypes\u0026#34;), } ) return run_config 5.2 Load\u003e 5.2 Load # Tiá»n xá»­ lÃ½ data types cho DataFrame tá»« upstream vÃ  load vÃ o S3 dÆ°á»›i dáº¡ng parquet.\nload_data_to_s3 def load_data_to_s3(context, upstream): if upstream is None: return None updated_at = upstream.get(\u0026#34;updated_at\u0026#34;) s3_bucket = os.getenv(\u0026#34;DATALAKE_BUCKET\u0026#34;) if type(updated_at) == list: updated_at = max(updated_at) s3_file = f\u0026#34;s3://{s3_bucket}/{upstream.get(\u0026#39;s3_path\u0026#39;)}/updated_at={updated_at}\u0026#34; context.log.info(f\u0026#34;Loading data to S3: {s3_file}\u0026#34;) # Load data to S3 pd_data = upstream.get(\u0026#34;data\u0026#34;) # Preprocess data load_dtypes = upstream.get(\u0026#34;load_dtypes\u0026#34;) try: for col, data_type in load_dtypes.items(): if data_type == \u0026#34;str\u0026#34;: pd_data[col] = pd_data[col].fillna(\u0026#34;\u0026#34;) pd_data[col] = pd_data[col].astype(str) pd_data[col] = pd_data[col].str.strip() pd_data[col] = pd_data[col].str.rstrip() pd_data[col] = pd_data[col].str.replace(\u0026#34;\u0026#39;\u0026#34;, \u0026#34;\u0026#34;) pd_data[col] = pd_data[col].str.replace(\u0026#39;\u0026#34;\u0026#39;, \u0026#34;\u0026#34;) pd_data[col] = pd_data[col].str.replace(r\u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;, regex=True) elif data_type == \u0026#34;int\u0026#34;: cur_bit = np.log2(pd_data[col].max()) if cur_bit \u0026gt; 32: pd_data[col] = pd_data[col].astype({col: \u0026#34;int64\u0026#34;}) elif cur_bit \u0026gt; 16: pd_data[col] = pd_data[col].astype({col: \u0026#34;int32\u0026#34;}) elif cur_bit \u0026gt; 8: pd_data[col] = pd_data[col].astype({col: \u0026#34;int16\u0026#34;}) else: pd_data[col] = pd_data[col].astype({col: \u0026#34;int8\u0026#34;}) elif data_type == \u0026#34;float\u0026#34;: pd_data[col] = pd_data[col].astype({col: \u0026#34;float32\u0026#34;}) context.log.info(f\u0026#34;Data preprocessed successfully\u0026#34;) except Exception as e: context.log.info(f\u0026#34;Exception: {e}\u0026#34;) # Write parquet object to S3 pa_data = pa.Table.from_pandas(df=pd_data, preserve_index=False) pq.write_table(pa_data, s3_file) context.log.info(\u0026#34;Data loaded successfully to S3\u0026#34;) # Update stream upstream.update({\u0026#34;s3_bucket\u0026#34;: s3_bucket, \u0026#34;s3_file\u0026#34;: s3_file}) return upstream load_data_to_psql def load_data_to_psql(context, upstream): if upstream is None: return None # Load data to target context.log.info(\u0026#34;Loading data to postgreSQL\u0026#34;) context.log.info(f\u0026#34;Extracting data from {upstream.get(\u0026#39;s3_file\u0026#39;)}\u0026#34;) pd_stag = pd.read_parquet(upstream.get(\u0026#34;s3_file\u0026#34;)) context.log.info(f\u0026#34;Extracted data shape: {pd_stag.shape}\u0026#34;) if len(pd_stag) == 0: context.log.info(\u0026#34;No data to upload!\u0026#34;) return \u0026#34;No data to upload!\u0026#34; # Execute db_loader = PsqlLoader(upstream.get(\u0026#34;target_db_params\u0026#34;)) result = db_loader.load_data(pd_stag, upstream) context.log.info(f\u0026#34;Batch inserted status: {result}\u0026#34;) return result 5.3 Transform\u003e 5.3 Transform # Do dataset háº§u háº¿t Ä‘Ã£ Ä‘Æ°á»£c clean. ChÃºng ta chá»n cÃ¡c column cáº§n thiáº¿t cho viá»‡c visualization.\ncleaned_my_tracks.sql {{ config(materialized=\u0026#39;table\u0026#39;) }} with my_unique_tracks AS ( select * from spotify.my_tracks group by track_unique_id ) select * from my_unique_tracks cleaned_spotify_tracks.sql {{ config(materialized=\u0026#39;table\u0026#39;) }} with unique_tracks AS ( select acousticness, album_id, artists_id, country, danceability, duration_ms, energy, track_id, instrumentalness, track_key, liveness, loudness, mode, name, popularity, speechiness, tempo, valence from spotify.spotify_tracks group by track_id ) select * from unique_tracks cleaned_spotify_artists.sql {{ config(materialized=\u0026#39;table\u0026#39;) }} with unique_artists AS ( select artist_popularity, followers, genres, artist_id, name, track_id from spotify.spotify_artists group by artist_id ) select * from unique_artists cleaned_spotify_albums.sql {{ config(materialized=\u0026#39;table\u0026#39;) }} with unique_albums AS ( select album_id, artist_id, album_type, name, release_date, release_date_precision, total_tracks, track_id from spotify.spotify_albums group by album_id ) select * from unique_albums 5.4 Check results\u003e 5.4 Check results # Sau khi transform thÃ nh cÃ´ng, má»Ÿ dbeaver lÃªn vÃ  chÃºng ta tháº¥y cÃ¡c table Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ analytics.\n6. Tear down infrastructure\u003e 6. Tear down infrastructure # Dá»¡ bá» infra sau khi xong viá»‡c (thá»±c hiá»‡n dÆ°á»›i mÃ¡y local):\n# Tear down containers make down # Tear down AWS cd terraform make tf-down Note: LÃªn AWS kiá»ƒm tra láº¡i cÃ¡c services sau Ä‘Ã£ dá»«ng vÃ  bá»‹ xÃ³a chÆ°a (náº¿u khÃ´ng muá»‘n máº¥t tiá»n oan nhÆ° mÃ¬nh): EC2, S3. 7. Design considerations\u003e 7. Design considerations # Sau khi deploy thÃ nh cÃ´ng pipeline, giá» lÃ  lÃºc Ä‘Ã¡nh giÃ¡ project.\nTá»‘c Ä‘á»™: Tá»‘c Ä‘á»™ extract data khÃ¡ cháº­m (vÃ¬ load vÃ o pandas.DataFrame 2 láº§n). Má»™t sá»‘ giáº£i phÃ¡p thay tháº¿: polars, json, \u0026hellip; KÃ­ch thÆ°á»›c: Chuyá»‡n gÃ¬ sáº½ xáº£y ra khi data lá»›n lÃªn gáº¥p 10x, 100x, 1000x? LÃºc Ä‘áº¥y ta cáº§n xem xÃ©t cÃ¡c giáº£i phÃ¡p giÃºp lÆ°u trá»¯ big data, thay Ä‘á»•i data warehouse thÃ nh Amazon RDS, Google BigQuery, \u0026hellip; MÃ´i trÆ°á»ng phÃ¡t triá»ƒn: Khi project cÃ³ thÃªm nhiá»u ngÆ°á»i cÃ¹ng sá»­ dá»¥ng lÃ  cÅ©ng lÃ  lÃºc phÃ¢n chia mÃ´i trÆ°á»ng thÃ nh testing, staging, production. 8. Further actions\u003e 8. Further actions # TÄƒng lÆ°á»£ng data: TÃ­ch há»£p nhiá»u data hÆ¡n tá»« Spotify API: Khi ingest bÃ i hÃ¡t má»›i, ingest luÃ´n thÃ´ng tin vá» artist, album, táº¡o thÃ nh há»‡ sinh thÃ¡i bÃ i hÃ¡t Ä‘áº§y Ä‘á»§. Stream ingestion: DÃ¹ng má»™t tech stack khÃ¡c cho job API theo hÆ°á»›ng streaming. Há»‡ thá»‘ng sáº½ listen má»—i láº§n nghe xong bÃ i hÃ¡t lÃ  tá»± Ä‘á»™ng cáº­p nháº­t vÃ o pipeline. My wrap-up: Tá»± thá»±c hÃ nh phÃ¢n tÃ­ch dá»¯ liá»‡u nhÆ° tÃ­nh nÄƒng wrap-up cá»§a spotify. Recommender system: Thá»±c hÃ nh lÃ m má»™t há»‡ thá»‘ng gá»£i Ã½ dá»±a trÃªn nhá»¯ng bÃ i Ä‘Ã£ nghe. ","date":"2022-12-18","permalink":"/projects/fde_project/","section":"Projects","summary":"XÃ¢y dá»±ng má»™t data pipeline ELT Ä‘á»ƒ ingest dá»¯ liá»‡u tá»« MySQL vÃ  API Ä‘Æ¡n giáº£n tá»« Spotify (batch ingestion)","title":"Spotify data pipeline ingestion"},{"content":"","date":"2022-12-18","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"Xin chÃ o! MÃ¬nh lÃ  lelouvincx, bÃ¬nh thÆ°á»ng má»i ngÆ°á»i gá»i lÃ  ChÃ­nh, tÃ¡c giáº£ cá»§a blog nÃ y. Báº¡n cÃ³ thá»ƒ gá»i mÃ¬nh theo cáº£ hai cÃ¡ch Ä‘á»u Ä‘Æ°á»£c. Náº¿u gáº·p mÃ¬nh ngoÃ i Ä‘á»i, báº¡n cÃ³ thá»ƒ sáº½ cáº£m tháº¥y tháº¥t vá»ng má»™t chÃºt. MÃ¬nh khÃ´ng giá»i giao tiáº¿p vÃ  duy trÃ¬ mood cho cuá»™c trÃ² chuyá»‡n. Äá»«ng hiá»ƒu nháº§m! MÃ¬nh thÃ­ch giao tiáº¿p, chia sáº» thÃ´ng tin, nhÆ°ng mÃ¬nh khÃ´ng giá»i lÃ m nÃ³. Váº­y nÃªn mÃ¬nh viáº¿t blog nÃ y Ä‘á»ƒ truyá»n Ä‘áº¡t tri thá»©c, tinh tháº§n vÃ  Ä‘am mÃª cá»§a báº£n thÃ¢n. Báº¡n cÃ³ thá»ƒ xem má»—i bÃ i viáº¿t nhÆ° má»™t cuá»™c trÃ² chuyá»‡n giá»¯a báº¡n vÃ  mÃ¬nh.\nHiá»‡n táº¡i mÃ¬nh lÃ  sinh viÃªn trÆ°á»ng Äáº¡i há»c Khoa há»c Tá»± nhiÃªn HCM, ngÃ nh khoa há»c dá»¯ liá»‡u. MÃ¬nh thÃ­ch xÃ¢y dá»±ng vÃ  tá»‘i Æ°u hÃ³a há»‡ thá»‘ng, á»©ng dá»¥ng, cuá»™c sá»‘ng, vÃ¢n vÃ¢n cá»© cÃ¡i gÃ¬ cÃ³ thá»ƒ tá»‘i Æ°u Ä‘Æ°á»£c lÃ  mÃ¬nh thÃ­ch. VÃ¬ váº­y mÃ¬nh thÆ°á»ng dÃ nh thá»i gian tÃ¬m hiá»ƒu, build, Ä‘áº­p Ä‘i build láº¡i x3.14 (thÆ°á»ng xuyÃªn tá»›i 2, 3 giá» sÃ¡ng) cho tá»›i khi Ä‘áº¡t Ä‘Æ°á»£c mong muá»‘n (hoáº·c lÆ°á»i khÃ´ng lÃ m ná»¯a).\nVá» blog lelouvincx\u003e Vá» blog lelouvincx # ÄÃ¢y lÃ  má»™t blog vá» báº£n thÃ¢n mÃ¬nh.\nBáº¡n cÃ³ thá»ƒ tÃ¬m tháº¥y á»Ÿ Ä‘Ã¢y kiáº¿n thá»©c vá» data science, dev, productivity, táº£n máº¡n, Ä‘Ã´i khi lÃ  review sÃ¡ch, nháº¡c, tranh, \u0026hellip; BÃªn cáº¡nh Ä‘Ã³ lÃ  nhá»¯ng kinh nghiá»‡m mÃ¬nh há»c Ä‘Æ°á»£c trÃªn con Ä‘Æ°á»ng mÃ¬nh trÆ°á»Ÿng thÃ nh, nhá»¯ng cÃ¢u chuyá»‡n Ã½ nghÄ©a mÃ¬nh gáº·p trong cuá»™c sá»‘ng háº±ng ngÃ y.\nPháº£i thÃº tháº­t lÃ  mÃ¬nh ráº¥t thÃ­ch viáº¿t, nhÆ°ng Ä‘á»“ng thá»i cÅ©ng ráº¥t lÆ°á»i. LÃºc nhá», mÃ¬nh tá»«ng cÃ³ má»™t blog trÃªn spiderum chuyÃªn chÃ©m giÃ³ vá» vÃ i váº¥n Ä‘á» tuá»•i teen. Báºµng qua má»™t thá»i gian, mÃ¬nh lÃªn tá»‰nh há»c cáº¥p 3 chuyÃªn, blog Ä‘Ã³ cÅ©ng dáº§n bá»‹ bá» ngá» vÃ¬ lÆ°á»i. BÆ°á»›c chÃ¢n lÃªn Ä‘áº¡i há»c, mÃ¬nh cÅ©ng hÃ¹ng há»• láº­p blog má»›i, Ä‘Ã²i tá»± build láº¡i tá»« Ä‘áº§u nÃ y ná», nhÆ°ng rá»“i cÅ©ng bá» dá»Ÿ vÃ¬ lÆ°á»i. Rá»“i mÃ¬nh nháº­n ra nhá»¯ng gÃ¬ mÃ¬nh cáº§n lÃ m chá»‰ lÃ  viáº¿t thÃ´i. CÃ²n nhá»¯ng thá»© khÃ¡c, Ä‘á»ƒ sau hÃ£y tÃ­nh (khÃ´ng pháº£i lÆ°á»i Ä‘Ã¢u nhÃ© =))).\nVá» báº£n thÃ¢n mÃ¬nh\u003e Vá» báº£n thÃ¢n mÃ¬nh # MÃ¬nh tá»‘t nghiá»‡p cáº¥p 3 á»Ÿ trÆ°á»ng THPT ChuyÃªn HÃ¹ng VÆ°Æ¡ng - Gia Lai, khÃ³a 18 - 21. Suá»‘t nhá»¯ng nÄƒm cáº¥p 3, mÃ¬nh Ä‘iá»u hÃ nh má»™t cÃ¢u láº¡c bá»™ vá» tranh biá»‡n vÃ  tá»• chá»©c há»™i nghá»‹ MUN (Model United Nations) cho há»c sinh trong tá»‰nh. MÃ¬nh cÅ©ng lÃ  thÃ nh viÃªn cá»§a Ä‘á»™i tuyá»ƒn Tin há»c quá»‘c gia á»Ÿ trÆ°á»ng. Hiá»‡n táº¡i mÃ¬nh lÃ  sinh viÃªn nÄƒm hai, khÃ³a K21 trÆ°á»ng Ä‘áº¡i há»c KHTN HCM.\nNhá»¯ng thá»© mÃ¬nh lÃ m\u003e Nhá»¯ng thá»© mÃ¬nh lÃ m # Chá»§ yáº¿u lÃ  nhá»¯ng thá»© mÃ¬nh giÃºp Ã­ch cho cuá»™c sá»‘ng cá»§a mÃ¬nh. Báº¡n cÃ³ thá»ƒ xem thÃªm á»Ÿ trang project vÃ  github cá»§a mÃ¬nh.\nKáº¿t ná»‘i vá»›i mÃ¬nh\u003e Káº¿t ná»‘i vá»›i mÃ¬nh # Email (active hÃ ng ngÃ y) TrÃ² chuyá»‡n trá»±c tiáº¿p vá»›i mÃ¬nh Nháº¯n tin qua discord (active thÆ°á»ng xuyÃªn) Q\u0026amp;A\u003e Q\u0026amp;A # ","date":"0001-01-01","permalink":"/about/","section":"About me","summary":"Xin chÃ o! MÃ¬nh lÃ  lelouvincx, bÃ¬nh thÆ°á»ng má»i ngÆ°á»i gá»i lÃ  ChÃ­nh, tÃ¡c giáº£ cá»§a blog nÃ y. Báº¡n cÃ³ thá»ƒ gá»i mÃ¬nh theo cáº£ hai cÃ¡ch Ä‘á»u Ä‘Æ°á»£c. Náº¿u gáº·p mÃ¬nh ngoÃ i Ä‘á»i, báº¡n cÃ³ thá»ƒ sáº½ cáº£m tháº¥y tháº¥t vá»ng má»™t chÃºt.","title":"About me"},{"content":"","date":"0001-01-01","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"0001-01-01","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"1. Táº¡i sao nÃªn Ä‘á»c bÃ i nÃ y?\u003e 1. Táº¡i sao nÃªn Ä‘á»c bÃ i nÃ y? # Chuyá»‡n lÃ  mÃ¬nh vá»«a Ä‘á»c Ä‘Æ°á»£c má»™t bÃ i ráº¥t tÃ¢m huyáº¿t vá» cÃ¡c zsh frameworks trÃªn Reddit, trong Ä‘Ã³ cÃ³ Ä‘oáº¡n viáº¿t nhÆ° nÃ y:\nThey took the best ideas, and wrote them as brand new modules with even higher-quality code, and ended up becoming by far the fastest \u0026ldquo;full-featured zsh framework\u0026rdquo; with the fastest startup time, most efficient Git status prompt updates, etc. Its clean code organization ideas were inspired by Prezto, and some of the modules (such as the git module) were taken from Prezto and cleaned up and refactored to be faster and better. Most modules were written from scratch. The entire project is very minimalistic with super clean, tiny code modules, and it\u0026rsquo;s well-organized within the \u0026ldquo;ZimFW\u0026rdquo; organization on GitHub.\nTáº¡m dá»‹ch: Há» cháº¯t lá»c nhá»¯ng Ã½ tÆ°á»Ÿng tá»‘t nháº¥t, viáº¿t láº¡i nhá»¯ng modules hoÃ n toÃ n má»›i vá»›i code cháº¥t lÆ°á»£ng cao, táº¡o thÃ nh má»™t zsh framework Ä‘áº§y Ä‘á»§ tÃ­nh nÄƒng, vá»›i thá»i gian khá»Ÿi Ä‘á»™ng nhanh nháº¥t, nhiá»u updates hiá»‡u quáº£ nháº¥t, etc. Cáº¥u trÃºc code sáº¡ch Ä‘áº¹p Ä‘Æ°á»£c truyá»n cáº£m há»©ng bá»Ÿi Prezto, vÃ  má»™t vÃ i modules (vÃ­ dá»¥ nhÆ° git module) láº¥y tá»« Prezto Ä‘Æ°á»£c clean vÃ  refactor. Háº§u háº¿t modules Ä‘Æ°á»£c viáº¿t láº¡i tá»« Ä‘áº§u. ToÃ n bá»™ project Ä‘á»u tá»‘i giáº£n vá»›i modules siÃªu sáº¡ch, gá»n, náº±m trong \u0026ldquo;ZimFW\u0026rdquo; organization trÃªn GitHub. (\u0026hellip;)\nWow, cÅ©ng Ä‘Ã¡ng Ä‘á»ƒ thá»­ Ä‘áº¥y chá»©! Cá»™ng thÃªm viá»‡c mÃ¬nh Ä‘Ã£ quÃ¡ ngÃ¡n vá»›i con zsh cá»§a mÃ¬nh khi lÃºc nÃ o báº­t lÃªn cÅ©ng tá»‘n ~0.5 - 1 giÃ¢y (quÃ¡ cháº­m cho 1 shell), váº­y lÃ  quÃ¡ Ä‘á»§ lÃ½ do Ä‘á»ƒ tá»« bá» oh-my-zsh. VÃ  tháº¿ lÃ  cÃ´ng cuá»™c chuyá»ƒn nhÃ  tá»« oh-my-zsh cá»§a mÃ¬nh sang Zim báº¯t Ä‘áº§u.\nzimfw/zimfw Zim: Modular, customizable, and blazing fast Zsh framework HTML 2989 170 2. CÃ i Ä‘áº·t\u003e 2. CÃ i Ä‘áº·t # Äáº§u tiÃªn báº¡n cáº§n gá»¡ bá» oh-my-zsh trÆ°á»›c:\nsource ~/.oh-my-zsh/tools/uninstall.sh Sau Ä‘áº¥y khá»Ÿi Ä‘á»™ng láº¡i terminal, zsh sáº½ trá»‘ng trÆ¡n. Tiáº¿p theo, cÃ i Zim báº±ng curl:\ncurl -fsSL https://raw.githubusercontent.com/zimfw/install/master/install.zsh | zsh Hoáº·c báº±ng wget:\nwget -nv -O - https://raw.githubusercontent.com/zimfw/install/master/install.zsh | zsh Náº¿u khÃ´ng thÃ nh cÃ´ng, hÃ£y thá»­ cÃ i thá»§ cÃ´ng á»Ÿ link nÃ y. Khá»Ÿi Ä‘á»™ng láº¡i terminal vÃ  kiá»ƒm tra Zim Ä‘Ã£ Ä‘Æ°á»£c cÃ i thÃ nh cÃ´ng chÆ°a:\nzimfw Zim themes\u003e Zim themes # Theme máº·c Ä‘á»‹nh cá»§a Zim lÃ  asciiship, báº¡n cÃ³ thá»ƒ chá»n themes Ä‘Æ°á»£c Zim build sáºµn hoáº·c theme khÃ¡c tÃ¹y Ã½. VÃ­ dá»¥ á»Ÿ Ä‘Ã¢y mÃ¬nh sáº½ dÃ¹ng theme eriner, vÃ o file ~/.zimrc, comment dÃ²ng zmodule asciiship vÃ  thÃªm zmodule enriner:\n# ~/.zimrc # Theme enriner # zmodule asciiship zmodule eriner Sau Ä‘Ã³ cháº¡y lá»‡nh:\nzimfw uninstall \u0026amp;\u0026amp; zimfw install Khá»Ÿi Ä‘á»™ng láº¡i terminal vÃ  enjoy!\nNhÆ° váº­y tá»›i Ä‘Ã¢y báº¡n Ä‘Ã£ biáº¿t cÃ¡ch thÃªm 1 zsh plugin vÃ o báº±ng Zim: thÃªm zmodule + tÃªn plugin vÃ o file ~/.zimrc, rá»“i cháº¡y lá»‡nh zimfw install.\nCÃ¡c module Ä‘Æ°á»£c cÃ i náº±m á»Ÿ ~/.zim/modules\nNhÆ°ng vÃ¬ mÃ¬nh quen dÃ¹ng powerlevel10k rá»“i nÃªn mÃ¬nh sáº½ cÃ i powerlevel10k:\n# ~/.zimrc # ... config modules zmodule romkatv/powerlevel10k zimfw install, khá»Ÿi Ä‘á»™ng láº¡i terminal rá»“i p10k configure.\np10k sequence Æ°a thÃ­ch cá»§a mÃ¬nh lÃ  y y y n 3 1 3 4 4 5 2 3 4 2 2 1 1 y 2 y Done! Khá»Ÿi Ä‘á»™ng láº¡i terminal vÃ  enjoy. Modules há»— trá»£\u003e Modules há»— trá»£ # Máº·c Ä‘á»‹nh Zim Ä‘Ã£ cÃ i sáºµn cho báº¡n vÃ i plugin nhÆ° git, zsh-autosuggestions, zsh-syntax-highlighting, etc. Báº¡n cÃ³ thá»ƒ dÃ¹ng luÃ´n mÃ  khÃ´ng cáº§n config gÃ¬ thÃªm. NhÆ°ng á»Ÿ Ä‘Ã¢y mÃ¬nh muá»‘n shell cá»§a mÃ¬nh Ä‘Æ°á»£c viá»‡c hÆ¡n tÃ­ nÃªn sáº½ cÃ i thÃªm zoxide vÃ  zsh-autoswitch-virtualenv.\nZoxide lÃ  má»™t tool tá»± Ä‘á»™ng nhá»› cÃ¡c Ä‘Æ°á»ng dáº«n mÃ  báº¡n Ä‘Ã£ cd, giÃºp báº¡n di chuyá»ƒn nhan hÆ¡n trong terminal. ajeetdsouza/zoxide A smarter cd command. Supports all major shells. Rust 9270 335 Náº¿u chÆ°a cÃ³ zoxide, hÃ£y cÃ i nÃ³ vÃ o trÆ°á»›c:\n# Ubuntu sudo apt-get update sudo apt-get install zoxide ThÃªm dÃ²ng nÃ y vÃ o ~/.zshrc:\n# Zoxide eval \u0026#34;$(zoxide init zsh)\u0026#34; Sau Ä‘Ã³ cÃ i module vÃ o ~/.zimrc:\n# ~/.zimrc # ... config modules # Zoxide zmodule kiesman99/zim-zoxide Sau Ä‘áº¥y thÃ¬ zimfw install thÃ´i :D zsh-autoswitch-virtualenv lÃ  tool giÃºp tá»± activate/deactivate khi báº¡n cd vÃ o project cÃ³ virtualenv cá»§a python. NÃ³ há»— trá»£ nhiá»u loáº¡i cho báº¡n nhÆ° venv, pipenv, poetry báº±ng cÃ¡ch nháº­n diá»‡n má»™t trong cÃ¡c file sau (xem thÃªm á»Ÿ GitHub): MichaelAquilina/zsh-autoswitch-virtualenv ğŸ ZSH plugin to automatically switch python virtualenvs (including pipenv and poetry) as you move between directories Shell 373 67 setup.py requirements.txt Pipfile poetry.lock CÃ¡c bÆ°á»›c cÃ i cÅ©ng nhÆ° trÆ°á»›c, thÃªm Ä‘oáº¡n dÆ°á»›i vÃ o cuá»‘i file ~/.zimrc rá»“i zimfw install:\n# ~/.zimrc # ... config modules # Auto-switch python virtualenv zmodule MichaelAquilina/zsh-autoswitch-virtualenv Äáº¿n Ä‘Ã¢y module Ä‘Ã£ Ä‘Æ°á»£c download vá» ~/.zim/modules, nhÆ°ng Zim chÆ°a source Ä‘Æ°á»£c nÃ³ vÃ¬ trong module khÃ´ng cÃ³ file init.zsh. Äá»ƒ kháº¯c phá»¥c, vÃ o ~/.zim/modules/zsh-autoswitch-virtualenv cháº¡y cÃ¡c lá»‡nh\ntouch init.zsh cp autoswitch-virtualenv.plugin.zsh init.zsh zimfw build Kiá»ƒm tra file ~/.zim/init.zsh cÃ³ module nÃ y lÃ  OK. Module sáº½ tá»± Ä‘á»™ng activate/deactivate khi ra/vÃ o project python cÃ³ virtualenv Má»™t vÃ i config khÃ¡c\u003e Má»™t vÃ i config khÃ¡c # Má»™t vÃ i config khÃ¡c giÃºp mÃ¬nh lÃ m viá»‡c nhanh vÃ  hiá»‡u quáº£ hÆ¡n:\nSource Ruby vá»›i rvm, khi dÃ¹ng chá»‰ cáº§n gÃµ source_ruby: # ~/.zshrc # Add RVM to PATH for scripting. Make sure this is the last PATH variable change. export PATH=\u0026#34;$PATH:$HOME/.rvm/bin\u0026#34; alias source_ruby=\u0026#34;source $HOME/.rvm/scripts/rvm\u0026#34; DÃ¹ng exa thay cho ls, exa nhanh vÃ  cho mÃ u Ä‘áº¹p hÆ¡n ls: ogham/exa A modern replacement for â€˜lsâ€™. Rust 20657 609 # ~/.zshrc # Use exa instead of ls alias ls=\u0026#34;exa\u0026#34; alias la=\u0026#34;exa -a\u0026#34; alias ll=\u0026#34;exa -l\u0026#34; DÃ¹ng bat thay cho cat: # ~/.zshrc # Use bat instead for cat alias cat=\u0026#34;bat\u0026#34; Shortcut cho tmux: # ~/.zshrc # Map tmux shortcut alias ta=\u0026#34;tmux a\u0026#34; alias tl=\u0026#34;tmux ls\u0026#34; Káº¿t luáº­n\u003e Káº¿t luáº­n # Æ¯u Ä‘iá»ƒm:\nZim cho mÃ¬nh tá»‘c Ä‘á»™ nhanh hÆ¡n háº³n oh-my-zsh tá»«ng dÃ¹ng, má»Ÿ cÃ¡i lÃ  lÃªn liá»n chá»© khÃ´ng cáº§n Ä‘á»£i nhÆ° oh-my-zsh Dá»… dÃ¹ng, chá»‰ cáº§n nhá»› vÃ i cÃ¢u lá»‡nh nhÆ° zimfw install, uninstall, compile, build lÃ  Ä‘Æ°á»£c NhÆ°á»£c Ä‘iá»ƒm:\nCáº§n má»™t chÃºt thá»i gian thÃ­ch nghi Pháº£i cáº§n file init.zsh trong má»—i module Ä‘á»ƒ load vÃ o shell Má»™t framework tá»‘i giáº£n, thÃº vá»‹. MÃ¬nh sáº½ tÃ¬m hiá»ƒu sÃ¢u hÆ¡n vÃ  Ä‘Æ°a ra cÃ¡c updates sau nÃ y.\n","date":"0001-01-01","permalink":"/posts/oh-my-zsh-to-zim/","section":"Posts","summary":"HÆ°á»›ng dáº«n cÃ¡ch chuyá»ƒn tá»« oh-my-zsh sang zim vÃ  bÆ°á»›c Ä‘áº§u lÃ m quen vá»›i framework nÃ y","title":"Chuyá»ƒn nhÃ  tá»« oh-my-zsh sang zimfw"},{"content":"1. Táº¡i sao nÃªn Ä‘á»c bÃ i nÃ y?\u003e 1. Táº¡i sao nÃªn Ä‘á»c bÃ i nÃ y? # Vá»›i nhá»¯ng ai yÃªu thÃ­ch Neovim vÃ  sá»­ dá»¥ng nÃ³ lÃ m IDE chÃ­nh, báº¡n sáº½ Ä‘Ã´i lÃºc gáº·p váº¥n Ä‘á» khÃ³ chá»‹u khi code Ruby trÃªn Neovim nhÆ° thiáº¿u suggestion, lint, format, \u0026hellip; NhÃ¢n dá»‹p nÄƒm má»›i 2023, mÃ¬nh sáº½ hÆ°á»›ng dáº«n cÃ¡c báº¡n cÃ¡ch setup mÃ´i trÆ°á»ng phÃ¡t triá»ƒn Ruby báº±ng Neovim sá»­ dá»¥ng LSP Solargraph.\nSolargraph lÃ  má»™t language server cung cáº¥p IntelliSense, code completion vÃ  docs cho Ruby. NÃ³ há»— trá»£ nhiá»u IDE vÃ  text editor nhÆ° VSCode, Emacs, Atom, etc vÃ  dÄ© nhiÃªn, Neovim. Trang chá»§ cá»§a solargraph.\n2. YÃªu cáº§u\u003e 2. YÃªu cáº§u # Äá»ƒ setup báº¡n cáº§n config trÆ°á»›c nhá»¯ng thá»© sau:\nRuby cÃ i vÃ  cháº¡y Ä‘Æ°á»£c Neovim version 0.5.0, Ä‘Ã£ cÃ i mason vÃ  config LSP 3. CÃ i Ä‘áº·t\u003e 3. CÃ i Ä‘áº·t # Äáº§u tiÃªn báº¡n cáº§n cÃ i gem solargraph:\ngem install solargraph Kiá»ƒm tra solargraph Ä‘Ã£ Ä‘Æ°á»£c cÃ i chÆ°a:\ngem list | grep solargraph # Náº¿u chÆ°a thÃ¬ dÃ¹ng command sau gem install --user-install solargraph Hoáº·c thÃªm dÃ²ng sau vÃ o Gemfile:\ngem \u0026#39;solargraph\u0026#39;, group: :development Tiáº¿p theo vÃ o Neovim vÃ  má»Ÿ mason: :Mason, tÃ¬m vÃ  cÃ i 2 gÃ³i sau: solargraph vÃ  rubocop.\nNáº¿u cÃ³ config LSP theo dotfiles cá»§a mÃ¬nh, báº¡n chá»‰ cáº§n thÃªm Ä‘oáº¡n sau vÃ o ~/config/nvim/lua/$USER/plugins/lsp/lspconfig.lua:\n-- ... -- configure ruby language server lspconfig[\u0026#34;solargraph\u0026#34;].setup({ capabilities = capabilities, on_attach = on_attach, filetypes = { \u0026#34;ruby\u0026#34; }, }) -- ... Xong! BÃ¢y giá» chá»‰ cáº§n má»Ÿ file Ruby vÃ  check :LspInfo, lÃºc nÃ y sáº½ hiá»‡n 1 client cÃ³ tÃªn solargraph Ä‘Æ°á»£c attach vÃ o buffer .rb:\n","date":"0001-01-01","permalink":"/posts/neovim-setup-ruby/","section":"Posts","summary":"HÆ°á»›ng dáº«n setup mÃ´i trÆ°á»ng phÃ¡t triá»ƒn Ruby trÃªn Neovim","title":"Code Ruby trÃªn Neovim"},{"content":"","date":"0001-01-01","permalink":"/tags/neovim/","section":"Tags","summary":"","title":"neovim"},{"content":"","date":"0001-01-01","permalink":"/tags/post/","section":"Tags","summary":"","title":"post"},{"content":" ","date":"0001-01-01","permalink":"/posts/","section":"Posts","summary":" ","title":"Posts"},{"content":"","date":"0001-01-01","permalink":"/resources/","section":"Resources","summary":"","title":"Resources"},{"content":"","date":"0001-01-01","permalink":"/tags/ruby/","section":"Tags","summary":"","title":"ruby"},{"content":"","date":"0001-01-01","permalink":"/series/","section":"Series","summary":"","title":"Series"},{"content":" Cáº­p nháº­t ngÃ y 05/01/2023, Viá»‡t Nam.\nHÃ nh trÃ¬nh váº¡n dáº·m khá»Ÿi Ä‘áº§u báº±ng bÆ°á»›c chÃ¢n Ä‘áº§u tiÃªn. ~ LÃ£o Tá»­\nHá»c\u003e Há»c # MÃ¬nh Ä‘ang há»c nÄƒm hai, trÆ°á»ng Äáº¡i há»c Khoa há»c Tá»± nhiÃªn TP. HCM, ngÃ nh Khoa Há»c Dá»¯ Liá»‡u.\nTrong má»™t dá»± Ã¡n, mÃ¬nh bá»—ng dÆ°ng \u0026ldquo;fall in love\u0026rdquo; vá»›i Ruby. MÃ¬nh Ä‘ang há»c OOP cá»§a Ruby song song vá»›i Rails.\nÄang cáº£i thiá»‡n trÃ¬nh Ä‘á»™ SQL vÃ  Python báº±ng viá»‡c lÃ m project vÃ  Hackerrank hÃ ng ngÃ y. NgoÃ i ra, mÃ¬nh cÅ©ng cÃ³ niá»m yÃªu thÃ­ch Ä‘áº·c biá»‡t vá»›i an toÃ n thÃ´ng tin (cybersecurity).\nLÃ m\u003e LÃ m # VÃ i project nhá» mÃ  mÃ¬nh vÃ  báº¡n mÃ¬nh ná»•i há»©ng nghÄ© ra:\nSpotify data pipeline ingestion: Data pipeline Ä‘Æ¡n giáº£n theo hÆ°á»›ng ELT, Ä‘á»ƒ ingest dá»¯ liá»‡u tá»« MySQL vÃ  Spotify API (batch ingestion). Ubunchuu TrÆ°á»ng Ãš: Má»™t project nhá» vá»›i má»¥c tiÃªu Ä‘em láº¡i giÃºp cÃ¡c báº¡n sinh viÃªn tiáº¿p cáº­n dá»… hÆ¡n vá»›i linux (ubuntu). Tá»± design vÃ  implement má»™t RDBMS Ä‘Æ¡n giáº£n (vá»›i B-Tree). Äá»c\u003e Äá»c # Goodreads cá»§a mÃ¬nh Big Data: CÃ´ng nghá»‡ cá»‘t lÃµi trong ká»· nguyÃªn sá»‘ - Thomas Davenport Atomic Habits - James Clear ","date":"0001-01-01","permalink":"/now/","section":"What I'm doing now","summary":"Cáº­p nháº­t ngÃ y 05/01/2023, Viá»‡t Nam.\nHÃ nh trÃ¬nh váº¡n dáº·m khá»Ÿi Ä‘áº§u báº±ng bÆ°á»›c chÃ¢n Ä‘áº§u tiÃªn. ~ LÃ£o Tá»­\nHá»c\u003e Há»c # MÃ¬nh Ä‘ang há»c nÄƒm hai, trÆ°á»ng Äáº¡i há»c Khoa há»c Tá»± nhiÃªn TP.","title":"What I'm doing now"},{"content":"","date":"0001-01-01","permalink":"/tags/zimfw/","section":"Tags","summary":"","title":"zimfw"},{"content":"","date":"0001-01-01","permalink":"/tags/zsh/","section":"Tags","summary":"","title":"zsh"}]