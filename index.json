[{"content":"","date":"2024-01-13","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"2024-01-13","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"2024-01-13","permalink":"/tags/dagster/","section":"Tags","summary":"","title":"dagster"},{"content":"","date":"2024-01-13","permalink":"/categories/data-engineering/","section":"Categories","summary":"","title":"data engineering"},{"content":"","date":"2024-01-13","permalink":"/tags/imdb/","section":"Tags","summary":"","title":"imdb"},{"content":"","date":"2024-01-13","permalink":"/authors/lelouvincx/","section":"Authors","summary":"","title":"lelouvincx"},{"content":"","date":"2024-01-13","permalink":"/","section":"lelouvincx's blog","summary":"","title":"lelouvincx's blog"},{"content":"","date":"2024-01-13","permalink":"/categories/mlops/","section":"Categories","summary":"","title":"MLOps"},{"content":"","date":"2024-01-13","permalink":"/projects/","section":"Projects","summary":"","title":"Projects"},{"content":"My personal stuff shown here.\n","date":"2024-01-13","permalink":"/resources/","section":"Resources","summary":"My personal stuff shown here.","title":"Resources"},{"content":"","date":"2024-01-13","permalink":"/tags/spark/","section":"Tags","summary":"","title":"spark"},{"content":"","date":"2024-01-13","permalink":"/projects/spark-data-platform/","section":"Projects","summary":"X√¢y d·ª±ng data platform v·ªõi on-premise Spark Cluster","title":"Spark Data Platform"},{"content":"","date":"2024-01-13","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":" C·∫≠p nh·∫≠t ng√†y 13/01/2024, Vi·ªát Nam.\nH√†nh tr√¨nh v·∫°n d·∫∑m kh·ªüi ƒë·∫ßu b·∫±ng b∆∞·ªõc ch√¢n ƒë·∫ßu ti√™n.\n~ L√£o T·ª≠\nH·ªçc # M√¨nh ƒëang h·ªçc nƒÉm ba, tr∆∞·ªùng ƒê·∫°i h·ªçc Khoa H·ªçc T·ª± Nhi√™n TP. HCM, ng√†nh khoa h·ªçc d·ªØ li·ªáu.\nƒêang t·ªïng h·ª£p nh·ªØng ki·∫øn th·ª©c data ƒë√£ h·ªçc t·ª´ ƒë√≥ gi·ªù. V√† t·∫≠p t·ª± modelling v·ªÅ star schema tr√™n 1 b·ªô d·ªØ li·ªáu.\nCI/CD l√† m·ªôt qu√° tr√¨nh ph·ª©c t·∫°p. M√¨nh ƒëang h·ªçc c√°c best practices trong deployment v√† s·ª≠ d·ª•ng GitOps.\nL√†m # Data Engineer t·∫°i KMS Healthcare.\nV√†i project nh·ªè m√† m√¨nh v√† b·∫°n m√¨nh n·ªïi h·ª©ng nghƒ© ra:\nGoodreads ELT pipeline: X√¢y d·ª±ng m·ªôt ELT (Extract - Load - Transform) data pipeline ho√†n ch·ªânh v·ªõi b·ªô d·ªØ li·ªáu s√°ch t·ª´ Goodreads. Spark Data Platform: X√¢y d·ª±ng data platform v·ªõi b·ªô d·ªØ li·ªáu phim t·ª´ IMDb.com - k·∫øt h·ª£p Spark Cluster d·∫°ng on-premise. Ubunchuu Tr∆∞·ªùng √ö: M·ªôt project nh·ªè v·ªõi m·ª•c ti√™u gi√∫p cho c√°c b·∫°n sinh vi√™n ti·∫øp c·∫≠n d·ªÖ h∆°n v·ªõi Linux (c·ª• th·ªÉ l√† Ubuntu). Database replication: Replicate t·ª´ upstream database v·ªÅ downstream, s·ª≠ d·ª•ng Apache Kafka. Projectopia: Auto-devops tool, gi√∫p ƒë·ª° kh√¢u DevOps cho c√°c d·ª± √°n quy m√¥ sinh vi√™n V√†i post ideas ƒëang trong qu√° tr√¨nh s·∫£n xu·∫•t:\nData engineer roadmap Configure Spark Cluster d∆∞·ªõi local d√πng multipass v√† k3s ƒê·ªçc # Goodreads c·ªßa m√¨nh S√°ch:\nEconomix: How and Why Our Economy Works (and Doesn\u0026rsquo;t Work), in Words and Pictures by Michael Goodwin Data Engineering Design Patterns (DEDP) by Simon Sp√§ti V√†i b√†i vi·∫øt m√† m√¨nh t√¢m ƒë·∫Øc:\nXu·∫•t kh·∫©u gian l·∫≠n by Th√°i vnhacker Awesome prompts for LLM models ","date":"2024-01-13","permalink":"/resources/now/","section":"Resources","summary":"What lelouvincx\u0026rsquo;s doing now?","title":"What I'm doing now"},{"content":"","date":"2023-04-16","permalink":"/tags/dbt/","section":"Tags","summary":"","title":"dbt"},{"content":"","date":"2023-04-16","permalink":"/tags/elt/","section":"Tags","summary":"","title":"ELT"},{"content":"","date":"2023-04-16","permalink":"/tags/goodreads/","section":"Tags","summary":"","title":"goodreads"},{"content":"Trong project n√†y m√¨nh s·∫Ω h∆∞·ªõng d·∫´n x√¢y d·ª±ng m·ªôt data pipeline c∆° b·∫£n theo m√¥ h√¨nh ELT (extract - load - transform), s·ª≠ d·ª•ng b·ªô d·ªØ li·ªáu t·ª´ goodreads ƒë·ªÉ ingest, transform data ph·ª•c v·ª• h·ªá th·ªëng recommend s√°ch cho b·∫£n th√¢n.\nProject n√†y ho√†n th√†nh d·ª±a tr√™n ki·∫øn th·ª©c ƒë√£ h·ªçc ƒë∆∞·ª£c t·ª´ kh√≥a Fundamental Data Engineering 02 c·ªßa AIDE. Xin g·ª≠i l·ªùi c·∫£m ∆°n ƒë·∫∑c bi·ªát t·ªõi th·∫ßy Nguy·ªÖn Thanh B√¨nh, anh √îng Xu√¢n H·ªìng v√† anh H√πng L√™.\nlelouvincx/goodreads-elt-pipeline This project implements an ELT (Extract - Load - Transform) data pipeline with the goodreads dataset, using dagster (orchestration), spark (calculation) and dbt (transformation) Jupyter Notebook 12 3 Demo video 1. Introduction # M√¨nh th√≠ch ƒë·ªçc s√°ch. V√† m√¨nh c√≥ m·ªôt chi·∫øc m√°y ƒë·ªçc s√°ch kindle d√πng cho vi·ªác ƒë·ªçc h√†ng ng√†y.\nC√≥ m·ªôt ƒëi·ªÉm m√¨nh th√≠ch ·ªü chi·∫øc kindle l√† n√≥ c√≥ m·ªôt ƒë·ªãa ch·ªâ email ri√™ng bi·ªát ƒë∆∞·ª£c amazon c·∫•p ph√°t. N·∫øu s·ª≠ d·ª•ng email c·ªßa m√¨nh ƒë·ªÉ g·ª≠i file s√°ch (d·∫°ng .epub/.mobi), h·ªá th·ªëng tr√™n amazon s·∫Ω t·ª± ƒë·ªông g·ª≠i file s√°ch v√†o kindle gi√∫p m√¨nh, mi·ªÖn l√† c√≥ k·∫øt n·ªëi m·∫°ng.\nTh·∫ø th√¨ t·∫°i sao m√¨nh kh√¥ng t·ª± build m·ªôt app, c√≥ th·ªÉ l·∫•y data t·ª´ goodreads (m·ªôt m·∫°ng x√£ h·ªôi cho c√°c m·ªçt s√°ch), x·ª≠ l√Ω v√† ƒë∆∞a ra l·ªùi g·ª£i √Ω cho nh·ªØng cu·ªën s√°ch ti·∫øp theo cho m√¨nh nh·ªâ? V√† th·∫ø l√† project b·∫Øt ƒë·∫ßu :D\n2. Objective # Dataset c∆° b·∫£n ƒë∆∞·ª£c l·∫•y t·ª´ Kaggle, OpenLibrary API, Google Drive API v√† Notion API\nM·ª•c ti√™u c·ªßa project, l√† v·ªõi ngu·ªìn data ƒë∆∞·ª£c thu th·∫≠p v√† x·ª≠ l√Ω s·∫µn, khi ng∆∞·ªùi d√πng nh·∫≠p th√¥ng tin cu·ªën s√°ch ƒë√£ ƒë·ªçc, c√≥ th·ªÉ g·ª£i √Ω c√°c cu·ªën s√°ch ti·∫øp theo. N·∫øu cu·ªën s√°ch ƒë√≥ c√≥ file .epub, app s·∫Ω hi·ªán t√≠nh nƒÉng g·ª≠i s√°ch v√†o kindle.\n3. Design # 3.1 Directory tree # app: Giao di·ªán ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi ng∆∞·ªùi d√πng, vi·∫øt b·∫±ng streamlit dagster_home: Config cho dagit (dagster UI) v√† dagster daemon dataset: L∆∞u dataset d∆∞·ªõi d·∫°ng .csv, m·∫∑c ƒë·ªãnh load v√†o MySQL docker-compose: Compose docker containers dockerimages: Ch·ª©a c√°c image t·ª± build nh∆∞ dagster (dagit + daemon), spark master, streamlit app EDA.ipynb: Exploratory Data Analysis, xem tr·ª±c ti·∫øp t·∫°i ƒë√¢y elt_pipeline: To√†n b·ªô pipeline dbt_transform: code location c·ªßa dbt, d√πng cho b∆∞·ªõc transform ·ªü cu·ªëi Dockerfile + requirements.txt: Th√¥ng tin build image elt_pipeline: pipeline t·ª´ ƒë·∫ßu t·ªõi tr∆∞·ªõc dbt .env + .spark_master.env + .spark_worker.env: L∆∞u bi·∫øn m√¥i tr∆∞·ªùng (v√≠ d·ª• POSTGRES_USER, MYSQL_USER, SPARK, \u0026hellip;) env.template: Template ƒë·ªÉ b·∫°n t·ª± ƒëi·ªÅn th√¥ng tin bi·∫øn m√¥i tr∆∞·ªùng .git + .gitignore: Thu·ªôc git, qu·∫£n l√Ω code version Makefile: Shortcut c√¢u l·ªánh tr√™n terminal load_dataset: Ch·ª©a script .sql ƒë·ªÉ t·∫°o schema v√† load v√†o database MySQL v√† Postgres requirements.txt + Pipfile + Pipfile.lock: Dependencies c·ªßa python Ngo√†i ra c√≤n c√≥ c√°c th∆∞ m·ª•c s·ªü h·ªØu ri√™ng c·ªßa container:\nminio storage mysql_data postgres_data metabase_data Chi ti·∫øt xem ·ªü file tree.txt\n3.2 Pipeline design # Ta s·ª≠ d·ª•ng docker ƒë·ªÉ ƒë√≥ng g√≥i ·ª©ng d·ª•ng v√† dagster ƒë·ªÉ orchestrate assets (theo ƒë·ªãnh nghƒ©a c·ªßa daster) D·ªØ li·ªáu Goodreads ƒë∆∞·ª£c download t·ª´ kaggle d∆∞·ªõi d·∫°ng .csv, sau ƒë√≥ import v√†o MySQL m√¥ ph·ªèng d·ªØ li·ªáu development Sau khi c√≥ th√¥ng tin ISBN (m√£ ƒë·ªãnh danh qu·ªëc t·∫ø) c·ªßa s√°ch, ti·∫øn h√†nh collect th√™m data t·ª´ c√°c api li√™n quan Genre, author, pages number, image, description t·ª´ OpenLibrary API Download link t·ª´ Notion API Epub file t·ª´ Google Drive API Image t·ª´ OpenLibrary API ho·∫∑c Google Drive API Extract d·ªØ li·ªáu d·∫°ng b·∫£ng ·ªü tr√™n b·∫±ng polars, load v√†o datalake - MinIO T·ª´ MinIO load ra spark ƒë·ªÉ transform th√†nh layer silver v√† gold Convert Spark DataFrame th√†nh parquet, load l·∫°i v√†o MinIO Gold layer ƒë∆∞·ª£c load ti·∫øp v√†o data warehouse - postgreSQL, t·∫°o th√†nh warehouse layer Transform t√πy m·ª•c ƒë√≠ch b·∫±ng dbt tr√™n n·ªÅn postgres Tr·ª±c quan h√≥a d·ªØ li·ªáu b·∫±ng metabase Giao di·ªán app g·ª£i √Ω s√°ch b·∫±ng streamlit 3.3 Database schema # book: OLTP table ch·ª©a th√¥ng tin cu·ªën s√°ch (ISBN, Authors, Rating, Description\u0026hellip;) genre: table ch·ª©a t√™n c√°c th·ªÉ lo·∫°i s√°ch book_genre: quan h·ªá n-n gi·ªØa book v√† genre book_download_link: table ch·ª©a link google drive ƒë·ªÉ tham chi·∫øu BookFile files: object storage ch·ª©a file download s√°ch (.epub/.pdf/.mobi) images: object storage ch·ª©a h√¨nh ·∫£nh cu·ªën s√°ch 3.4 Datalake structure # Datalake chia theo c√°c layer: bronze, silver, gold C√°c lo·∫°i file ƒë·ªÅu d·∫°ng .parquet ƒë·ªÉ cho k·∫øt qu·∫£ ƒë·ªçc t·ªët h∆°n .csv Ngo√†i ra c√≥ files: l∆∞u file .epub theo d·∫°ng abc.epub, trong ƒë√≥ abc l√† ISBN c·ªßa cu·ªën s√°ch T∆∞∆°ng t·ª±, abc.jpeg l∆∞u h√¨nh ·∫£nh c·ªßa cu·ªën s√°ch 3.5 Data lineage # T·ªïng quan V·ªõi data lineage d√†y ƒë·∫∑c, dagster l√† big help khi c√≥ th·ªÉ visualize ch√∫ng m·ªôt c√°ch tr·ª±c quan:\nD·ªØ li·ªáu xu·∫•t ph√°t t·ª´ MySQL v√† c√°c lo·∫°i API, load v√†o bronze layer T·ª´ bronze layer, d·ªØ li·ªáu ƒë∆∞·ª£c dedupe, clean v√† fill missing ·ªü silver layer Sau ƒë√≥ t√≠nh to√°n n√¢ng cao v√† ph√¢n t√°ch ·ªü gold layer Load v√†o data warehouse - Postgres ·ªü warehouse layer V√† cu·ªëi c√πng, transform theo nhu c·∫ßu ·ªü recommendations layer b·∫±ng dbt Bronze layer G·ªìm c√°c asset:\nbronze_book: B·∫£ng book t·ª´ MySQL, v√¨ qu√° l·ªõn (tr√™n 1.2 tri·ªáu d√≤ng n√™n ƒë∆∞·ª£c partitions theo nƒÉm t·ª´ 1975 t·ªõi 2022) bronze_genre: B·∫£ng genre t·ª´ MySQL bronze_book_genre: B·∫£ng book_genre t·ª´ MySQL bronze_book_download_link: B·∫£ng book_download_link t·ª´ MySQL bronze_images_and_files_download: ƒê·∫£m nh·∫≠n vi·ªác k·∫øt n·ªëi t·ªõi google drive api, k√©o file .epub v√† h√¨nh ·∫£nh v·ªÅ, l∆∞u trong datalake Silver layer G·ªìm c√°c asset:\nsilver_cleaned_book: Clean data t·ª´ upstream bronze_book, ƒë∆∞·ª£c partition ƒë·ªÉ ƒë·∫£m b·∫£o spark standalone mode c√≥ th·ªÉ ch·∫°y silver_collected_book: Collect th√™m missing data t·ª´ upstream nh∆∞ authors, pages number, description t·ª´ OpenLibrary API silver_isbn: T√°ch c·ªôt isbn t·ª´ book ƒë·ªÉ l√†m dependency cho asset li√™n quan t·ªõi genre silver_cleaned_genre: T∆∞∆°ng t·ª± silver_cleaned_book, kh√°c c√°i kh√¥ng c·∫ßn partition v√¨ size kh√¥ng l·ªõn l·∫Øm silver_collected_genre: D·ª±a v√†o silver_isbn, collect th√™m genre cho m·ªói cu·ªën s√°ch b·ªã thi·∫øu genre. N·∫øu kh√¥ng c√≥ genre th√¨ kh√¥ng th·ªÉ l√†m recommendations cho c√°c task sau silver_collected_book_genre: K·∫øt n·ªëi quan h·ªá n-n gi·ªØa book v√† genre Gold layer G·ªìm c√°c asset:\ngold_genre: T√≠nh to√°n, s·∫Øp x·∫øp c√°c genre cho ph√π h·ª£p t·ª´ upstream silver_collected_genre, ƒë·ªìng th·ªùi l∆∞u v√†o minIO gold_book_genre: T∆∞∆°ng t·ª±, t·ª´ upstream silver_collected_book_genre gold_with_info: Ph√¢n t√°ch, ch·ªâ ch·ª©a th√¥ng tin c∆° b·∫£n v·ªÅ cu·ªën s√°ch nh∆∞ ISBN, Name, Authors, Language, PagesNumber gold_with_publish: Ph√¢n t√°ch, ch·ªâ ch·ª©a th√¥ng tin v·ªÅ nh√† xu·∫•t b·∫£n, th·ªùi gian xu·∫•t b·∫£n gold_with_rating: Ph√¢n t√°ch v√† t√≠nh to√°n c√°c lo·∫°i rating Warehouse layer Load c√°c asset t·ª´ gold layer v√†o postgres. Trong ƒë√≥ c√≥ 1 asset t·ª´ bronze layer l√† book_download_link.\nTrong t∆∞∆°ng lai s·∫Ω c·∫≠p nh·∫≠t asset ƒë·ªÉ b·ªï sung download link t·ª± ƒë·ªông t·ª´ Notion API, v√† thi·∫øt l·∫≠p schedule.\nTransform layer G·ªìm c√°c model (asset):\nsearch: Transform th√¥ng tin ƒë·ªÉ t·∫°o b·∫£ng index, khi ng∆∞·ªùi d√πng t√¨m ki·∫øm s·∫Ω query tr√™n b·∫£ng n√†y search_prior: C≈©ng l√† b·∫£ng index, nh∆∞ng ch·ª©a c√°c cu·ªën s√°ch ƒë∆∞·ª£c ∆∞u ti√™n h∆°n d·ª±a v√†o vi·ªác c√≥ download link, OpenLibrary API ho·∫°t ƒë·ªông, rating cao, \u0026hellip; criteria: Ti√™u ch√≠ ƒë·ªÉ query nh·ªØng cu·ªën s√°ch li√™n quan khi t√¨m ki·∫øm 1 cu·ªën s√°ch 4. Setup # 4.1 Prequisites # ƒê·ªÉ s·ª≠ d·ª•ng pipeline n√†y, download nh·ªØng ph·∫ßn m·ªÅm sau:\nGit Docker √≠t nh·∫•t 4GB RAM, 6 core CPU, 2GB swap, 16GB disk CMake, n·∫øu d√πng h·ªá m√°y UNIX (Linux/MacOS), check make --version ƒë∆∞·ª£c c√†i s·∫µn Python 3.x (3.9.16 recommended v√¨ image c·ªßa spark ch·∫°y tr√™n version n√†y, khuy·∫øn kh√≠ch c√†i b·∫±ng asdf) v√† m√¥i tr∆∞·ªùng ·∫£o (pipenv recommended) M√°y local ƒë√£ free c√°c port sau: 3306, 5432, 9000, 9001, 3001, 8501, 4040, 7077, 8080, 3030 Dbeaver ho·∫∑c m·ªôt db client b·∫•t k·ª≥ (n·∫øu kh√¥ng c√≥ th·ªÉ d√πng command-line) N·∫øu d√πng Windows, setup th√™m WSL2 v√† m·ªôt m√°y ·∫£o local Ubuntu, c√†i ƒë·∫∑t nh·ªØng th·ª© tr√™n cho ubuntu. Clone project v·ªÅ:\ngit clone https://github.com/lelouvincx/goodreads-elt-pipeline.git project cd project Download dataset ·ªü ƒë√¢y, sau ƒë√≥ ƒë·∫∑t 4 file .csv v√†o project/dataset.\n4.2 Setup google drive api # ƒê·∫ßu ti√™n ch√∫ng ta c·∫ßn t·∫°o m·ªôt OAuth 2.0 token t·ªõi google,¬†Google API Console.\nCh·ªçn create new project ƒë·ªÉ m·ªü h·ªôp tho·∫°i.\nƒêi·ªÅn t√™n c·ªßa project v√†o (goodreads-elt_pipeline), t√πy ch·ªçn location c·ªßa b·∫°n (m·∫∑c ƒë·ªãnh No organization).\nSau khi t·∫°o xong project, ch·ªçn tab Library.\nSearch Google Drive API, enable n√≥.\nTi·∫øp theo, ch·ªçn tab OAuth consent screen,\nƒêi·ªÅn th√¥ng tin nh∆∞ d∆∞·ªõi\n·ªû m·ª•c scopes, ch·ªçn add or remove scopes, t√¨m google drive api, readonly r·ªìi tick v√†o, save and continue t·ªõi h·∫øt\nV√†o tab credentials -\u0026gt; create credentials v√† ch·ªçn OAuth client ID.\nCh·ªçn Desktop app, ƒë·∫∑t t√™n t√πy th√≠ch (goodreads-elt-pipeline)\nDownload json v√† ƒë·∫∑t file v√†o project/elt_pipeline/elt_pipeline\n4.3 Setup local infrastructure # Create env file\ntouch .env cp env.template .env touch .spark_master.env cp spark_master.env.template .spark_master.env touch .spark_worker.env cp spark_worker.env.template .spark_worker.env Sau ƒë√≥ ƒëi·ªÅn th√¥ng tin bi·∫øn m√¥i tr∆∞·ªùng v√†o 3 file env tr√™n. D∆∞·ªõi ƒë√¢y l√† v√≠ d·ª•:\n# MySQL MYSQL_HOST=de_mysql MYSQL_PORT=3306 MYSQL_DATABASE=goodreads MYSQL_USER=admin MYSQL_PASSWORD=admin123 MYSQL_ROOT_PASSWORD=root123 # PostgreSQL POSTGRES_HOST=de_psql POSTGRES_PORT=5432 POSTGRES_USER=admin POSTGRES_PASSWORD=admin123 POSTGRES_DB=goodreads POSTGRES_HOST_AUTH_METHOD=trust # Google Drive GDRIVE_CLIENT_SECRET_FILE=client_secret.json GDRIVE_PICKLE_FILE=token_drive_v3.pickle GDRIVE_API_NAME=drive GDRIVE_API_VERSION=v3 GDRIVE_SCOPES=https://www.googleapis.com/auth/drive.readonly # Dagster DAGSTER_PG_HOSTNAME=de_psql DAGSTER_PG_USERNAME=admin DAGSTER_PG_PASSWORD=admin123 DAGSTER_PG_DB=postgres DAGSTER_OVERALL_CONCURRENCY_LIMIT=1 DAGSTER_HOME=/opt/dagster/dagster_home # dbt DBT_HOST=de_psql DBT_USER=admin DBT_PASSWORD=admin123 DBT_DATABASE=goodreads DBT_SCHEMA=recommendations # MinIO MINIO_ENDPOINT=minio:9000 MINIO_ROOT_USER=minio MINIO_ROOT_PASSWORD=minio123 MINIO_ACCESS_KEY=minio MINIO_SECRET_KEY=minio123 DATALAKE_BUCKET=lakehouse AWS_ACCESS_KEY_ID=minio AWS_SECRET_ACCESS_KEY=minio123 AWS_REGION=us-east-1 # MinIO client (mc) AWS_ACCESS_KEY_ID=minio AWS_SECRET_ACCESS_KEY=minio123 AWS_REGION=us-east-1 # Spark SPARK_MASTER_URL=spark://spark-master:7077 SPARK_VERSION=3.3.2 HADOOP_VERSION=3 # Metabase MB_DB_TYPE=postgres MB_DB_DBNAME=goodreads MB_DB_PORT=5432 MB_DB_USER=admin MB_DB_PASS=admin123 MB_DB_HOST=de_psql MB_DB_FILE=/metabase_data/metabase.db B·∫°n c√≥ th·ªÉ thay c√°c th√¥ng tin v·ªÅ user, pasword, \u0026hellip;\nCh·ªâ d√πng cho m√¥i tr∆∞·ªùng development, kh√¥ng d√πng cho testing, staging, production. Ch·∫°y c√°c l·ªánh sau ƒë·ªÉ setup:\n# DO NOT RUN BOTH BELOW COMMANDS, ONLY CHOOSE ONE # Setup python environment pipenv install # Or create virtualenv and install manually by requirements.txt make install # Build docker images make build-dagster make build-spark make build-pipeline make build-streamlit # Run containers dettached make up-bg # Check running containers docker compose ps -a # Check code quality make check make lint # Format pipelines black ./elt_pipeline # Test coverage make test L√∫c n√†y s·∫Ω c√≥ 11 services sau ƒëang ch·∫°y: Ports MySQL: 3306 PostgreSQL: 5432 Dagit: 3001 MinIO UI: 9001 API: 9000 Spark master: UI: 8080 API: 7077 Pipeline: Spark jobs running: 4040 Metabase: 3030 Streamlit: 8501 4.4 Import data into MySQL # B√¢y gi·ªù ch√∫ng ta import dataset goodreads (d·∫°ng csv) v√†o MySQL. Source t·ª´ng file theo th·ª© t·ª±:\nmake to_mysql_root SET GLOBAL local_infile=TRUE; -- Check if local_infile was turned on SHOW VARIABLES LIKE \u0026#34;local_infile\u0026#34;; exit # Create tables with schema make mysql_create # Load csv into created tables make mysql_load 4.5 Create schema in Postgres # make psql_create 4.6 User interfaces # http://localhost:3001 - Dagit http://localhost:4040 - Spark jobs http://localhost:8080 - Spark master http://localhost:9001 - MinIO http://localhost:3030 - Metabase http://localhost:8501 - Streamlit 5. Detailed code walkthrough # 5.1 Exploratory Data Analysis # Chi ti·∫øt xem ·ªü gist\n5.2 Extract (MySQL/API) # mysql_io_manager\ndef connect_mysql(config) -\u0026gt; str: conn_info = ( f\u0026#34;mysql://{config[\u0026#39;user\u0026#39;]}:{config[\u0026#39;password\u0026#39;]}\u0026#34; + f\u0026#34;@{config[\u0026#39;host\u0026#39;]}:{config[\u0026#39;port\u0026#39;]}\u0026#34; + f\u0026#34;/{config[\u0026#39;database\u0026#39;]}\u0026#34; ) return conn_info def extract_data(self, sql: str) -\u0026gt; pl.DataFrame: \u0026#34;\u0026#34;\u0026#34; Extract data from MySQL database as polars DataFrame \u0026#34;\u0026#34;\u0026#34; conn_info = connect_mysql(self._config) df_data = pl.read_database(query=sql, connection_uri=conn_info) return df_data constants\nCOMPUTE_KIND = \u0026#34;SQL\u0026#34; LAYER = \u0026#34;bronze\u0026#34; YEARLY = StaticPartitionsDefinition( [str(year) for year in range(1975, datetime.today().year)] ) bronze_book\n# book from my_sql @asset( description=\u0026#34;Load table \u0026#39;book\u0026#39; from MySQL database as polars DataFrame, and save to minIO\u0026#34;, partitions_def=YEARLY, io_manager_key=\u0026#34;minio_io_manager\u0026#34;, required_resource_keys={\u0026#34;mysql_io_manager\u0026#34;}, key_prefix=[\u0026#34;bronze\u0026#34;, \u0026#34;goodreads\u0026#34;], compute_kind=COMPUTE_KIND, group_name=LAYER, ) def bronze_book(context) -\u0026gt; Output[pl.DataFrame]: query = \u0026#34;SELECT * FROM book\u0026#34; try: partion_year_str = context.asset_partition_key_for_output() partition_by = \u0026#34;PublishYear\u0026#34; query += f\u0026#34; WHERE {partition_by} = {partion_year_str}\u0026#34; context.log.info(f\u0026#34;Partition by {partition_by} = {partion_year_str}\u0026#34;) except Exception: context.log.info(\u0026#34;No partition key found, full load data\u0026#34;) df_data = context.resources.mysql_io_manager.extract_data(query) context.log.info(f\u0026#34;Table extracted with shape: {df_data.shape}\u0026#34;) return Output( value=df_data, metadata={ \u0026#34;table\u0026#34;: \u0026#34;book\u0026#34;, \u0026#34;row_count\u0026#34;: df_data.shape[0], \u0026#34;column_count\u0026#34;: df_data.shape[1], \u0026#34;columns\u0026#34;: df_data.columns, }, ) bronze_genre\n# genre from my_sql @asset( description=\u0026#34;Load table \u0026#39;genre\u0026#39; from MySQL database as polars DataFrame, and save to minIO\u0026#34;, io_manager_key=\u0026#34;minio_io_manager\u0026#34;, required_resource_keys={\u0026#34;mysql_io_manager\u0026#34;}, key_prefix=[\u0026#34;bronze\u0026#34;, \u0026#34;goodreads\u0026#34;], compute_kind=COMPUTE_KIND, group_name=LAYER, ) def bronze_genre(context) -\u0026gt; Output[pl.DataFrame]: query = \u0026#34;SELECT * FROM genre;\u0026#34; df_data = context.resources.mysql_io_manager.extract_data(query) context.log.info(f\u0026#34;Table extracted with shape: {df_data.shape}\u0026#34;) return Output( value=df_data, metadata={ \u0026#34;table\u0026#34;: \u0026#34;genre\u0026#34;, \u0026#34;row_count\u0026#34;: df_data.shape[0], \u0026#34;column_count\u0026#34;: df_data.shape[1], \u0026#34;columns\u0026#34;: df_data.columns, }, ) C√°c asset kh√°c t∆∞∆°ng t·ª±.\n5.3 Load (datalake - minIO) # minio_io_manager\n@contextmanager def connect_minio(config): client = Minio( endpoint=config.get(\u0026#34;endpoint_url\u0026#34;), access_key=config.get(\u0026#34;minio_access_key\u0026#34;), secret_key=config.get(\u0026#34;minio_secret_key\u0026#34;), secure=False, ) try: yield client except Exception as e: raise e # Make bucket if not exists def make_bucket(client: Minio, bucket_name): found = client.bucket_exists(bucket_name) if not found: client.make_bucket(bucket_name) else: print(f\u0026#34;Bucket {bucket_name} already exists.\u0026#34;) def _get_path(self, context: Union[InputContext, OutputContext]): # E.g context.asset_key.path: [\u0026#39;bronze\u0026#39;, \u0026#39;goodreads\u0026#39;, \u0026#39;book\u0026#39;] layer, schema, table = context.asset_key.path # NOTE: E.g: bronze/goodreads/book key = \u0026#34;/\u0026#34;.join([layer, schema, table.replace(f\u0026#34;{layer}_\u0026#34;, \u0026#34;\u0026#34;)]) # E.g /tmp/file_bronze_goodreads_book_20210101000000.parquet tmp_file_path = \u0026#34;/tmp/file_{}_{}.parquet\u0026#34;.format( \u0026#34;_\u0026#34;.join(context.asset_key.path), datetime.today().strftime(\u0026#34;%Y%m%d%H%M%S\u0026#34;) ) # Partition by year if context.has_partition_key: # E.g partition_str: book_2021 partition_str = str(table) + \u0026#34;_\u0026#34; + context.asset_partition_key # E.g key_name: bronze/goodreads/book/book_2021.parquet # tmp_file_path: /tmp/file_bronze_goodreads_book_20210101000000.parquet return os.path.join(key, f\u0026#34;{partition_str}.parquet\u0026#34;), tmp_file_path else: # E.g key_name: bronze/goodreads/book.parquet return f\u0026#34;{key}.parquet\u0026#34;, tmp_file_path def handle_output(self, context: \u0026#34;OutputContext\u0026#34;, obj: pl.DataFrame): key_name, tmp_file_path = self._get_path(context) # Convert from polars DataFrame to parquet format obj.write_parquet(tmp_file_path) # Upload file to minIO try: bucket_name = self._config.get(\u0026#34;bucket\u0026#34;) with connect_minio(self._config) as client: # Make bucket if not exist make_bucket(client, bucket_name) # Upload file to minIO # E.g bucket_name: lakehouse, # key_name: bronze/goodreads/book/book_2021.parquet, # tmp_file_path: /tmp/file_bronze_goodreads_book_20210101000000.parquet client.fput_object(bucket_name, key_name, tmp_file_path) context.log.info( f\u0026#34;(MinIO handle_output) Number of rows and columns: {obj.shape}\u0026#34; ) context.add_output_metadata({\u0026#34;path\u0026#34;: key_name, \u0026#34;tmp\u0026#34;: tmp_file_path}) # Clean up tmp file os.remove(tmp_file_path) except Exception as e: raise e def load_input(self, context: \u0026#34;InputContext\u0026#34;) -\u0026gt; pl.DataFrame: \u0026#34;\u0026#34;\u0026#34; Prepares input for downstream asset, and downloads parquet file from minIO and converts to polars DataFrame \u0026#34;\u0026#34;\u0026#34; bucket_name = self._config.get(\u0026#34;bucket\u0026#34;) key_name, tmp_file_path = self._get_path(context) try: with connect_minio(self._config) as client: # Make bucket if not exist make_bucket(client=client, bucket_name=bucket_name) # E.g bucket_name: lakehouse, # key_name: bronze/goodreads/book/book_2021.parquet, # tmp_file_path: /tmp/file_bronze_goodreads_book_20210101000000.parquet context.log.info(f\u0026#34;(MinIO load_input) from key_name: {key_name}\u0026#34;) client.fget_object(bucket_name, key_name, tmp_file_path) df_data = pl.read_parquet(tmp_file_path) context.log.info( f\u0026#34;(MinIO load_input) Got polars dataframe with shape: {df_data.shape}\u0026#34; ) return df_data except Exception as e: raise e 5.4 Transform (spark) # # Silver cleaned book @asset( description=\u0026#34;Load book table from bronze layer in minIO, into a Spark dataframe, then clean data\u0026#34;, partitions_def=YEARLY, ins={ \u0026#34;bronze_book\u0026#34;: AssetIn( key_prefix=[\u0026#34;bronze\u0026#34;, \u0026#34;goodreads\u0026#34;], ), }, io_manager_key=\u0026#34;spark_io_manager\u0026#34;, key_prefix=[\u0026#34;silver\u0026#34;, \u0026#34;goodreads\u0026#34;], compute_kind=COMPUTE_KIND, group_name=LAYER, ) def silver_cleaned_book(context, bronze_book: pl.DataFrame): \u0026#34;\u0026#34;\u0026#34; Load book table from bronze layer in minIO, into a Spark dataframe, then clean data \u0026#34;\u0026#34;\u0026#34; config = { \u0026#34;endpoint_url\u0026#34;: os.getenv(\u0026#34;MINIO_ENDPOINT\u0026#34;), \u0026#34;minio_access_key\u0026#34;: os.getenv(\u0026#34;MINIO_ACCESS_KEY\u0026#34;), \u0026#34;minio_secret_key\u0026#34;: os.getenv(\u0026#34;MINIO_SECRET_KEY\u0026#34;), } context.log.debug(\u0026#34;Start creating spark session\u0026#34;) with get_spark_session(config, str(context.run.run_id).split(\u0026#34;-\u0026#34;)[0]) as spark: # Convert bronze_book from polars DataFrame to Spark DataFrame pandas_df = bronze_book.to_pandas() context.log.debug( f\u0026#34;Converted to pandas DataFrame with shape: {pandas_df.shape}\u0026#34; ) spark_df = spark.createDataFrame(pandas_df) spark_df.cache() context.log.info(\u0026#34;Got Spark DataFrame\u0026#34;) # Dedupe books spark_df = spark_df.dropDuplicates() # Drop rows with null value in column \u0026#39;Name\u0026#39; spark_df = spark_df.na.drop(subset=[\u0026#34;Name\u0026#34;]) # Drop rows with null values in column ISBN spark_df = spark_df.na.drop(subset=[\u0026#34;isbn\u0026#34;]) # Drop rows will null values in column \u0026#39;Language\u0026#39; spark_df = spark_df.na.drop(subset=[\u0026#34;Language\u0026#34;]) # Drop rows with value \u0026#39;--\u0026#39; in column \u0026#39;Language\u0026#39; spark_df = spark_df.filter(spark_df.Language != \u0026#34;--\u0026#34;) # Drop rows with value \u0026gt; 350 in column \u0026#39;PagesNumber\u0026#39; spark_df = spark_df.filter(spark_df.PagesNumber \u0026lt;= 350) # Drop column \u0026#39;CountsOfReview\u0026#39; (overlap with \u0026#39;RatingDistTotal\u0026#39;) spark_df = spark_df.drop(\u0026#34;CountsOfReview\u0026#34;) # Choose rows with \u0026#39;PublishYear\u0026#39; from 1900 to datetime.today().year spark_df = spark_df.filter( (spark_df.PublishYear \u0026gt;= 1975) \u0026amp; (spark_df.PublishYear \u0026lt;= datetime.today().year) ) # Update value of column \u0026#39;RatingDist...\u0026#39;, splitting by \u0026#39;:\u0026#39; and take the second value spark_df = spark_df.withColumn( \u0026#34;RatingDist5\u0026#34;, split_take_second(col(\u0026#34;RatingDist5\u0026#34;)) ) spark_df = spark_df.withColumn( \u0026#34;RatingDist4\u0026#34;, split_take_second(col(\u0026#34;RatingDist4\u0026#34;)) ) spark_df = spark_df.withColumn( \u0026#34;RatingDist3\u0026#34;, split_take_second(col(\u0026#34;RatingDist3\u0026#34;)) ) spark_df = spark_df.withColumn( \u0026#34;RatingDist2\u0026#34;, split_take_second(col(\u0026#34;RatingDist2\u0026#34;)) ) spark_df = spark_df.withColumn( \u0026#34;RatingDist1\u0026#34;, split_take_second(col(\u0026#34;RatingDist1\u0026#34;)) ) spark_df = spark_df.withColumn( \u0026#34;RatingDistTotal\u0026#34;, split_take_second(col(\u0026#34;RatingDistTotal\u0026#34;)) ) # Cast column \u0026#39;RatingDist...\u0026#39; to Interger spark_df = spark_df.withColumn( \u0026#34;RatingDist5\u0026#34;, spark_df.RatingDist5.cast(\u0026#34;Integer\u0026#34;) ) spark_df = spark_df.withColumn( \u0026#34;RatingDist4\u0026#34;, spark_df.RatingDist4.cast(\u0026#34;Integer\u0026#34;) ) spark_df = spark_df.withColumn( \u0026#34;RatingDist3\u0026#34;, spark_df.RatingDist3.cast(\u0026#34;Integer\u0026#34;) ) spark_df = spark_df.withColumn( \u0026#34;RatingDist2\u0026#34;, spark_df.RatingDist2.cast(\u0026#34;Integer\u0026#34;) ) spark_df = spark_df.withColumn( \u0026#34;RatingDist1\u0026#34;, spark_df.RatingDist1.cast(\u0026#34;Integer\u0026#34;) ) spark_df = spark_df.withColumn( \u0026#34;RatingDistTotal\u0026#34;, spark_df.RatingDistTotal.cast(\u0026#34;Integer\u0026#34;) ) # Change column name \u0026#39;Count of text reviews\u0026#39; to \u0026#39;CountOfTextReviews\u0026#39; spark_df = spark_df.withColumnRenamed( \u0026#34;Count of text reviews\u0026#34;, \u0026#34;CountOfTextReviews\u0026#34; ) # Change value of column \u0026#39;Language\u0026#39; from [\u0026#39;en-US\u0026#39;, \u0026#39;en-GB\u0026#39;, \u0026#39;en-CA\u0026#39;], to \u0026#39;eng\u0026#39;, from \u0026#39;nl\u0026#39; to \u0026#39;nld\u0026#39; spark_df = spark_df.withColumn( \u0026#34;Language\u0026#34;, regexp_replace(\u0026#34;Language\u0026#34;, \u0026#34;en-US\u0026#34;, \u0026#34;eng\u0026#34;) ) spark_df = spark_df.withColumn( \u0026#34;Language\u0026#34;, regexp_replace(\u0026#34;Language\u0026#34;, \u0026#34;en-GB\u0026#34;, \u0026#34;eng\u0026#34;) ) spark_df = spark_df.withColumn( \u0026#34;Language\u0026#34;, regexp_replace(\u0026#34;Language\u0026#34;, \u0026#34;en-CA\u0026#34;, \u0026#34;eng\u0026#34;) ) spark_df = spark_df.withColumn( \u0026#34;Language\u0026#34;, regexp_replace(\u0026#34;Language\u0026#34;, \u0026#34;nl\u0026#34;, \u0026#34;nld\u0026#34;) ) spark_df.unpersist() return Output( value=spark_df, metadata={ \u0026#34;table\u0026#34;: \u0026#34;silver_cleaned_book\u0026#34;, \u0026#34;row_count\u0026#34;: spark_df.count(), \u0026#34;column_count\u0026#34;: len(spark_df.columns), \u0026#34;columns\u0026#34;: spark_df.columns, }, ) # Silver collected book @asset( description=\u0026#34;Collect more infomation about cleaned books, such as authors, number of pages\u0026#34;, partitions_def=YEARLY, ins={ \u0026#34;silver_cleaned_book\u0026#34;: AssetIn( key_prefix=[\u0026#34;silver\u0026#34;, \u0026#34;goodreads\u0026#34;], metadata={\u0026#34;full_load\u0026#34;: False}, ), }, io_manager_key=\u0026#34;spark_io_manager\u0026#34;, key_prefix=[\u0026#34;silver\u0026#34;, \u0026#34;goodreads\u0026#34;], compute_kind=\u0026#34;OpenLibrary API\u0026#34;, group_name=LAYER, ) def silver_collected_book(context, silver_cleaned_book: DataFrame) -\u0026gt; Output[DataFrame]: \u0026#34;\u0026#34;\u0026#34; Collect more infomation about cleaned books - Authors: if missing - Number of pages: if missing \u0026#34;\u0026#34;\u0026#34; spark_df = silver_cleaned_book context.log.debug(\u0026#34;Caching spark_df ...\u0026#34;) spark_df.cache() context.log.info(\u0026#34;Starting filling missing data ...\u0026#34;) null_authors_df = spark_df.filter( (spark_df.Authors.isNull()) | (spark_df.Authors == \u0026#34;\u0026#34;) ) null_pages_number_df = spark_df.filter((spark_df.PagesNumber.isNull())) count = 0 for row in null_authors_df.select(\u0026#34;ISBN\u0026#34;).collect(): isbn = row[0] context.log.debug(f\u0026#34;Got isbn: {isbn}\u0026#34;) if isbn is not None: # Get request from OpenLibrary API req = requests.get( f\u0026#34;https://openlibrary.org/api/books?bibkeys=ISBN:{isbn}\u0026amp;format=json\u0026amp;jscmd=data\u0026#34; ) json = req.json() if len(json.keys()) \u0026gt; 0: context.log.debug(\u0026#34;Got json with data\u0026#34;) # Check if spark_df with column \u0026#39;ISBN\u0026#39; = isbn has missing value in column \u0026#39;Authors\u0026#39; row_from_df = spark_df.filter(spark_df.ISBN == isbn).collect()[0] if row_from_df.Authors is None or row_from_df.Authors is \u0026#34;\u0026#34;: context.log.debug(\u0026#34;Authors is missing, start filling ...\u0026#34;) # Take the first author author = json.get(f\u0026#34;ISBN:{isbn}\u0026#34; or {}).get(\u0026#34;authors\u0026#34; or []) author = author[0].get(\u0026#34;name\u0026#34;) if len(author) \u0026gt; 0 else None if author: count += 1 # Update spark_df with column \u0026#39;ISBN\u0026#39; = isbn and column \u0026#39;Authors\u0026#39; = author spark_df = spark_df.withColumn( \u0026#34;Authors\u0026#34;, when( (spark_df.ISBN == isbn) \u0026amp; ( (spark_df.Authors.isNull()) | (spark_df.Authors == \u0026#34;\u0026#34;) ), author, ).otherwise(spark_df.Authors), ) context.log.info(f\u0026#34;Filled in {count} authors\u0026#34;) count = 0 for row in null_pages_number_df.select(\u0026#34;ISBN\u0026#34;).collect(): isbn = row[0] context.log.debug(f\u0026#34;Got isbn: {isbn}\u0026#34;) if isbn is not None: # Get request from OpenLibrary API req = requests.get( f\u0026#34;https://openlibrary.org/api/books?bibkeys=ISBN:{isbn}\u0026amp;format=json\u0026amp;jscmd=data\u0026#34; ) json = req.json() if len(json.keys()) \u0026gt; 0: context.log.debug(\u0026#34;Got json with real data\u0026#34;) # Check if spark_df with column \u0026#39;ISBN\u0026#39; = isbn has missing value in column \u0026#39;Authors\u0026#39; row_from_df = spark_df.filter(spark_df.ISBN == isbn).collect()[0] # Check if spark_df with column \u0026#39;ISBN\u0026#39; = isbn has missing value in column \u0026#39;PagesNumber\u0026#39; if row_from_df.PagesNumber is None or row_from_df.PagesNumber == 0: context.log.debug(\u0026#34;PagesNumber is missing, start filling ...\u0026#34;) # Take the number of pages pages_number = json.get(f\u0026#34;ISBN:{isbn}\u0026#34; or {}).get(\u0026#34;number_of_pages\u0026#34;) if pages_number: count += 1 # Update spark_df with column \u0026#39;ISBN\u0026#39; = isbn and column \u0026#39;PagesNumber\u0026#39; = pages_number spark_df = spark_df.withColumn( \u0026#34;PagesNumber\u0026#34;, when( (spark_df.ISBN == isbn) \u0026amp; (spark_df.PagesNumber.isNull()), pages_number, ).otherwise(spark_df.PagesNumber), ) context.log.info(f\u0026#34;Filled in {count} pages numbers\u0026#34;) spark_df.unpersist() return Output( value=spark_df, metadata={ \u0026#34;table\u0026#34;: \u0026#34;silver_collected_book\u0026#34;, \u0026#34;row_count\u0026#34;: spark_df.count(), \u0026#34;column_count\u0026#34;: len(spark_df.columns), \u0026#34;columns\u0026#34;: spark_df.columns, }, ) 5.5 Load (data warehouse - Postgres) # psql_io_manager\n@contextmanager def connect_psql(config): try: yield psycopg2.connect( host=config[\u0026#34;host\u0026#34;], port=config[\u0026#34;port\u0026#34;], database=config[\u0026#34;database\u0026#34;], user=config[\u0026#34;user\u0026#34;], password=config[\u0026#34;password\u0026#34;], ) except (Exception) as e: print(f\u0026#34;Error while connecting to PostgreSQL: {e}\u0026#34;) def handle_output(self, context: \u0026#34;OutputContext\u0026#34;, obj: pl.DataFrame): # E.g context.asset_key.path = [\u0026#39;warehouse\u0026#39;, \u0026#39;gold\u0026#39;, \u0026#39;book_genre\u0026#39;] schema = context.asset_key.path[-2] # NOTE: Replace pattern is \u0026#39;warehouse\u0026#39;, not general table = str(context.asset_key.path[-1]).replace(\u0026#34;warehouse_\u0026#34;, \u0026#34;\u0026#34;) context.log.debug(f\u0026#34;Schema: {schema}, Table: {table}\u0026#34;) tmp_tbl = f\u0026#34;{table}_tmp_{datetime.now().strftime(\u0026#39;%Y_%m_%d\u0026#39;)}\u0026#34; try: with connect_psql(self._config) as conn: context.log.debug(f\u0026#34;Connected to PostgreSQL: {conn}\u0026#34;) primary_keys = (context.metadata or {}).get(\u0026#34;primary_keys\u0026#34;, []) context.log.debug(f\u0026#34;Primary keys: {primary_keys}\u0026#34;) with conn.cursor() as cursor: context.log.debug(f\u0026#34;Cursor info: {cursor}\u0026#34;) cursor.execute(\u0026#34;SELECT version()\u0026#34;) context.log.info(f\u0026#34;PostgreSQL version: {cursor.fetchone()}\u0026#34;) # Create temp file cursor.execute( f\u0026#34;CREATE TEMP TABLE IF NOT EXISTS {tmp_tbl} (LIKE {schema}.{table})\u0026#34; ) cursor.execute(f\u0026#34;SELECT COUNT(*) FROM {tmp_tbl}\u0026#34;) context.log.debug( f\u0026#34;Log for creating temp table: {cursor.fetchone()}\u0026#34; ) # Create sql identifiers for the column names # Do this to safely insert into a sql query columns = sql.SQL(\u0026#34;,\u0026#34;).join( sql.Identifier(name.lower()) for name in obj.columns ) # Create a placeholder for the values. These will be filled later values = sql.SQL(\u0026#34;,\u0026#34;).join(sql.Placeholder() for _ in obj.columns) # Create the insert query context.log.debug(\u0026#34;Inserting data into temp table\u0026#34;) insert_query = sql.SQL(\u0026#34;INSERT INTO {} ({}) VALUES({});\u0026#34;).format( sql.Identifier(tmp_tbl), columns, values ) # Execute the insert query psycopg2.extras.execute_batch(cursor, insert_query, obj.rows()) conn.commit() # Check data inserted context.log.debug(\u0026#34;Checking data inserted\u0026#34;) cursor.execute(f\u0026#34;SELECT COUNT(*) FROM {tmp_tbl};\u0026#34;) context.log.info(f\u0026#34;Number of rows inserted: {cursor.fetchone()}\u0026#34;) # Upsert data if len(primary_keys) \u0026gt; 0: context.log.debug(\u0026#34;Table has primary keys, upserting data\u0026#34;) conditions = \u0026#34; AND \u0026#34;.join( [ f\u0026#34;\u0026#34;\u0026#34; {schema}.{table}.\u0026#34;{k}\u0026#34; = {tmp_tbl}.\u0026#34;{k}\u0026#34; \u0026#34;\u0026#34;\u0026#34; for k in primary_keys ] ) command = f\u0026#34;\u0026#34;\u0026#34; BEGIN TRANSACTION; DELETE FROM {schema}.{table} USING {tmp_tbl} WHERE {conditions}; INSERT INTO {schema}.{table} SELECT * FROM {tmp_tbl}; END TRANSACTION; \u0026#34;\u0026#34;\u0026#34; else: context.log.debug(\u0026#34;Table has no primary keys, replacing data\u0026#34;) command = f\u0026#34;\u0026#34;\u0026#34; BEGIN TRANSACTION; DELETE FROM {schema}.{table}; INSERT INTO {schema}.{table} SELECT * FROM {tmp_tbl}; END TRANSACTION; \u0026#34;\u0026#34;\u0026#34; # context.log.debug(f\u0026#34;Command: {command}\u0026#34;) context.log.debug(f\u0026#34;Upserting data into {schema}.{table}\u0026#34;) cursor.execute(command) context.log.debug(f\u0026#34;{cursor.statusmessage}\u0026#34;) conn.commit() except (Exception) as e: print(f\u0026#34;Error while handling output to PostgreSQL: {e}\u0026#34;) try: with connect_psql(self._config) as conn: with conn.cursor() as cursor: context.log.debug(f\u0026#34;{cursor.fetchone()}\u0026#34;) cursor.execute(f\u0026#34;SELECT COUNT(*) FROM {schema}.{table};\u0026#34;) context.log.info( f\u0026#34;Number of rows upserted in {schema}.{table}: {cursor.fetchone()}\u0026#34; ) # Drop temp table cursor.execute(f\u0026#34;DROP TABLE {tmp_tbl}\u0026#34;) conn.commit() except (Exception) as e: print(f\u0026#34;Error while testing handle_output to PostgreSQL: {e}\u0026#34;) 5.6 Transform (dbt) # search\nselect isbn, name from {{ source(\u0026#39;gold\u0026#39;, \u0026#39;book_with_info\u0026#39;) }} search_prior\nselect isbn, name from {{ source(\u0026#39;gold\u0026#39;, \u0026#39;book_with_info\u0026#39;) }} right join {{ source(\u0026#39;recommendations\u0026#39;, \u0026#39;book_download_link\u0026#39;) }} using (isbn) where link is not null criteria\nwith tmp_avg_rating as ( select isbn, rating from {{ source(\u0026#39;gold\u0026#39;, \u0026#39;book_with_rating\u0026#39;) }} ), tmp_download_link as ( select isbn, case when link is null then 0 else 1 end as hasdownloadlink, rating from {{ source(\u0026#39;recommendations\u0026#39;, \u0026#39;book_download_link\u0026#39;) }} right join tmp_avg_rating using (isbn) ) select * from tmp_download_link 6. Tear down infrastructure # D·ª° b·ªè containers sau khi xong vi·ªác\nmake down 7. Considerations # Gi·ªù l√† l√∫c ƒë√°nh gi√° project:\nT·ªëc ƒë·ªô: spark ƒë∆∞·ª£c c√†i ·ªü ch·∫ø ƒë·ªô standalone n√™n kh√¥ng ƒë·∫°t hi·ªáu su·∫•t cao, ƒë√¥i khi b·ªã crash gi·ªØa ch·ª´ng khi th·ª±c hi·ªán c√°c task shuffle/read/write M√¥i tr∆∞·ªùng ph√°t tri·ªÉn: Hi·ªán t·∫°i m·ªõi c√≥ develop, t∆∞∆°ng lai s·∫Ω xem x√©t m√¥i tr∆∞·ªùng testing, staging, production dbt hi·ªán t·∫°i l√† project nh·ªè, t∆∞∆°ng lai n·∫øu c·∫ßn transform nhi·ªÅu th√¨ c·∫ßn t√°ch service ri√™ng, ph√¢n quy·ªÅn, \u0026hellip; Deployment: S·ª≠ d·ª•ng m·ªôt trong c√°c d·ªãnh v·ª• ƒëi·ªán to√°n ƒë√°m m√¢y: AWS, Azure, GCP 8. Further actions # Ho√†n thi·ªán recommender system T√≠ch h·ª£p jupyter notebook ƒë·ªÉ l√†m task c·ªßa data scientist - dagstermill Testing environment Continuous Integration v·ªõi Github Actions ","date":"2023-04-16","permalink":"/projects/fde2-goodreads-elt-pipeline/","section":"Projects","summary":"X√¢y d·ª±ng ELT data pipelines ho√†n ch·ªânh v·ªõi b·ªô d·ªØ li·ªáu s√°ch t·ª´ Goodreads","title":"Goodreads ELT pipelines"},{"content":"","date":"2023-04-16","permalink":"/tags/minio/","section":"Tags","summary":"","title":"minio"},{"content":"1. T·∫°i sao n√™n ƒë·ªçc b√†i n√†y? # Chuy·ªán l√† m√¨nh v·ª´a ƒë·ªçc ƒë∆∞·ª£c m·ªôt b√†i r·∫•t t√¢m huy·∫øt v·ªÅ c√°c zsh frameworks tr√™n Reddit, trong ƒë√≥ c√≥ ƒëo·∫°n vi·∫øt nh∆∞ n√†y:\nThey took the best ideas, and wrote them as brand new modules with even higher-quality code, and ended up becoming by far the fastest \u0026ldquo;full-featured zsh framework\u0026rdquo; with the fastest startup time, most efficient Git status prompt updates, etc. Its clean code organization ideas were inspired by Prezto, and some of the modules (such as the git module) were taken from Prezto and cleaned up and refactored to be faster and better. Most modules were written from scratch. The entire project is very minimalistic with super clean, tiny code modules, and it\u0026rsquo;s well-organized within the \u0026ldquo;ZimFW\u0026rdquo; organization on GitHub.\nT·∫°m d·ªãch: H·ªç ch·∫Øt l·ªçc nh·ªØng √Ω t∆∞·ªüng t·ªët nh·∫•t, vi·∫øt l·∫°i nh·ªØng modules ho√†n to√†n m·ªõi v·ªõi code ch·∫•t l∆∞·ª£ng cao, t·∫°o th√†nh m·ªôt zsh framework ƒë·∫ßy ƒë·ªß t√≠nh nƒÉng, v·ªõi th·ªùi gian kh·ªüi ƒë·ªông nhanh nh·∫•t, nhi·ªÅu updates hi·ªáu qu·∫£ nh·∫•t, etc. C·∫•u tr√∫c code s·∫°ch ƒë·∫πp ƒë∆∞·ª£c truy·ªÅn c·∫£m h·ª©ng b·ªüi Prezto, v√† m·ªôt v√†i modules (v√≠ d·ª• nh∆∞ git module) l·∫•y t·ª´ Prezto ƒë∆∞·ª£c clean v√† refactor. H·∫ßu h·∫øt modules ƒë∆∞·ª£c vi·∫øt l·∫°i t·ª´ ƒë·∫ßu. To√†n b·ªô project ƒë·ªÅu t·ªëi gi·∫£n v·ªõi modules si√™u s·∫°ch, g·ªçn, n·∫±m trong \u0026ldquo;ZimFW\u0026rdquo; organization tr√™n GitHub. (\u0026hellip;)\nWow, c≈©ng ƒë√°ng ƒë·ªÉ th·ª≠ ƒë·∫•y ch·ª©! C·ªông th√™m vi·ªác m√¨nh ƒë√£ qu√° ng√°n v·ªõi con zsh c·ªßa m√¨nh khi l√∫c n√†o b·∫≠t l√™n c≈©ng t·ªën ~0.5 - 1 gi√¢y (qu√° ch·∫≠m cho 1 shell), v·∫≠y l√† qu√° ƒë·ªß l√Ω do ƒë·ªÉ t·ª´ b·ªè oh-my-zsh. V√† th·∫ø l√† c√¥ng cu·ªôc chuy·ªÉn nh√† t·ª´ oh-my-zsh c·ªßa m√¨nh sang Zim b·∫Øt ƒë·∫ßu.\nzimfw/zimfw Zim: Modular, customizable, and blazing fast Zsh framework HTML 3411 179 2. C√†i ƒë·∫∑t # ƒê·∫ßu ti√™n b·∫°n c·∫ßn g·ª° b·ªè oh-my-zsh tr∆∞·ªõc:\nsource ~/.oh-my-zsh/tools/uninstall.sh Sau ƒë·∫•y kh·ªüi ƒë·ªông l·∫°i terminal, zsh s·∫Ω tr·ªëng tr∆°n. Ti·∫øp theo, c√†i Zim b·∫±ng curl:\ncurl -fsSL https://raw.githubusercontent.com/zimfw/install/master/install.zsh | zsh Ho·∫∑c b·∫±ng wget:\nwget -nv -O - https://raw.githubusercontent.com/zimfw/install/master/install.zsh | zsh N·∫øu kh√¥ng th√†nh c√¥ng, h√£y th·ª≠ c√†i th·ªß c√¥ng ·ªü link n√†y. Kh·ªüi ƒë·ªông l·∫°i terminal v√† ki·ªÉm tra Zim ƒë√£ ƒë∆∞·ª£c c√†i th√†nh c√¥ng ch∆∞a:\nzimfw Zim themes # Theme m·∫∑c ƒë·ªãnh c·ªßa Zim l√† asciiship, b·∫°n c√≥ th·ªÉ ch·ªçn themes ƒë∆∞·ª£c Zim build s·∫µn ho·∫∑c theme kh√°c t√πy √Ω. V√≠ d·ª• ·ªü ƒë√¢y m√¨nh s·∫Ω d√πng theme eriner, v√†o file ~/.zimrc, comment d√≤ng zmodule asciiship v√† th√™m zmodule enriner:\n# ~/.zimrc # Theme enriner # zmodule asciiship zmodule eriner Sau ƒë√≥ ch·∫°y l·ªánh:\nzimfw uninstall \u0026amp;\u0026amp; zimfw install Kh·ªüi ƒë·ªông l·∫°i terminal v√† enjoy!\nNh∆∞ v·∫≠y t·ªõi ƒë√¢y b·∫°n ƒë√£ bi·∫øt c√°ch th√™m 1 zsh plugin v√†o b·∫±ng Zim: th√™m zmodule + t√™n plugin v√†o file ~/.zimrc, r·ªìi ch·∫°y l·ªánh zimfw install.\nC√°c module ƒë∆∞·ª£c c√†i n·∫±m ·ªü ~/.zim/modules\nNh∆∞ng v√¨ m√¨nh quen d√πng powerlevel10k r·ªìi n√™n m√¨nh s·∫Ω c√†i powerlevel10k:\n# ~/.zimrc # ... config modules zmodule romkatv/powerlevel10k zimfw install, kh·ªüi ƒë·ªông l·∫°i terminal r·ªìi p10k configure.\np10k sequence ∆∞a th√≠ch c·ªßa m√¨nh l√† y y y n 3 1 3 4 4 5 2 3 4 2 2 1 1 y 2 y Done! Kh·ªüi ƒë·ªông l·∫°i terminal v√† enjoy. Modules h·ªó tr·ª£ # M·∫∑c ƒë·ªãnh Zim ƒë√£ c√†i s·∫µn cho b·∫°n v√†i plugin nh∆∞ git, zsh-autosuggestions, zsh-syntax-highlighting, etc. B·∫°n c√≥ th·ªÉ d√πng lu√¥n m√† kh√¥ng c·∫ßn config g√¨ th√™m. Nh∆∞ng ·ªü ƒë√¢y m√¨nh mu·ªën shell c·ªßa m√¨nh ƒë∆∞·ª£c vi·ªác h∆°n t√≠ n√™n s·∫Ω c√†i th√™m zoxide v√† zsh-autoswitch-virtualenv.\nZoxide l√† m·ªôt tool t·ª± ƒë·ªông nh·ªõ c√°c ƒë∆∞·ªùng d·∫´n m√† b·∫°n ƒë√£ cd, gi√∫p b·∫°n di chuy·ªÉn nhan h∆°n trong terminal. ajeetdsouza/zoxide A smarter cd command. Supports all major shells. Rust 13034 423 N·∫øu ch∆∞a c√≥ zoxide, h√£y c√†i n√≥ v√†o tr∆∞·ªõc:\n# Ubuntu sudo apt-get update sudo apt-get install zoxide Th√™m d√≤ng n√†y v√†o ~/.zshrc:\n# Zoxide eval \u0026#34;$(zoxide init zsh)\u0026#34; Sau ƒë√≥ c√†i module v√†o ~/.zimrc:\n# ~/.zimrc # ... config modules # Zoxide zmodule kiesman99/zim-zoxide Sau ƒë·∫•y th√¨ zimfw install th√¥i :D zsh-autoswitch-virtualenv l√† tool gi√∫p t·ª± activate/deactivate khi b·∫°n cd v√†o project c√≥ virtualenv c·ªßa python. N√≥ h·ªó tr·ª£ nhi·ªÅu lo·∫°i cho b·∫°n nh∆∞ venv, pipenv, poetry b·∫±ng c√°ch nh·∫≠n di·ªán m·ªôt trong c√°c file sau (xem th√™m ·ªü GitHub): MichaelAquilina/zsh-autoswitch-virtualenv üêç ZSH plugin to automatically switch python virtualenvs (including pipenv and poetry) as you move between directories Shell 449 72 setup.py requirements.txt Pipfile poetry.lock C√°c b∆∞·ªõc c√†i c≈©ng nh∆∞ tr∆∞·ªõc, th√™m ƒëo·∫°n d∆∞·ªõi v√†o cu·ªëi file ~/.zimrc r·ªìi zimfw install:\n# ~/.zimrc # ... config modules # Auto-switch python virtualenv zmodule MichaelAquilina/zsh-autoswitch-virtualenv ƒê·∫øn ƒë√¢y module ƒë√£ ƒë∆∞·ª£c download v·ªÅ ~/.zim/modules, nh∆∞ng Zim ch∆∞a source ƒë∆∞·ª£c n√≥ v√¨ trong module kh√¥ng c√≥ file init.zsh. ƒê·ªÉ kh·∫Øc ph·ª•c, v√†o ~/.zim/modules/zsh-autoswitch-virtualenv ch·∫°y c√°c l·ªánh\ntouch init.zsh cp autoswitch-virtualenv.plugin.zsh init.zsh zimfw build Ki·ªÉm tra file ~/.zim/init.zsh c√≥ module n√†y l√† OK. Module s·∫Ω t·ª± ƒë·ªông activate/deactivate khi ra/v√†o project python c√≥ virtualenv M·ªôt v√†i config kh√°c # M·ªôt v√†i config kh√°c gi√∫p m√¨nh l√†m vi·ªác nhanh v√† hi·ªáu qu·∫£ h∆°n:\nSource Ruby v·ªõi rvm, khi d√πng ch·ªâ c·∫ßn g√µ source_ruby: # ~/.zshrc # Add RVM to PATH for scripting. Make sure this is the last PATH variable change. export PATH=\u0026#34;$PATH:$HOME/.rvm/bin\u0026#34; alias source_ruby=\u0026#34;source $HOME/.rvm/scripts/rvm\u0026#34; D√πng exa thay cho ls, exa nhanh v√† cho m√†u ƒë·∫πp h∆°n ls: ogham/exa A modern replacement for ‚Äòls‚Äô. Rust 22954 677 # ~/.zshrc # Use exa instead of ls alias ls=\u0026#34;exa\u0026#34; alias la=\u0026#34;exa -a\u0026#34; alias ll=\u0026#34;exa -l\u0026#34; D√πng bat thay cho cat: # ~/.zshrc # Use bat instead for cat alias cat=\u0026#34;bat\u0026#34; Shortcut cho tmux: # ~/.zshrc # Map tmux shortcut alias ta=\u0026#34;tmux a\u0026#34; alias tl=\u0026#34;tmux ls\u0026#34; K·∫øt lu·∫≠n # ∆Øu ƒëi·ªÉm:\nZim cho m√¨nh t·ªëc ƒë·ªô nhanh h∆°n h·∫≥n oh-my-zsh t·ª´ng d√πng, m·ªü c√°i l√† l√™n li·ªÅn ch·ª© kh√¥ng c·∫ßn ƒë·ª£i nh∆∞ oh-my-zsh D·ªÖ d√πng, ch·ªâ c·∫ßn nh·ªõ v√†i c√¢u l·ªánh nh∆∞ zimfw install, uninstall, compile, build l√† ƒë∆∞·ª£c Nh∆∞·ª£c ƒëi·ªÉm:\nC·∫ßn m·ªôt ch√∫t th·ªùi gian th√≠ch nghi Ph·∫£i c·∫ßn file init.zsh trong m·ªói module ƒë·ªÉ load v√†o shell M·ªôt framework t·ªëi gi·∫£n, th√∫ v·ªã. M√¨nh s·∫Ω t√¨m hi·ªÉu s√¢u h∆°n v√† ƒë∆∞a ra c√°c updates sau n√†y.\n","date":"2023-01-31","permalink":"/posts/oh-my-zsh-to-zim/","section":"Posts","summary":"H∆∞·ªõng d·∫´n c√°ch chuy·ªÉn t·ª´ oh-my-zsh sang zim v√† b∆∞·ªõc ƒë·∫ßu l√†m quen v·ªõi framework n√†y","title":"Chuy·ªÉn nh√† t·ª´ oh-my-zsh sang zimfw"},{"content":"","date":"2023-01-31","permalink":"/categories/linux/","section":"Categories","summary":"","title":"linux"},{"content":"","date":"2023-01-31","permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":"2023-01-31","permalink":"/tags/zimfw/","section":"Tags","summary":"","title":"zimfw"},{"content":"","date":"2023-01-31","permalink":"/tags/zsh/","section":"Tags","summary":"","title":"zsh"},{"content":"1. T·∫°i sao n√™n ƒë·ªçc b√†i n√†y? # V·ªõi nh·ªØng ai y√™u th√≠ch Neovim v√† s·ª≠ d·ª•ng n√≥ l√†m IDE ch√≠nh, b·∫°n s·∫Ω ƒë√¥i l√∫c g·∫∑p v·∫•n ƒë·ªÅ kh√≥ ch·ªãu khi code Ruby tr√™n Neovim nh∆∞ thi·∫øu suggestion, lint, format, \u0026hellip; Nh√¢n d·ªãp nƒÉm m·ªõi 2023, m√¨nh s·∫Ω h∆∞·ªõng d·∫´n c√°c b·∫°n c√°ch setup m√¥i tr∆∞·ªùng ph√°t tri·ªÉn Ruby b·∫±ng Neovim s·ª≠ d·ª•ng LSP Solargraph.\nSolargraph l√† m·ªôt language server cung c·∫•p IntelliSense, code completion v√† docs cho Ruby. N√≥ h·ªó tr·ª£ nhi·ªÅu IDE v√† text editor nh∆∞ VSCode, Emacs, Atom, etc v√† dƒ© nhi√™n, Neovim. Trang ch·ªß c·ªßa solargraph.\n2. Y√™u c·∫ßu # ƒê·ªÉ setup b·∫°n c·∫ßn config tr∆∞·ªõc nh·ªØng th·ª© sau:\nRuby c√†i v√† ch·∫°y ƒë∆∞·ª£c Neovim version 0.5.0, ƒë√£ c√†i mason v√† config LSP 3. C√†i ƒë·∫∑t # ƒê·∫ßu ti√™n b·∫°n c·∫ßn c√†i gem solargraph:\ngem install solargraph Ki·ªÉm tra solargraph ƒë√£ ƒë∆∞·ª£c c√†i ch∆∞a:\ngem list | grep solargraph # N·∫øu ch∆∞a th√¨ d√πng command sau gem install --user-install solargraph Ho·∫∑c th√™m d√≤ng sau v√†o Gemfile:\ngem \u0026#39;solargraph\u0026#39;, group: :development Ti·∫øp theo v√†o Neovim v√† m·ªü mason: :Mason, t√¨m v√† c√†i 2 g√≥i sau: solargraph v√† rubocop.\nN·∫øu c√≥ config LSP theo dotfiles c·ªßa m√¨nh, b·∫°n ch·ªâ c·∫ßn th√™m ƒëo·∫°n sau v√†o ~/config/nvim/lua/$USER/plugins/lsp/lspconfig.lua:\n-- ... -- configure ruby language server lspconfig[\u0026#34;solargraph\u0026#34;].setup({ capabilities = capabilities, on_attach = on_attach, filetypes = { \u0026#34;ruby\u0026#34; }, }) -- ... Xong! B√¢y gi·ªù ch·ªâ c·∫ßn m·ªü file Ruby v√† check :LspInfo, l√∫c n√†y s·∫Ω hi·ªán 1 client c√≥ t√™n solargraph ƒë∆∞·ª£c attach v√†o buffer .rb:\n","date":"2023-01-01","permalink":"/posts/neovim-setup-ruby/","section":"Posts","summary":"H∆∞·ªõng d·∫´n setup m√¥i tr∆∞·ªùng ph√°t tri·ªÉn Ruby tr√™n Neovim","title":"Code Ruby tr√™n Neovim"},{"content":"","date":"2023-01-01","permalink":"/tags/ruby/","section":"Tags","summary":"","title":"ruby"},{"content":"","date":"2023-01-01","permalink":"/categories/vim/","section":"Categories","summary":"","title":"vim"},{"content":"","date":"2022-12-18","permalink":"/tags/aws/","section":"Tags","summary":"","title":"AWS"},{"content":"","date":"2022-12-18","permalink":"/tags/spotify/","section":"Tags","summary":"","title":"spotify"},{"content":"1. Introduction # Trong project n√†y m√¨nh s·∫Ω h∆∞·ªõng d·∫´n x√¢y d·ª±ng m·ªôt data pipeline c∆° b·∫£n theo m√¥ h√¨nh ELT (extract - load - transform), s·ª≠ d·ª•ng b·ªô d·ªØ li·ªáu t·ª´ spotify ƒë·ªÉ ph√¢n t√≠ch xu h∆∞·ªõng nghe nh·∫°c.\nProject n√†y ho√†n th√†nh d·ª±a tr√™n ki·∫øn th·ª©c ƒë√£ h·ªçc ƒë∆∞·ª£c t·ª´ kh√≥a Fundamental Data Engineering c·ªßa AIDE. Xin g·ª≠i l·ªùi c·∫£m ∆°n ƒë·∫∑c bi·ªát t·ªõi th·∫ßy Nguy·ªÖn Thanh B√¨nh, anh √îng Xu√¢n H·ªìng v√† anh H√πng L√™.\nlelouvincx/spotify-data-pipeline-ingestion A basic data pipeline follows ELT principle, to ingest data from MySQL and Spotify Open API Jupyter Notebook 1 2 2. Objective # M·ª•c ti√™u c·ªßa project n√†y l√† x√¢y d·ª±ng m·ªôt data pipeline ƒë·ªÉ ƒë∆∞a d·ªØ li·ªáu c·ªßa b·∫£ng spotify_tracks t·ª´ mySQL v√† my_tracks t·ª´ API c·ªßa Spotify th√†nh dashboard ƒë·ªÉ ph√¢n t√≠ch. C√°c b·∫£ng ƒë∆∞·ª£c h·ªó tr·ª£ b·ªüi spotify_albums v√† spotify_artists nh∆∞ m√¥ t·∫£ d∆∞·ªõi ƒë√¢y:\nspotify_tracks: OLTP table ch·ª©a th√¥ng tin b√†i h√°t t·ª´ spotify my_tracks: l·ªãch s·ª≠ stream nh·∫°c c·ªßa b·∫£n th√¢n, l·∫•y schema gi·ªëng spotify_tracks spotify_albums: ch·ª©a th√¥ng tin albums t·ª´ dataset spotify_artists: th√¥ng tin ngh·ªá sƒ© t·ª´ dataset table schema Chi ti·∫øt h∆°n xem ·ªü: Exploratory Data Analysis\n3. Design # 3.1 Pipeline design # Ch√∫ng ta s·ª≠ d·ª•ng m√°y ·∫£o AWS EC2 ƒë·ªÉ t√≠nh to√°n v√† dagster ƒë·ªÉ orchestrate tasks.\nD·ªØ li·ªáu spotify ƒë∆∞·ª£c download t·ª´ kaggle d∆∞·ªõi d·∫°ng csv, sau ƒë√≥ import v√†o mySQL m√¥ ph·ªèng l√†m d·ªØ li·ªáu doanh nghi·ªáp D·ªØ li·ªáu streaming history c·ªßa b·∫£n th√¢n ƒë∆∞·ª£c extract t·ª´ spotify API Extract 2 ngu·ªìn d·ªØ li·ªáu tr√™n b·∫±ng pandas ƒë·ªÉ preprocessing (optimize size consumed) Load v√†o Amazon S3, t·ª´ ƒë√≥ load ti·∫øp v√†o data warehouse (PostgreSQL) ƒë·ªÉ l√†m analytics Transform d·ªØ li·ªáu b·∫±ng dbt tr√™n n·ªÅn PostgreSQL Tr·ª±c quan h√≥a d·ªØ li·ªáu b·∫±ng Metabase ELT pipeline design 3.2 Data lake structure # Ch√∫ng ta s·ª≠ d·ª•ng AWS S3 l√†m data lake. M·ªçi d·ªØ li·ªáu tr∆∞·ªõc h·∫øt s·∫Ω ƒë∆∞·ª£c ch·ª©a ·ªü ƒë√¢y. Trong project n√†y, ta ch·ªâ c·∫ßn 1 bucket v·ªõi nhi·ªÅu th∆∞ m·ª•c.\nBronze: L∆∞u d·ªØ li·ªáu th√¥ m·ªõi l·∫•y v·ªÅ. Ch√∫ng l√† step 1, 2, 3 trong pipeline design Silver: L∆∞u d·ªØ li·ªáu ƒë∆∞·ª£c ti·ªÅn x·ª≠ l√Ω. Ch√∫ng l√† step 4 trong pipeline design Gold: L∆∞u d·ªØ li·ªáu s·∫°ch khi transform b·∫±ng dbt (step 5) structure of S3 3.3 Directory tree # docker-compose: compose c√°c container ch·∫°y trong docker EDA: kh√°m ph√° dataset v√† profiling .gitignore: gi√∫p git kh√¥ng track file (nh∆∞ env, cache, \u0026hellip;) .gitlab-ci: config qu√° tr√¨nh CI tr√™n gitlab Makefile: thu g·ªçn c√¢u l·ªánh requirements.txt: packages python c·∫ßn thi·∫øt v√† thi·∫øt l·∫≠p virtualenv Folder dagser_home ch·ª©a dagster.yaml ƒë·ªÉ config th√†nh ph·∫ßn dagster c√≤n workspace.yaml ƒë·ªÉ ch·ªâ ƒë·ªãnh dagster ch·∫°y host elt_pipeline Folder dockers ch·ª©a file config c√°c container: dagster v√† jupyter Folder load_dataset ch·ª©a c√°c file d√πng ƒë·ªÉ load d·ªØ li·ªáu ban ƒë·∫ßu v√†o mySQL Folder terraform ƒë·ªÉ kh·ªüi t·∫°o v√† config server tr√™n AWS Chi ti·∫øt c√¢y th∆∞ m·ª•c xem ·ªü: directory tree\n4. Setup # 4.1 Prequisites # ƒê·ªÉ s·ª≠ d·ª•ng pipeline n√†y, download nh·ªØng ph·∫ßn m·ªÅm sau:\nGit T√†i kho·∫£n gitlab Terraform T√†i kho·∫£n AWS AWS CLI v√† configure Docker √≠t nh·∫•t 4GB RAM v√† Docker Compose √≠t nh·∫•t v1.29.2 N·∫øu d√πng Windows, setup th√™m WSL v√† m·ªôt m√°y ·∫£o local Ubuntu, c√†i ƒë·∫∑t nh·ªØng th·ª© tr√™n cho ubuntu.\n4.2 Setup data infrastructure local # Clone repository:\ngit clone https://gitlab.com/lelouvincx/fde01_project_fde220103_dinhminhchinh.git mv fde01_project_fde220103_dinhminhchinh project cd project # Create env file touch env cp env.template env ƒêi·ªÅn c√°c bi·∫øn m√¥i tr∆∞·ªùng v√†o file env.\nCh·∫°y c√°c l·ªánh sau ƒë·ªÉ setup infra d∆∞·ªõi local:\n# Setup python packages make install # Build docker images make build # Run locally make up # Check running containers docker ps # Check code quality make check make lint # Use black to reformat if any tests failed, then try again black ./elt_pipeline # Test coverage make test L√∫c n√†y s·∫Ω c√≥ 7 services sau ƒëang ch·∫°y: running services B√¢y gi·ªù ch√∫ng ta import dataset spotify (d·∫°ng csv) v√†o mySQL:\n# Enter mysql cli make to_mysql SET GLOBAL local_infile=true; -- Check if local_infile is turned on SHOW VARIABLES LIKE \u0026#34;local_infile\u0026#34;; exit Source t·ª´ng file theo th·ª© t·ª±:\n# Create tables with schema make mysql_create # Load csv into created tables make mysql_load # Set their foreign keys make mysql_set_foreign_key Kh·ªüi t·∫°o schema v√† table trong PostgreSQL:\n# Enter psql cli make psql_create Testing:\n# Test utils python3 -m pytest -vv --cov=utils elt_pipeline/tests/utils # Test ops python3 -m pytest -vv --cov=ops elt_pipeline/tests/ops Truy c·∫≠p giao di·ªán c·ªßa pipeline b·∫±ng dagit: https://localhost:3001/\n4.3 Setup data infrastructure on AWS # Ch√∫ng ta d√πng terraform l√†m IaC (Infrastructure as Code) ƒë·ªÉ setup h·∫° t·∫ßng tr√™n AWS (nh·ªõ c·∫•p credential key cho AWS nh√©) cd terraform # Initialize infra make tf-init # Checkout planned infra make tf-plan # Build up infra make tf-up ƒê·ª£i m·ªôt ch√∫t ƒë·ªÉ setup xong. Ch√∫ng ta l√™n Amazon Web Services\nTrong EC2 s·∫Ω th·∫•y 1 m√°y ·∫£o t√™n project-spotify-EC2 Trong S3 th·∫•y 1 bucket t√™n project-spotify-bucket Sau khi project-spotify-EC2 hi·ªán ƒë√£ pass h·∫øt status th√¨ ch√∫ng ta ƒë√£ setup th√†nh c√¥ng.\nB√¢y gi·ªù ch√∫ng ta truy c·∫≠p v√†o EC2 ƒë·ªÉ ho√†n t·∫•t setup # Connect to EC2 from local terminal make ssh-ec2 # Generate new ssh key for gitlab ssh-keygen # Then press Enter until done cat ~/.ssh/id_rsa.pub Copy ƒëo·∫°n m√£ SSH V√†o gitlab, ph√≠a tr√™n g√≥c ph·∫£i c√≥ h√¨nh avatar -\u0026gt; Preferences -\u0026gt; SSH Keys -\u0026gt; paste key v·ª´a copy v√†o -\u0026gt; ƒë·∫∑t t√™n l√† \u0026lsquo;project-spotify-vm\u0026rsquo; -\u0026gt; Add key V√†o terminal c·ªßa EC2 (v·ª´a connect l√∫c n√£y), clone v·ªÅ b·∫±ng SSH L·∫∑p l·∫°i b∆∞·ªõc setup infra local ƒë√£ tr√¨nh b√†y ·ªü ph·∫ßn tr√™n 5. Detailed code walkthrough # ELT pipeline g·ªìm 2 job ch·∫°y 2 t√°c v·ª• ƒë·ªôc l·∫≠p: EL data t·ª´ MySQL v√† EL data t·ª´ API nh∆∞ng nh√¨n chung ch√∫ng c√≥ c·∫•u tr√∫c gi·ªëng nhau. C·ª• th·ªÉ:\nextractdata_from{mysql/api}: L·∫•y data t·ª´ MySQL ho·∫∑c api (th√¥ng qua access token) v√† l∆∞u t·∫°m d∆∞·ªõi d·∫°ng pandas.DataFrame. T√πy theo chi·∫øn l∆∞·ª£c ingest data (full load/incremental by partition/incremental by watermark) m√† c√≥ c√°ch gi·∫£i quy·∫øt ph√π h·ª£p. load_data_to_s3: Ti·ªÅn x·ª≠ l√Ω data types cho DataFrame t·ª´ upstream v√† load v√†o S3 d∆∞·ªõi d·∫°ng parquet. load_data_to_psql: Extract data d·∫°ng parquet trong S3 th√†nh pandas.DataFrame v√† load v√†o PostgreSQL. ƒê·ªÉ d·ªØ li·ªáu ƒë∆∞·ª£c to√†n v·∫πn (kh√¥ng b·ªã crash, l·ªói ƒë∆∞·ªùng truy·ªÅn) trong qu√° tr√¨nh crash, ta t·∫°o TEMP TABLE v√† load v√†o ƒë√≥ tr∆∞·ªõc. validate_{mssql2psql/api2psql}_ingestion: Th·∫©m ƒë·ªãnh 3 step tr√™n ƒë√£ ƒë∆∞·ª£c EL th√†nh c√¥ng hay ch∆∞a trigger_dbt_spotify: Sensor ƒë·ªÉ trigger dbt nh·∫±m transform data. job_mssql2psql_ingestion job_api2psql_ingestion 5.1 Extract # L·∫•y data t·ª´ MySQL ho·∫∑c api (th√¥ng qua access token) v√† l∆∞u t·∫°m d∆∞·ªõi d·∫°ng pandas.DataFrame. T√πy theo chi·∫øn l∆∞·ª£c ingest data (full load/incremental by partition/incremental by watermark) m√† c√≥ c√°ch gi·∫£i quy·∫øt ph√π h·ª£p.\nTa ƒë·ªãnh nghƒ©a ph∆∞∆°ng th·ª©c extract data c·ªßa mysql v√† api trong th∆∞ m·ª•c utils.\nutils/mysql_loader/extract def extract_data(self, sql: str) -\u0026gt; pd.DataFrame: pd_data = None with self.get_db_connection() as db_conn: pd_data = pd.read_sql(sql, db_conn) return pd_data utils/api_loader/get_recently def get_recently(self, number: int, token: str) -\u0026gt; (int, dict): headers = { \u0026#34;Accept\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer \u0026#34; + token, } params = [(\u0026#34;limit\u0026#34;, number),] try: response = requests.get( \u0026#34;https://api.spotify.com/v1/me/player/recently-played\u0026#34;, headers=headers, params=params, timeout=10, ) return (response.status_code, response.json()) except: return None utils/api_loader/extract def extract_data(self, token: str) -\u0026gt; pd.DataFrame: (code, content) = self.get_recently(50, token) my_tracks = { \u0026#34;album_id\u0026#34;: [], \u0026#34;artists_id\u0026#34;: [], \u0026#34;track_id\u0026#34;: [], \u0026#34;track_unique_id\u0026#34;: [], \u0026#34;name\u0026#34;: [], \u0026#34;popularity\u0026#34;: [], \u0026#34;type\u0026#34;: [], \u0026#34;duration_ms\u0026#34;: [], \u0026#34;played_at\u0026#34;: [], \u0026#34;danceability\u0026#34;: [], \u0026#34;energy\u0026#34;: [], \u0026#34;track_key\u0026#34;: [], \u0026#34;loudness\u0026#34;: [], \u0026#34;mode\u0026#34;: [], \u0026#34;speechiness\u0026#34;: [], \u0026#34;acousticness\u0026#34;: [], \u0026#34;instrumentalness\u0026#34;: [], \u0026#34;liveness\u0026#34;: [], \u0026#34;valence\u0026#34;: [], \u0026#34;tempo\u0026#34;: [], } items = content.get(\u0026#34;items\u0026#34;, []) for item in items: # Take album_id, artists_id, track_id, name, popularity, type, duration_ms played_at = item.get(\u0026#34;played_at\u0026#34;, []) track = item.get(\u0026#34;track\u0026#34;, []) album = track.get(\u0026#34;album\u0026#34;, []) album_id = album.get(\u0026#34;id\u0026#34;, []) artists = track.get(\u0026#34;artists\u0026#34;, []) artists_id = [] for artist in artists: artists_id.append(artist.get(\u0026#34;id\u0026#34;, [])) track_id = track.get(\u0026#34;id\u0026#34;, []) name = track.get(\u0026#34;name\u0026#34;, []) popularity = track.get(\u0026#34;popularity\u0026#34;, []) type = track.get(\u0026#34;type\u0026#34;, []) duration_ms = track.get(\u0026#34;duration_ms\u0026#34;, []) # Take features features = self.get_features(track_id, token) danceability = features.get(\u0026#34;danceability\u0026#34;, []) energy = features.get(\u0026#34;energy\u0026#34;, []) track_key = features.get(\u0026#34;key\u0026#34;, []) loudness = features.get(\u0026#34;loudness\u0026#34;, []) mode = features.get(\u0026#34;mode\u0026#34;, []) speechiness = features.get(\u0026#34;speechiness\u0026#34;, []) acousticness = features.get(\u0026#34;acousticness\u0026#34;, []) instrumentalness = features.get(\u0026#34;instrumentalness\u0026#34;, []) liveness = features.get(\u0026#34;liveness\u0026#34;, []) valence = features.get(\u0026#34;valence\u0026#34;, []) tempo = features.get(\u0026#34;tempo\u0026#34;, []) # Extract row into dict my_tracks[\u0026#34;album_id\u0026#34;].append(album_id) my_tracks[\u0026#34;artists_id\u0026#34;].append(artists_id) my_tracks[\u0026#34;track_id\u0026#34;].append(track_id) my_tracks[\u0026#34;track_unique_id\u0026#34;].append(track_id + played_at) my_tracks[\u0026#34;name\u0026#34;].append(name) my_tracks[\u0026#34;popularity\u0026#34;].append(popularity) my_tracks[\u0026#34;type\u0026#34;].append(type) my_tracks[\u0026#34;duration_ms\u0026#34;].append(duration_ms) my_tracks[\u0026#34;played_at\u0026#34;].append(played_at[:10]) my_tracks[\u0026#34;danceability\u0026#34;].append(danceability) my_tracks[\u0026#34;energy\u0026#34;].append(energy) my_tracks[\u0026#34;track_key\u0026#34;].append(track_key) my_tracks[\u0026#34;loudness\u0026#34;].append(loudness) my_tracks[\u0026#34;mode\u0026#34;].append(mode) my_tracks[\u0026#34;speechiness\u0026#34;].append(speechiness) my_tracks[\u0026#34;acousticness\u0026#34;].append(acousticness) my_tracks[\u0026#34;instrumentalness\u0026#34;].append(instrumentalness) my_tracks[\u0026#34;liveness\u0026#34;].append(liveness) my_tracks[\u0026#34;valence\u0026#34;].append(valence) my_tracks[\u0026#34;tempo\u0026#34;].append(tempo) pd_data = pd.DataFrame(my_tracks) return pd_data Gi·ªù l√† l√∫c extract data.\nextract_data_from_mysql def extract_data_from_mysql(context, run_config): updated_at = context.op_config.get(\u0026#34;updated_at\u0026#34;) context.log.info(f\u0026#34;Updated at: {updated_at}\u0026#34;) if updated_at is None or updated_at == \u0026#34;\u0026#34;: context.log.info(\u0026#34;Nothing to do!\u0026#34;) return None context.log.info(f\u0026#34;Op extracts data from MySQL at {updated_at}\u0026#34;) # Choose extract strategy (default: full load) sql_stm = f\u0026#34;\u0026#34;\u0026#34; SELECT * FROM {run_config.get(\u0026#39;src_tbl\u0026#39;)} WHERE 1=1 \u0026#34;\u0026#34;\u0026#34; if run_config.get(\u0026#34;strategy\u0026#34;) == \u0026#34;incremental_by_partition\u0026#34;: if updated_at != \u0026#34;init_dump\u0026#34;: sql_stm += f\u0026#34;\u0026#34;\u0026#34; AND CAST({run_config.get(\u0026#39;partition\u0026#39;)} AS DATE) = \u0026#39;{updated_at}\u0026#39; \u0026#34;\u0026#34;\u0026#34; if run_config.get(\u0026#34;strategy\u0026#34;) == \u0026#34;incremental_by_watermark\u0026#34;: data_loader = get_data_loader( run_config.get(\u0026#34;db_provider\u0026#34;), run_config.get(\u0026#34;target_db_params\u0026#34;) ) watermark = data_loader.get_watermark( f\u0026#34;{run_config.get(\u0026#39;target_schema\u0026#39;)}.{run_config.get(\u0026#39;target_tbl\u0026#39;)}\u0026#34;, run_config.get(\u0026#34;watermark\u0026#34;), ) watermark = ( updated_at if watermark is None or watermark \u0026gt; updated_at else watermark ) if updated_at != \u0026#34;init_dump\u0026#34;: sql_stm += f\u0026#34;\u0026#34;\u0026#34; AND {run_config.get(\u0026#39;watermark\u0026#39;)} \u0026gt;= \u0026#39;{watermark}\u0026#39; \u0026#34;\u0026#34;\u0026#34; context.log.info(f\u0026#34;Extracting with SQL: {sql_stm}\u0026#34;) db_loader = MysqlLoader(run_config.get(\u0026#34;src_db_params\u0026#34;)) pd_data = db_loader.extract_data(sql_stm) context.log.info(f\u0026#34;Data extracted successfully with shape: {pd_data.shape}\u0026#34;) # Update params run_config.update( { \u0026#34;updated_at\u0026#34;: updated_at, \u0026#34;data\u0026#34;: pd_data, \u0026#34;s3_path\u0026#34;: f\u0026#34;bronze/{run_config.get(\u0026#39;data_source\u0026#39;)}/{run_config.get(\u0026#39;ls_target\u0026#39;).get(\u0026#39;target_tbl\u0026#39;)}\u0026#34;, \u0026#34;load_dtypes\u0026#34;: run_config.get(\u0026#34;load_dtypes\u0026#34;), } ) return run_config extract_data_from_api def extract_data_from_api(context, run_config): updated_at = context.op_config.get(\u0026#34;updated_at\u0026#34;) context.log.info(f\u0026#34;Updated at: {updated_at}\u0026#34;) if updated_at is None or updated_at == \u0026#34;\u0026#34;: context.log.info(\u0026#34;Nothing to do!\u0026#34;) return None context.log.info(f\u0026#34;Op extracts data from API at {updated_at}\u0026#34;) # Extract strategy (only support incremental_by_partition) context.log.info(f\u0026#34;Extracting on date: {updated_at}\u0026#34;) api_loader = ApiLoader(run_config.get(\u0026#34;src_api_params\u0026#34;)) token = api_loader.get_api_token() pd_data = api_loader.extract_data(token) index_played_at = pd_data[pd_data[\u0026#34;played_at\u0026#34;] != updated_at].index # Drop data pd_data.drop(index_played_at, inplace=True) context.log.info( f\u0026#34;Data loaded and filtered successfully with shape: {pd_data.shape}\u0026#34; ) run_config.update( { \u0026#34;updated_at\u0026#34;: updated_at, \u0026#34;data\u0026#34;: pd_data, \u0026#34;s3_path\u0026#34;: f\u0026#34;bronze/{run_config.get(\u0026#39;data_source\u0026#39;)}/{run_config.get(\u0026#39;ls_target\u0026#39;).get(\u0026#39;target_tbl\u0026#39;)}\u0026#34;, \u0026#34;load_dtypes\u0026#34;: run_config.get(\u0026#34;load_dtypes\u0026#34;), } ) return run_config 5.2 Load # Ti·ªÅn x·ª≠ l√Ω data types cho DataFrame t·ª´ upstream v√† load v√†o S3 d∆∞·ªõi d·∫°ng parquet.\nload_data_to_s3 def load_data_to_s3(context, upstream): if upstream is None: return None updated_at = upstream.get(\u0026#34;updated_at\u0026#34;) s3_bucket = os.getenv(\u0026#34;DATALAKE_BUCKET\u0026#34;) if type(updated_at) == list: updated_at = max(updated_at) s3_file = f\u0026#34;s3://{s3_bucket}/{upstream.get(\u0026#39;s3_path\u0026#39;)}/updated_at={updated_at}\u0026#34; context.log.info(f\u0026#34;Loading data to S3: {s3_file}\u0026#34;) # Load data to S3 pd_data = upstream.get(\u0026#34;data\u0026#34;) # Preprocess data load_dtypes = upstream.get(\u0026#34;load_dtypes\u0026#34;) try: for col, data_type in load_dtypes.items(): if data_type == \u0026#34;str\u0026#34;: pd_data[col] = pd_data[col].fillna(\u0026#34;\u0026#34;) pd_data[col] = pd_data[col].astype(str) pd_data[col] = pd_data[col].str.strip() pd_data[col] = pd_data[col].str.rstrip() pd_data[col] = pd_data[col].str.replace(\u0026#34;\u0026#39;\u0026#34;, \u0026#34;\u0026#34;) pd_data[col] = pd_data[col].str.replace(\u0026#39;\u0026#34;\u0026#39;, \u0026#34;\u0026#34;) pd_data[col] = pd_data[col].str.replace(r\u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;, regex=True) elif data_type == \u0026#34;int\u0026#34;: cur_bit = np.log2(pd_data[col].max()) if cur_bit \u0026gt; 32: pd_data[col] = pd_data[col].astype({col: \u0026#34;int64\u0026#34;}) elif cur_bit \u0026gt; 16: pd_data[col] = pd_data[col].astype({col: \u0026#34;int32\u0026#34;}) elif cur_bit \u0026gt; 8: pd_data[col] = pd_data[col].astype({col: \u0026#34;int16\u0026#34;}) else: pd_data[col] = pd_data[col].astype({col: \u0026#34;int8\u0026#34;}) elif data_type == \u0026#34;float\u0026#34;: pd_data[col] = pd_data[col].astype({col: \u0026#34;float32\u0026#34;}) context.log.info(f\u0026#34;Data preprocessed successfully\u0026#34;) except Exception as e: context.log.info(f\u0026#34;Exception: {e}\u0026#34;) # Write parquet object to S3 pa_data = pa.Table.from_pandas(df=pd_data, preserve_index=False) pq.write_table(pa_data, s3_file) context.log.info(\u0026#34;Data loaded successfully to S3\u0026#34;) # Update stream upstream.update({\u0026#34;s3_bucket\u0026#34;: s3_bucket, \u0026#34;s3_file\u0026#34;: s3_file}) return upstream load_data_to_psql def load_data_to_psql(context, upstream): if upstream is None: return None # Load data to target context.log.info(\u0026#34;Loading data to postgreSQL\u0026#34;) context.log.info(f\u0026#34;Extracting data from {upstream.get(\u0026#39;s3_file\u0026#39;)}\u0026#34;) pd_stag = pd.read_parquet(upstream.get(\u0026#34;s3_file\u0026#34;)) context.log.info(f\u0026#34;Extracted data shape: {pd_stag.shape}\u0026#34;) if len(pd_stag) == 0: context.log.info(\u0026#34;No data to upload!\u0026#34;) return \u0026#34;No data to upload!\u0026#34; # Execute db_loader = PsqlLoader(upstream.get(\u0026#34;target_db_params\u0026#34;)) result = db_loader.load_data(pd_stag, upstream) context.log.info(f\u0026#34;Batch inserted status: {result}\u0026#34;) return result 5.3 Transform # Do dataset h·∫ßu h·∫øt ƒë√£ ƒë∆∞·ª£c clean. Ch√∫ng ta ch·ªçn c√°c column c·∫ßn thi·∫øt cho vi·ªác visualization.\ncleaned_my_tracks.sql {{ config(materialized=\u0026#39;table\u0026#39;) }} with my_unique_tracks AS ( select * from spotify.my_tracks group by track_unique_id ) select * from my_unique_tracks cleaned_spotify_tracks.sql {{ config(materialized=\u0026#39;table\u0026#39;) }} with unique_tracks AS ( select acousticness, album_id, artists_id, country, danceability, duration_ms, energy, track_id, instrumentalness, track_key, liveness, loudness, mode, name, popularity, speechiness, tempo, valence from spotify.spotify_tracks group by track_id ) select * from unique_tracks cleaned_spotify_artists.sql {{ config(materialized=\u0026#39;table\u0026#39;) }} with unique_artists AS ( select artist_popularity, followers, genres, artist_id, name, track_id from spotify.spotify_artists group by artist_id ) select * from unique_artists cleaned_spotify_albums.sql {{ config(materialized=\u0026#39;table\u0026#39;) }} with unique_albums AS ( select album_id, artist_id, album_type, name, release_date, release_date_precision, total_tracks, track_id from spotify.spotify_albums group by album_id ) select * from unique_albums 5.4 Check results # Sau khi transform th√†nh c√¥ng, m·ªü dbeaver l√™n v√† ch√∫ng ta th·∫•y c√°c table ƒë√£ s·∫µn s√†ng ƒë·ªÉ analytics.\n6. Tear down infrastructure # D·ª° b·ªè infra sau khi xong vi·ªác (th·ª±c hi·ªán d∆∞·ªõi m√°y local):\n# Tear down containers make down # Tear down AWS cd terraform make tf-down Note: L√™n AWS ki·ªÉm tra l·∫°i c√°c services sau ƒë√£ d·ª´ng v√† b·ªã x√≥a ch∆∞a (n·∫øu kh√¥ng mu·ªën m·∫•t ti·ªÅn oan nh∆∞ m√¨nh): EC2, S3. 7. Design considerations # Sau khi deploy th√†nh c√¥ng pipeline, gi·ªù l√† l√∫c ƒë√°nh gi√° project.\nT·ªëc ƒë·ªô: T·ªëc ƒë·ªô extract data kh√° ch·∫≠m (v√¨ load v√†o pandas.DataFrame 2 l·∫ßn). M·ªôt s·ªë gi·∫£i ph√°p thay th·∫ø: polars, json, \u0026hellip; K√≠ch th∆∞·ªõc: Chuy·ªán g√¨ s·∫Ω x·∫£y ra khi data l·ªõn l√™n g·∫•p 10x, 100x, 1000x? L√∫c ƒë·∫•y ta c·∫ßn xem x√©t c√°c gi·∫£i ph√°p gi√∫p l∆∞u tr·ªØ big data, thay ƒë·ªïi data warehouse th√†nh Amazon RDS, Google BigQuery, \u0026hellip; M√¥i tr∆∞·ªùng ph√°t tri·ªÉn: Khi project c√≥ th√™m nhi·ªÅu ng∆∞·ªùi c√πng s·ª≠ d·ª•ng l√† c≈©ng l√† l√∫c ph√¢n chia m√¥i tr∆∞·ªùng th√†nh testing, staging, production. 8. Further actions # TƒÉng l∆∞·ª£ng data: T√≠ch h·ª£p nhi·ªÅu data h∆°n t·ª´ Spotify API: Khi ingest b√†i h√°t m·ªõi, ingest lu√¥n th√¥ng tin v·ªÅ artist, album, t·∫°o th√†nh h·ªá sinh th√°i b√†i h√°t ƒë·∫ßy ƒë·ªß. Stream ingestion: D√πng m·ªôt tech stack kh√°c cho job API theo h∆∞·ªõng streaming. H·ªá th·ªëng s·∫Ω listen m·ªói l·∫ßn nghe xong b√†i h√°t l√† t·ª± ƒë·ªông c·∫≠p nh·∫≠t v√†o pipeline. My wrap-up: T·ª± th·ª±c h√†nh ph√¢n t√≠ch d·ªØ li·ªáu nh∆∞ t√≠nh nƒÉng wrap-up c·ªßa spotify. Recommender system: Th·ª±c h√†nh l√†m m·ªôt h·ªá th·ªëng g·ª£i √Ω d·ª±a tr√™n nh·ªØng b√†i ƒë√£ nghe. ","date":"2022-12-18","permalink":"/projects/fde_project/","section":"Projects","summary":"X√¢y d·ª±ng m·ªôt data pipeline ELT ƒë·ªÉ ingest d·ªØ li·ªáu t·ª´ MySQL v√† API ƒë∆°n gi·∫£n t·ª´ Spotify (batch ingestion)","title":"Spotify data pipeline ingestion"},{"content":"","date":"0001-01-01","permalink":"/resources/reading-list/","section":"Resources","summary":"","title":""},{"content":"","date":"0001-01-01","permalink":"/resources/recommendations/","section":"Resources","summary":"","title":""},{"content":"Xin ch√†o! M√¨nh l√† lelouvincx, b√¨nh th∆∞·ªùng m·ªçi ng∆∞·ªùi g·ªçi l√† Ch√≠nh, t√°c gi·∫£ c·ªßa blog n√†y. B·∫°n c√≥ th·ªÉ g·ªçi m√¨nh theo c·∫£ hai c√°ch ƒë·ªÅu ƒë∆∞·ª£c. N·∫øu g·∫∑p m√¨nh ngo√†i ƒë·ªùi, b·∫°n c√≥ th·ªÉ s·∫Ω c·∫£m th·∫•y th·∫•t v·ªçng m·ªôt ch√∫t. M√¨nh kh√¥ng gi·ªèi giao ti·∫øp v√† duy tr√¨ mood cho cu·ªôc tr√≤ chuy·ªán. ƒê·ª´ng hi·ªÉu nh·∫ßm! M√¨nh th√≠ch giao ti·∫øp, chia s·∫ª th√¥ng tin, nh∆∞ng m√¨nh kh√¥ng gi·ªèi l√†m n√≥. V·∫≠y n√™n m√¨nh vi·∫øt blog n√†y ƒë·ªÉ truy·ªÅn ƒë·∫°t tri th·ª©c, tinh th·∫ßn v√† ƒëam m√™ c·ªßa b·∫£n th√¢n. B·∫°n c√≥ th·ªÉ xem m·ªói b√†i vi·∫øt nh∆∞ m·ªôt cu·ªôc tr√≤ chuy·ªán gi·ªØa b·∫°n v√† m√¨nh.\nHi·ªán t·∫°i m√¨nh l√† sinh vi√™n tr∆∞·ªùng ƒê·∫°i h·ªçc Khoa h·ªçc T·ª± nhi√™n HCM, ng√†nh khoa h·ªçc d·ªØ li·ªáu. M√¨nh th√≠ch x√¢y d·ª±ng v√† t·ªëi ∆∞u h√≥a h·ªá th·ªëng, ·ª©ng d·ª•ng, cu·ªôc s·ªëng, v√¢n v√¢n c·ª© c√°i g√¨ c√≥ th·ªÉ t·ªëi ∆∞u ƒë∆∞·ª£c l√† m√¨nh th√≠ch. V√¨ v·∫≠y m√¨nh th∆∞·ªùng d√†nh th·ªùi gian t√¨m hi·ªÉu, build, ƒë·∫≠p ƒëi build l·∫°i x3.14 (th∆∞·ªùng xuy√™n t·ªõi 2, 3 gi·ªù s√°ng) cho t·ªõi khi ƒë·∫°t ƒë∆∞·ª£c mong mu·ªën (ho·∫∑c l∆∞·ªùi kh√¥ng l√†m n·ªØa).\nV·ªÅ blog lelouvincx # ƒê√¢y l√† m·ªôt blog v·ªÅ b·∫£n th√¢n m√¨nh.\nB·∫°n c√≥ th·ªÉ t√¨m th·∫•y ·ªü ƒë√¢y ki·∫øn th·ª©c v·ªÅ data science, dev, productivity, t·∫£n m·∫°n, ƒë√¥i khi l√† review s√°ch, nh·∫°c, tranh, \u0026hellip; B√™n c·∫°nh ƒë√≥ l√† nh·ªØng kinh nghi·ªám m√¨nh h·ªçc ƒë∆∞·ª£c tr√™n con ƒë∆∞·ªùng m√¨nh tr∆∞·ªüng th√†nh, nh·ªØng c√¢u chuy·ªán √Ω nghƒ©a m√¨nh g·∫∑p trong cu·ªôc s·ªëng h·∫±ng ng√†y.\nPh·∫£i th√∫ th·∫≠t l√† m√¨nh r·∫•t th√≠ch vi·∫øt, nh∆∞ng ƒë·ªìng th·ªùi c≈©ng r·∫•t l∆∞·ªùi. L√∫c nh·ªè, m√¨nh t·ª´ng c√≥ m·ªôt blog tr√™n spiderum chuy√™n ch√©m gi√≥ v·ªÅ v√†i v·∫•n ƒë·ªÅ tu·ªïi teen. B·∫µng qua m·ªôt th·ªùi gian, m√¨nh l√™n t·ªânh h·ªçc c·∫•p 3 chuy√™n, blog ƒë√≥ c≈©ng d·∫ßn b·ªã b·ªè ng·ªè v√¨ l∆∞·ªùi. B∆∞·ªõc ch√¢n l√™n ƒë·∫°i h·ªçc, m√¨nh c≈©ng h√πng h·ªï l·∫≠p blog m·ªõi, ƒë√≤i t·ª± build l·∫°i t·ª´ ƒë·∫ßu n√†y n·ªç, nh∆∞ng r·ªìi c≈©ng b·ªè d·ªü v√¨ l∆∞·ªùi. R·ªìi m√¨nh nh·∫≠n ra nh·ªØng g√¨ m√¨nh c·∫ßn l√†m ch·ªâ l√† vi·∫øt th√¥i. C√≤n nh·ªØng th·ª© kh√°c, ƒë·ªÉ sau h√£y t√≠nh (kh√¥ng ph·∫£i l∆∞·ªùi ƒë√¢u nh√© =))).\nV·ªÅ b·∫£n th√¢n m√¨nh # M√¨nh t·ªët nghi·ªáp c·∫•p 3 ·ªü tr∆∞·ªùng THPT Chuy√™n H√πng V∆∞∆°ng - Gia Lai, kh√≥a 18 - 21. Su·ªët nh·ªØng nƒÉm c·∫•p 3, m√¨nh ƒëi·ªÅu h√†nh m·ªôt c√¢u l·∫°c b·ªô v·ªÅ tranh bi·ªán v√† t·ªï ch·ª©c h·ªôi ngh·ªã MUN (Model United Nations) cho h·ªçc sinh trong t·ªânh. M√¨nh c≈©ng l√† th√†nh vi√™n c·ªßa ƒë·ªôi tuy·ªÉn Tin h·ªçc qu·ªëc gia ·ªü tr∆∞·ªùng. Hi·ªán t·∫°i m√¨nh l√† sinh vi√™n nƒÉm hai, kh√≥a K21 tr∆∞·ªùng ƒë·∫°i h·ªçc KHTN HCM.\nNh·ªØng th·ª© m√¨nh l√†m # Ch·ªß y·∫øu l√† nh·ªØng th·ª© m√¨nh gi√∫p √≠ch cho cu·ªôc s·ªëng c·ªßa m√¨nh. B·∫°n c√≥ th·ªÉ xem th√™m ·ªü trang project v√† github c·ªßa m√¨nh.\nK·∫øt n·ªëi v·ªõi m√¨nh # Email (active h√†ng ng√†y) Tr√≤ chuy·ªán tr·ª±c ti·∫øp v·ªõi m√¨nh Nh·∫Øn tin qua discord (active th∆∞·ªùng xuy√™n) Q\u0026amp;A # ","date":"0001-01-01","permalink":"/resources/about/","section":"Resources","summary":"Introduction about me","title":"About me"},{"content":"","date":"0001-01-01","permalink":"/resources/list-100/","section":"Resources","summary":"My reading list","title":"My reading list"},{"content":"","date":"0001-01-01","permalink":"/series/","section":"Series","summary":"","title":"Series"}]