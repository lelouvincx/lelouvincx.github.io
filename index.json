[{"content":"","date":"18 December 2022","permalink":"/tags/data-engineering/","section":"Tags","summary":"","title":"data engineering"},{"content":"","date":"18 December 2022","permalink":"/","section":"lelouvincx's blog","summary":"","title":"lelouvincx's blog"},{"content":"","date":"18 December 2022","permalink":"/tags/project/","section":"Tags","summary":"","title":"project"},{"content":"","date":"18 December 2022","permalink":"/projects/","section":"Projects","summary":"","title":"Projects"},{"content":"1. Introduction\u003e 1. Introduction # Trong project này mình sẽ hướng dẫn xây dựng một data pipeline cơ bản theo mô hình ELT (extract - load - transform), sử dụng bộ dữ liệu từ spotify để phân tích xu hướng nghe nhạc.\nProject này hoàn thành dựa trên kiến thức đã học được từ khóa Fundamental Data Engineering của AIDE. Xin gửi lời cảm ơn đặc biệt tới thầy Nguyễn Thanh Bình, anh Ông Xuân Hồng và anh Hùng Lê.\nSource code của project.\n2. Objective\u003e 2. Objective # Mục tiêu của project này là xây dựng một data pipeline để đưa dữ liệu của bảng spotify_tracks từ mySQL và my_tracks từ API của Spotify thành dashboard để phân tích. Các bảng được hỗ trợ bởi spotify_albums và spotify_artists như mô tả dưới đây:\nspotify_tracks: OLTP table chứa thông tin bài hát từ spotify my_tracks: lịch sử stream nhạc của bản thân, lấy schema giống spotify_tracks spotify_albums: chứa thông tin albums từ dataset spotify_artists: thông tin nghệ sĩ từ dataset table schema Chi tiết hơn xem ở: Exploratory Data Analysis\n3. Design\u003e 3. Design # 3.1 Pipeline design\u003e 3.1 Pipeline design # Chúng ta sử dụng máy ảo AWS EC2 để tính toán và dagster để orchestrate tasks.\nDữ liệu spotify được download từ kaggle dưới dạng csv, sau đó import vào mySQL mô phỏng làm dữ liệu doanh nghiệp Dữ liệu streaming history của bản thân được extract từ spotify API Extract 2 nguồn dữ liệu trên bằng pandas để preprocessing (optimize size consumed) Load vào Amazon S3, từ đó load tiếp vào data warehouse (PostgreSQL) để làm analytics Transform dữ liệu bằng dbt trên nền PostgreSQL Trực quan hóa dữ liệu bằng Metabase ELT pipeline design 3.2 Data lake structure\u003e 3.2 Data lake structure # Chúng ta sử dụng AWS S3 làm data lake. Mọi dữ liệu trước hết sẽ được chứa ở đây. Trong project này, ta chỉ cần 1 bucket với nhiều thư mục.\nBronze: Lưu dữ liệu thô mới lấy về. Chúng là step 1, 2, 3 trong pipeline design Silver: Lưu dữ liệu được tiền xử lý. Chúng là step 4 trong pipeline design Gold: Lưu dữ liệu sạch khi transform bằng dbt (step 5) structure of S3 3.3 Directory tree\u003e 3.3 Directory tree # docker-compose: compose các container chạy trong docker EDA: khám phá dataset và profiling .gitignore: giúp git không track file (như env, cache, \u0026hellip;) .gitlab-ci: config quá trình CI trên gitlab Makefile: thu gọn câu lệnh requirements.txt: packages python cần thiết và thiết lập virtualenv Folder dagser_home chứa dagster.yaml để config thành phần dagster còn workspace.yaml để chỉ định dagster chạy host elt_pipeline Folder dockers chứa file config các container: dagster và jupyter Folder load_dataset chứa các file dùng để load dữ liệu ban đầu vào mySQL Folder terraform để khởi tạo và config server trên AWS Chi tiết cây thư mục xem ở: directory tree\n4. Setup\u003e 4. Setup # 4.1 Prequisites\u003e 4.1 Prequisites # Để sử dụng pipeline này, download những phần mềm sau:\nGit Tài khoản gitlab Terraform Tài khoản AWS AWS CLI và configure Docker ít nhất 4GB RAM và Docker Compose ít nhất v1.29.2 Nếu dùng Windows, setup thêm WSL và một máy ảo local Ubuntu, cài đặt những thứ trên cho ubuntu.\n4.2 Setup data infrastructure local\u003e 4.2 Setup data infrastructure local # Clone repository:\ngit clone https://gitlab.com/lelouvincx/fde01_project_fde220103_dinhminhchinh.git mv fde01_project_fde220103_dinhminhchinh project cd project # Create env file touch env cp env.template env Điền các biến môi trường vào file env.\nChạy các lệnh sau để setup infra dưới local:\n# Setup python packages make install # Build docker images make build # Run locally make up # Check running containers docker ps # Check code quality make check make lint # Use black to reformat if any tests failed, then try again black ./elt_pipeline # Test coverage make test Lúc này sẽ có 7 services sau đang chạy: running services Bây giờ chúng ta import dataset spotify (dạng csv) vào mySQL:\n# Enter mysql cli make to_mysql SET GLOBAL local_infile=true; -- Check if local_infile is turned on SHOW VARIABLES LIKE \u0026#34;local_infile\u0026#34;; exit Source từng file theo thứ tự:\n# Create tables with schema make mysql_create # Load csv into created tables make mysql_load # Set their foreign keys make mysql_set_foreign_key Khởi tạo schema và table trong PostgreSQL:\n# Enter psql cli make psql_create Testing:\n# Test utils python3 -m pytest -vv --cov=utils elt_pipeline/tests/utils # Test ops python3 -m pytest -vv --cov=ops elt_pipeline/tests/ops Truy cập giao diện của pipeline bằng dagit: https://localhost:3001/\n4.3 Setup data infrastructure on AWS\u003e 4.3 Setup data infrastructure on AWS # Chúng ta dùng terraform làm IaC (Infrastructure as Code) để setup hạ tầng trên AWS (nhớ cấp credential key cho AWS nhé) cd terraform # Initialize infra make tf-init # Checkout planned infra make tf-plan # Build up infra make tf-up Đợi một chút để setup xong. Chúng ta lên Amazon Web Services\nTrong EC2 sẽ thấy 1 máy ảo tên project-spotify-EC2 Trong S3 thấy 1 bucket tên project-spotify-bucket Sau khi project-spotify-EC2 hiện đã pass hết status thì chúng ta đã setup thành công.\nBây giờ chúng ta truy cập vào EC2 để hoàn tất setup # Connect to EC2 from local terminal make ssh-ec2 # Generate new ssh key for gitlab ssh-keygen # Then press Enter until done cat ~/.ssh/id_rsa.pub Copy đoạn mã SSH Vào gitlab, phía trên góc phải có hình avatar -\u0026gt; Preferences -\u0026gt; SSH Keys -\u0026gt; paste key vừa copy vào -\u0026gt; đặt tên là \u0026lsquo;project-spotify-vm\u0026rsquo; -\u0026gt; Add key Vào terminal của EC2 (vừa connect lúc nãy), clone về bằng SSH Lặp lại bước setup infra local đã trình bày ở phần trên 5. Detailed code walkthrough\u003e 5. Detailed code walkthrough # ELT pipeline gồm 2 job chạy 2 tác vụ độc lập: EL data từ MySQL và EL data từ API nhưng nhìn chung chúng có cấu trúc giống nhau. Cụ thể:\nextractdata_from{mysql/api}: Lấy data từ MySQL hoặc api (thông qua access token) và lưu tạm dưới dạng pandas.DataFrame. Tùy theo chiến lược ingest data (full load/incremental by partition/incremental by watermark) mà có cách giải quyết phù hợp. load_data_to_s3: Tiền xử lý data types cho DataFrame từ upstream và load vào S3 dưới dạng parquet. load_data_to_psql: Extract data dạng parquet trong S3 thành pandas.DataFrame và load vào PostgreSQL. Để dữ liệu được toàn vẹn (không bị crash, lỗi đường truyền) trong quá trình crash, ta tạo TEMP TABLE và load vào đó trước. validate_{mssql2psql/api2psql}_ingestion: Thẩm định 3 step trên đã được EL thành công hay chưa trigger_dbt_spotify: Sensor để trigger dbt nhằm transform data. job_mssql2psql_ingestion job_api2psql_ingestion 5.1 Extract\u003e 5.1 Extract # Lấy data từ MySQL hoặc api (thông qua access token) và lưu tạm dưới dạng pandas.DataFrame. Tùy theo chiến lược ingest data (full load/incremental by partition/incremental by watermark) mà có cách giải quyết phù hợp.\nTa định nghĩa phương thức extract data của mysql và api trong thư mục utils.\nutils/mysql_loader/extract def extract_data(self, sql: str) -\u0026gt; pd.DataFrame: pd_data = None with self.get_db_connection() as db_conn: pd_data = pd.read_sql(sql, db_conn) return pd_data utils/api_loader/get_recently def get_recently(self, number: int, token: str) -\u0026gt; (int, dict): headers = { \u0026#34;Accept\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;Authorization\u0026#34;: f\u0026#34;Bearer \u0026#34; + token, } params = [(\u0026#34;limit\u0026#34;, number),] try: response = requests.get( \u0026#34;https://api.spotify.com/v1/me/player/recently-played\u0026#34;, headers=headers, params=params, timeout=10, ) return (response.status_code, response.json()) except: return None utils/api_loader/extract def extract_data(self, token: str) -\u0026gt; pd.DataFrame: (code, content) = self.get_recently(50, token) my_tracks = { \u0026#34;album_id\u0026#34;: [], \u0026#34;artists_id\u0026#34;: [], \u0026#34;track_id\u0026#34;: [], \u0026#34;track_unique_id\u0026#34;: [], \u0026#34;name\u0026#34;: [], \u0026#34;popularity\u0026#34;: [], \u0026#34;type\u0026#34;: [], \u0026#34;duration_ms\u0026#34;: [], \u0026#34;played_at\u0026#34;: [], \u0026#34;danceability\u0026#34;: [], \u0026#34;energy\u0026#34;: [], \u0026#34;track_key\u0026#34;: [], \u0026#34;loudness\u0026#34;: [], \u0026#34;mode\u0026#34;: [], \u0026#34;speechiness\u0026#34;: [], \u0026#34;acousticness\u0026#34;: [], \u0026#34;instrumentalness\u0026#34;: [], \u0026#34;liveness\u0026#34;: [], \u0026#34;valence\u0026#34;: [], \u0026#34;tempo\u0026#34;: [], } items = content.get(\u0026#34;items\u0026#34;, []) for item in items: # Take album_id, artists_id, track_id, name, popularity, type, duration_ms played_at = item.get(\u0026#34;played_at\u0026#34;, []) track = item.get(\u0026#34;track\u0026#34;, []) album = track.get(\u0026#34;album\u0026#34;, []) album_id = album.get(\u0026#34;id\u0026#34;, []) artists = track.get(\u0026#34;artists\u0026#34;, []) artists_id = [] for artist in artists: artists_id.append(artist.get(\u0026#34;id\u0026#34;, [])) track_id = track.get(\u0026#34;id\u0026#34;, []) name = track.get(\u0026#34;name\u0026#34;, []) popularity = track.get(\u0026#34;popularity\u0026#34;, []) type = track.get(\u0026#34;type\u0026#34;, []) duration_ms = track.get(\u0026#34;duration_ms\u0026#34;, []) # Take features features = self.get_features(track_id, token) danceability = features.get(\u0026#34;danceability\u0026#34;, []) energy = features.get(\u0026#34;energy\u0026#34;, []) track_key = features.get(\u0026#34;key\u0026#34;, []) loudness = features.get(\u0026#34;loudness\u0026#34;, []) mode = features.get(\u0026#34;mode\u0026#34;, []) speechiness = features.get(\u0026#34;speechiness\u0026#34;, []) acousticness = features.get(\u0026#34;acousticness\u0026#34;, []) instrumentalness = features.get(\u0026#34;instrumentalness\u0026#34;, []) liveness = features.get(\u0026#34;liveness\u0026#34;, []) valence = features.get(\u0026#34;valence\u0026#34;, []) tempo = features.get(\u0026#34;tempo\u0026#34;, []) # Extract row into dict my_tracks[\u0026#34;album_id\u0026#34;].append(album_id) my_tracks[\u0026#34;artists_id\u0026#34;].append(artists_id) my_tracks[\u0026#34;track_id\u0026#34;].append(track_id) my_tracks[\u0026#34;track_unique_id\u0026#34;].append(track_id + played_at) my_tracks[\u0026#34;name\u0026#34;].append(name) my_tracks[\u0026#34;popularity\u0026#34;].append(popularity) my_tracks[\u0026#34;type\u0026#34;].append(type) my_tracks[\u0026#34;duration_ms\u0026#34;].append(duration_ms) my_tracks[\u0026#34;played_at\u0026#34;].append(played_at[:10]) my_tracks[\u0026#34;danceability\u0026#34;].append(danceability) my_tracks[\u0026#34;energy\u0026#34;].append(energy) my_tracks[\u0026#34;track_key\u0026#34;].append(track_key) my_tracks[\u0026#34;loudness\u0026#34;].append(loudness) my_tracks[\u0026#34;mode\u0026#34;].append(mode) my_tracks[\u0026#34;speechiness\u0026#34;].append(speechiness) my_tracks[\u0026#34;acousticness\u0026#34;].append(acousticness) my_tracks[\u0026#34;instrumentalness\u0026#34;].append(instrumentalness) my_tracks[\u0026#34;liveness\u0026#34;].append(liveness) my_tracks[\u0026#34;valence\u0026#34;].append(valence) my_tracks[\u0026#34;tempo\u0026#34;].append(tempo) pd_data = pd.DataFrame(my_tracks) return pd_data Giờ là lúc extract data.\nextract_data_from_mysql def extract_data_from_mysql(context, run_config): updated_at = context.op_config.get(\u0026#34;updated_at\u0026#34;) context.log.info(f\u0026#34;Updated at: {updated_at}\u0026#34;) if updated_at is None or updated_at == \u0026#34;\u0026#34;: context.log.info(\u0026#34;Nothing to do!\u0026#34;) return None context.log.info(f\u0026#34;Op extracts data from MySQL at {updated_at}\u0026#34;) # Choose extract strategy (default: full load) sql_stm = f\u0026#34;\u0026#34;\u0026#34; SELECT * FROM {run_config.get(\u0026#39;src_tbl\u0026#39;)} WHERE 1=1 \u0026#34;\u0026#34;\u0026#34; if run_config.get(\u0026#34;strategy\u0026#34;) == \u0026#34;incremental_by_partition\u0026#34;: if updated_at != \u0026#34;init_dump\u0026#34;: sql_stm += f\u0026#34;\u0026#34;\u0026#34; AND CAST({run_config.get(\u0026#39;partition\u0026#39;)} AS DATE) = \u0026#39;{updated_at}\u0026#39; \u0026#34;\u0026#34;\u0026#34; if run_config.get(\u0026#34;strategy\u0026#34;) == \u0026#34;incremental_by_watermark\u0026#34;: data_loader = get_data_loader( run_config.get(\u0026#34;db_provider\u0026#34;), run_config.get(\u0026#34;target_db_params\u0026#34;) ) watermark = data_loader.get_watermark( f\u0026#34;{run_config.get(\u0026#39;target_schema\u0026#39;)}.{run_config.get(\u0026#39;target_tbl\u0026#39;)}\u0026#34;, run_config.get(\u0026#34;watermark\u0026#34;), ) watermark = ( updated_at if watermark is None or watermark \u0026gt; updated_at else watermark ) if updated_at != \u0026#34;init_dump\u0026#34;: sql_stm += f\u0026#34;\u0026#34;\u0026#34; AND {run_config.get(\u0026#39;watermark\u0026#39;)} \u0026gt;= \u0026#39;{watermark}\u0026#39; \u0026#34;\u0026#34;\u0026#34; context.log.info(f\u0026#34;Extracting with SQL: {sql_stm}\u0026#34;) db_loader = MysqlLoader(run_config.get(\u0026#34;src_db_params\u0026#34;)) pd_data = db_loader.extract_data(sql_stm) context.log.info(f\u0026#34;Data extracted successfully with shape: {pd_data.shape}\u0026#34;) # Update params run_config.update( { \u0026#34;updated_at\u0026#34;: updated_at, \u0026#34;data\u0026#34;: pd_data, \u0026#34;s3_path\u0026#34;: f\u0026#34;bronze/{run_config.get(\u0026#39;data_source\u0026#39;)}/{run_config.get(\u0026#39;ls_target\u0026#39;).get(\u0026#39;target_tbl\u0026#39;)}\u0026#34;, \u0026#34;load_dtypes\u0026#34;: run_config.get(\u0026#34;load_dtypes\u0026#34;), } ) return run_config extract_data_from_api def extract_data_from_api(context, run_config): updated_at = context.op_config.get(\u0026#34;updated_at\u0026#34;) context.log.info(f\u0026#34;Updated at: {updated_at}\u0026#34;) if updated_at is None or updated_at == \u0026#34;\u0026#34;: context.log.info(\u0026#34;Nothing to do!\u0026#34;) return None context.log.info(f\u0026#34;Op extracts data from API at {updated_at}\u0026#34;) # Extract strategy (only support incremental_by_partition) context.log.info(f\u0026#34;Extracting on date: {updated_at}\u0026#34;) api_loader = ApiLoader(run_config.get(\u0026#34;src_api_params\u0026#34;)) token = api_loader.get_api_token() pd_data = api_loader.extract_data(token) index_played_at = pd_data[pd_data[\u0026#34;played_at\u0026#34;] != updated_at].index # Drop data pd_data.drop(index_played_at, inplace=True) context.log.info( f\u0026#34;Data loaded and filtered successfully with shape: {pd_data.shape}\u0026#34; ) run_config.update( { \u0026#34;updated_at\u0026#34;: updated_at, \u0026#34;data\u0026#34;: pd_data, \u0026#34;s3_path\u0026#34;: f\u0026#34;bronze/{run_config.get(\u0026#39;data_source\u0026#39;)}/{run_config.get(\u0026#39;ls_target\u0026#39;).get(\u0026#39;target_tbl\u0026#39;)}\u0026#34;, \u0026#34;load_dtypes\u0026#34;: run_config.get(\u0026#34;load_dtypes\u0026#34;), } ) return run_config 5.2 Load\u003e 5.2 Load # Tiền xử lý data types cho DataFrame từ upstream và load vào S3 dưới dạng parquet.\nload_data_to_s3 def load_data_to_s3(context, upstream): if upstream is None: return None updated_at = upstream.get(\u0026#34;updated_at\u0026#34;) s3_bucket = os.getenv(\u0026#34;DATALAKE_BUCKET\u0026#34;) if type(updated_at) == list: updated_at = max(updated_at) s3_file = f\u0026#34;s3://{s3_bucket}/{upstream.get(\u0026#39;s3_path\u0026#39;)}/updated_at={updated_at}\u0026#34; context.log.info(f\u0026#34;Loading data to S3: {s3_file}\u0026#34;) # Load data to S3 pd_data = upstream.get(\u0026#34;data\u0026#34;) # Preprocess data load_dtypes = upstream.get(\u0026#34;load_dtypes\u0026#34;) try: for col, data_type in load_dtypes.items(): if data_type == \u0026#34;str\u0026#34;: pd_data[col] = pd_data[col].fillna(\u0026#34;\u0026#34;) pd_data[col] = pd_data[col].astype(str) pd_data[col] = pd_data[col].str.strip() pd_data[col] = pd_data[col].str.rstrip() pd_data[col] = pd_data[col].str.replace(\u0026#34;\u0026#39;\u0026#34;, \u0026#34;\u0026#34;) pd_data[col] = pd_data[col].str.replace(\u0026#39;\u0026#34;\u0026#39;, \u0026#34;\u0026#34;) pd_data[col] = pd_data[col].str.replace(r\u0026#34;\\n\u0026#34;, \u0026#34;\u0026#34;, regex=True) elif data_type == \u0026#34;int\u0026#34;: cur_bit = np.log2(pd_data[col].max()) if cur_bit \u0026gt; 32: pd_data[col] = pd_data[col].astype({col: \u0026#34;int64\u0026#34;}) elif cur_bit \u0026gt; 16: pd_data[col] = pd_data[col].astype({col: \u0026#34;int32\u0026#34;}) elif cur_bit \u0026gt; 8: pd_data[col] = pd_data[col].astype({col: \u0026#34;int16\u0026#34;}) else: pd_data[col] = pd_data[col].astype({col: \u0026#34;int8\u0026#34;}) elif data_type == \u0026#34;float\u0026#34;: pd_data[col] = pd_data[col].astype({col: \u0026#34;float32\u0026#34;}) context.log.info(f\u0026#34;Data preprocessed successfully\u0026#34;) except Exception as e: context.log.info(f\u0026#34;Exception: {e}\u0026#34;) # Write parquet object to S3 pa_data = pa.Table.from_pandas(df=pd_data, preserve_index=False) pq.write_table(pa_data, s3_file) context.log.info(\u0026#34;Data loaded successfully to S3\u0026#34;) # Update stream upstream.update({\u0026#34;s3_bucket\u0026#34;: s3_bucket, \u0026#34;s3_file\u0026#34;: s3_file}) return upstream load_data_to_psql def load_data_to_psql(context, upstream): if upstream is None: return None # Load data to target context.log.info(\u0026#34;Loading data to postgreSQL\u0026#34;) context.log.info(f\u0026#34;Extracting data from {upstream.get(\u0026#39;s3_file\u0026#39;)}\u0026#34;) pd_stag = pd.read_parquet(upstream.get(\u0026#34;s3_file\u0026#34;)) context.log.info(f\u0026#34;Extracted data shape: {pd_stag.shape}\u0026#34;) if len(pd_stag) == 0: context.log.info(\u0026#34;No data to upload!\u0026#34;) return \u0026#34;No data to upload!\u0026#34; # Execute db_loader = PsqlLoader(upstream.get(\u0026#34;target_db_params\u0026#34;)) result = db_loader.load_data(pd_stag, upstream) context.log.info(f\u0026#34;Batch inserted status: {result}\u0026#34;) return result 5.3 Transform\u003e 5.3 Transform # Do dataset hầu hết đã được clean. Chúng ta chọn các column cần thiết cho việc visualization.\ncleaned_my_tracks.sql {{ config(materialized=\u0026#39;table\u0026#39;) }} with my_unique_tracks AS ( select * from spotify.my_tracks group by track_unique_id ) select * from my_unique_tracks cleaned_spotify_tracks.sql {{ config(materialized=\u0026#39;table\u0026#39;) }} with unique_tracks AS ( select acousticness, album_id, artists_id, country, danceability, duration_ms, energy, track_id, instrumentalness, track_key, liveness, loudness, mode, name, popularity, speechiness, tempo, valence from spotify.spotify_tracks group by track_id ) select * from unique_tracks cleaned_spotify_artists.sql {{ config(materialized=\u0026#39;table\u0026#39;) }} with unique_artists AS ( select artist_popularity, followers, genres, artist_id, name, track_id from spotify.spotify_artists group by artist_id ) select * from unique_artists cleaned_spotify_albums.sql {{ config(materialized=\u0026#39;table\u0026#39;) }} with unique_albums AS ( select album_id, artist_id, album_type, name, release_date, release_date_precision, total_tracks, track_id from spotify.spotify_albums group by album_id ) select * from unique_albums 5.4 Check results\u003e 5.4 Check results # Sau khi transform thành công, mở dbeaver lên và chúng ta thấy các table đã sẵn sàng để analytics.\n6. Tear down infrastructure\u003e 6. Tear down infrastructure # Dỡ bỏ infra sau khi xong việc (thực hiện dưới máy local):\n# Tear down containers make down # Tear down AWS cd terraform make tf-down Note: Lên AWS kiểm tra lại các services sau đã dừng và bị xóa chưa (nếu không muốn mất tiền oan như mình): EC2, S3.\n7. Design considerations\u003e 7. Design considerations # Sau khi deploy thành công pipeline, giờ là lúc đánh giá project.\nTốc độ: Tốc độ extract data khá chậm (vì load vào pandas.DataFrame 2 lần). Một số giải pháp thay thế: polars, json, \u0026hellip; Kích thước: Chuyện gì sẽ xảy ra khi data lớn lên gấp 10x, 100x, 1000x? Lúc đấy ta cần xem xét các giải pháp giúp lưu trữ big data, thay đổi data warehouse thành Amazon RDS, Google BigQuery, \u0026hellip; Môi trường phát triển: Khi project có thêm nhiều người cùng sử dụng là cũng là lúc phân chia môi trường thành testing, staging, production. 8. Further actions\u003e 8. Further actions # Tăng lượng data: Tích hợp nhiều data hơn từ Spotify API: Khi ingest bài hát mới, ingest luôn thông tin về artist, album, tạo thành hệ sinh thái bài hát đầy đủ. Stream ingestion: Dùng một tech stack khác cho job API theo hướng streaming. Hệ thống sẽ listen mỗi lần nghe xong bài hát là tự động cập nhật vào pipeline. My wrap-up: Tự thực hành phân tích dữ liệu như tính năng wrap-up của spotify. Recommender system: Thực hành làm một hệ thống gợi ý dựa trên những bài đã nghe. ","date":"18 December 2022","permalink":"/projects/fde_project/","section":"Projects","summary":"Xây dựng một data pipeline ELT để ingest dữ liệu từ MySQL và API đơn giản từ Spotify (batch ingestion)","title":"Spotify data pipeline ingestion"},{"content":"","date":"18 December 2022","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"Xin chào! Mình là lelouvincx, bình thường mọi người gọi là Chính, tác giả của blog này. Bạn có thể gọi mình theo cả hai cách đều được. Nếu gặp mình ngoài đời, bạn có thể sẽ cảm thấy thất vọng một chút. Mình không giỏi giao tiếp và duy trì mood cho cuộc trò chuyện. Đừng hiểu nhầm! Mình thích giao tiếp, chia sẻ thông tin, nhưng mình không giỏi làm nó. Vậy nên mình viết blog này để truyền đạt tri thức, tinh thần và đam mê của bản thân. Bạn có thể xem mỗi bài viết như một cuộc trò chuyện giữa bạn và mình.\nHiện tại mình là sinh viên trường Đại học Khoa học Tự nhiên HCM, ngành khoa học dữ liệu. Mình thích xây dựng và tối ưu hóa hệ thống, ứng dụng, cuộc sống, vân vân cứ cái gì có thể tối ưu được là mình thích. Vì vậy mình thường dành thời gian tìm hiểu, build, đập đi build lại x3.14 (thường xuyên tới 2, 3 giờ sáng) cho tới khi đạt được mong muốn (hoặc lười không làm nữa).\nVề blog lelouvincx\u003e Về blog lelouvincx # Đây là một blog về bản thân mình.\nBạn có thể tìm thấy ở đây kiến thức về data science, dev, productivity, tản mạn, đôi khi là review sách, nhạc, tranh, \u0026hellip; Bên cạnh đó là những kinh nghiệm mình học được trên con đường mình trưởng thành, những câu chuyện ý nghĩa mình gặp trong cuộc sống hằng ngày.\nPhải thú thật là mình rất thích viết, nhưng đồng thời cũng rất lười. Lúc nhỏ, mình từng có một blog trên spiderum chuyên chém gió về vài vấn đề tuổi teen. Bẵng qua một thời gian, mình lên tỉnh học cấp 3 chuyên, blog đó cũng dần bị bỏ ngỏ vì lười. Bước chân lên đại học, mình cũng hùng hổ lập blog mới, đòi tự build lại từ đầu này nọ, nhưng rồi cũng bỏ dở vì lười. Rồi mình nhận ra những gì mình cần làm chỉ là viết thôi. Còn những thứ khác, để sau hãy tính (không phải lười đâu nhé =))).\nVề bản thân mình\u003e Về bản thân mình # Mình tốt nghiệp cấp 3 ở trường THPT Chuyên Hùng Vương - Gia Lai, khóa 18 - 21. Suốt những năm cấp 3, mình điều hành một câu lạc bộ về tranh biện và tổ chức hội nghị MUN (Model United Nations) cho học sinh trong tỉnh. Mình cũng là thành viên của đội tuyển Tin học quốc gia ở trường. Hiện tại mình là sinh viên năm hai, khóa K21 trường đại học KHTN HCM.\nNhững thứ mình làm\u003e Những thứ mình làm # Chủ yếu là những thứ mình giúp ích cho cuộc sống của mình. Bạn có thể xem thêm ở trang project và github của mình.\nKết nối với mình\u003e Kết nối với mình # Email (active hàng ngày) Trò chuyện trực tiếp với mình Nhắn tin qua discord (active thường xuyên) Q\u0026amp;A\u003e Q\u0026amp;A # ","date":"1 January 0001","permalink":"/about/","section":"About me","summary":"Xin chào! Mình là lelouvincx, bình thường mọi người gọi là Chính, tác giả của blog này. Bạn có thể gọi mình theo cả hai cách đều được.","title":"About me"},{"content":"","date":"1 January 0001","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"1 January 0001","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":" Journey of a thousand miles begins with a single step. ~ Lao Tzu ~ This section contains my posts, aka articles about multiple topics.\n","date":"1 January 0001","permalink":"/posts/","section":"Posts","summary":"Journey of a thousand miles begins with a single step. ~ Lao Tzu ~ This section contains my posts, aka articles about multiple topics.","title":"Posts"},{"content":"","date":"1 January 0001","permalink":"/series/","section":"Series","summary":"","title":"Series"}]